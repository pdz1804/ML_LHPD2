{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: kagglehub in e:\\anaconda3\\envs\\ml_env_test\\lib\\site-packages (0.3.6)\n",
      "Requirement already satisfied: packaging in c:\\users\\user\\appdata\\roaming\\python\\python312\\site-packages (from kagglehub) (24.2)\n",
      "Requirement already satisfied: requests in e:\\anaconda3\\envs\\ml_env_test\\lib\\site-packages (from kagglehub) (2.32.3)\n",
      "Requirement already satisfied: tqdm in e:\\anaconda3\\envs\\ml_env_test\\lib\\site-packages (from kagglehub) (4.67.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in e:\\anaconda3\\envs\\ml_env_test\\lib\\site-packages (from requests->kagglehub) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in e:\\anaconda3\\envs\\ml_env_test\\lib\\site-packages (from requests->kagglehub) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in e:\\anaconda3\\envs\\ml_env_test\\lib\\site-packages (from requests->kagglehub) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in e:\\anaconda3\\envs\\ml_env_test\\lib\\site-packages (from requests->kagglehub) (2024.12.14)\n",
      "Requirement already satisfied: colorama in c:\\users\\user\\appdata\\roaming\\python\\python312\\site-packages (from tqdm->kagglehub) (0.4.6)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install kagglehub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\anaconda3\\envs\\ml_env_test\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "import kagglehub\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Path to dataset files: C:\\Users\\User\\.cache\\kagglehub\\datasets\\kazanova\\sentiment140\\versions\\2\n",
      "Path to dataset files: C:\\Users\\User\\.cache\\kagglehub\\datasets\\yasserh\\twitter-tweets-sentiment-dataset\\versions\\1\n",
      "Path to dataset files: C:\\Users\\User\\.cache\\kagglehub\\datasets\\saurabhshahane\\twitter-sentiment-dataset\\versions\\1\n"
     ]
    }
   ],
   "source": [
    "# Download latest version\n",
    "path1 = kagglehub.dataset_download(\"kazanova/sentiment140\")\n",
    "path2 = kagglehub.dataset_download(\"yasserh/twitter-tweets-sentiment-dataset\")\n",
    "path3 = kagglehub.dataset_download(\"saurabhshahane/twitter-sentiment-dataset\")\n",
    "\n",
    "print(\"Path to dataset files:\", path1)\n",
    "print(\"Path to dataset files:\", path2)\n",
    "print(\"Path to dataset files:\", path3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_and_move_datasets(dataset_ids, destination_directory):\n",
    "    \"\"\"\n",
    "    Download datasets using KaggleHub, move them to the specified directory, \n",
    "    and rename the files by prefixing them with the dataset ID.\n",
    "\n",
    "    :param dataset_ids: List of Kaggle dataset IDs to download.\n",
    "    :param destination_directory: Directory where datasets will be moved.\n",
    "    :return: A dictionary containing dataset IDs and their corresponding file paths.\n",
    "    \"\"\"\n",
    "    dataset_files = {}\n",
    "    for dataset_id in dataset_ids:\n",
    "        # Download dataset\n",
    "        cache_path = kagglehub.dataset_download(dataset_id)\n",
    "        \n",
    "        # Ensure the destination directory exists\n",
    "        os.makedirs(destination_directory, exist_ok=True)\n",
    "        \n",
    "        # List files in the cache directory\n",
    "        files = os.listdir(cache_path)\n",
    "        \n",
    "        # Move and rename each file to the destination directory\n",
    "        moved_files = []\n",
    "        for file in files:\n",
    "            src = os.path.join(cache_path, file)\n",
    "            # Prefix the file name with the dataset ID\n",
    "            renamed_file = f\"{dataset_id.replace('/', '_')}_{file}\"\n",
    "            dest = os.path.join(destination_directory, renamed_file)\n",
    "            shutil.copy(src, dest)\n",
    "            moved_files.append(dest)\n",
    "\n",
    "        dataset_files[dataset_id] = moved_files\n",
    "        print(f\"Dataset '{dataset_id}' files moved and renamed to: {destination_directory}\")\n",
    "    \n",
    "    return dataset_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inspect_datasets(dataset_files, headers_dict=None):\n",
    "    \"\"\"\n",
    "    Inspect the datasets by loading them into pandas DataFrames and showing an overview.\n",
    "    If headers are provided, the dataset is saved back after inspection.\n",
    "\n",
    "    :param dataset_files: Dictionary containing dataset IDs and their file paths.\n",
    "    :param headers_dict: Optional dictionary mapping dataset IDs to their headers.\n",
    "                         If the value for a dataset ID is an empty list, the first line of the file is treated as headers.\n",
    "                         Example: {\"dataset_id\": [\"col1\", \"col2\", ...], \"dataset_with_no_headers\": []}\n",
    "    \"\"\"\n",
    "    headers_dict = headers_dict or {}  # Default to an empty dictionary if not provided\n",
    "\n",
    "    for dataset_id, files in dataset_files.items():\n",
    "        print(f\"\\n### Dataset: {dataset_id} ###\")\n",
    "        for file in files:\n",
    "            try:\n",
    "                print(f\"\\nFile: {file}\")\n",
    "                # Determine headers based on the dataset ID\n",
    "                headers = headers_dict.get(dataset_id, None)\n",
    "                use_infer_header = headers == []  # Infer headers if explicitly marked with an empty list\n",
    "\n",
    "                # Attempt to load the file into a DataFrame\n",
    "                if file.endswith(\".csv\"):\n",
    "                    df = pd.read_csv(\n",
    "                        file,\n",
    "                        encoding=\"latin1\",\n",
    "                        low_memory=False,\n",
    "                        header=0 if use_infer_header else None,\n",
    "                        names=headers if headers else None\n",
    "                    )\n",
    "                elif file.endswith(\".tsv\"):\n",
    "                    df = pd.read_csv(\n",
    "                        file,\n",
    "                        sep=\"\\t\",\n",
    "                        encoding=\"latin1\",\n",
    "                        low_memory=False,\n",
    "                        header=0 if use_infer_header else None,\n",
    "                        names=headers if headers else None\n",
    "                    )\n",
    "                else:\n",
    "                    print(f\"Unsupported file format: {file}\")\n",
    "                    continue\n",
    "\n",
    "                # Display basic information\n",
    "                print(\"DataFrame Overview:\")\n",
    "                print(df.info())\n",
    "                print(\"\\nFirst 5 Rows:\")\n",
    "                print(df.head())\n",
    "\n",
    "                # Save the dataset back if headers were explicitly provided\n",
    "                if headers or use_infer_header:\n",
    "                    save_path = file.replace(\".csv\", \"_with_headers.csv\") if file.endswith(\".csv\") else file.replace(\".tsv\", \"_with_headers.tsv\")\n",
    "                    df.to_csv(save_path, index=False, sep=\"\\t\" if file.endswith(\".tsv\") else \",\")\n",
    "                    print(f\"Dataset with headers saved to: {save_path}\")\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Error reading file '{file}': {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of dataset IDs to download\n",
    "dataset_ids = [\n",
    "    \"kazanova/sentiment140\",\n",
    "    \"yasserh/twitter-tweets-sentiment-dataset\",\n",
    "    \"saurabhshahane/twitter-sentiment-dataset\"\n",
    "]\n",
    "\n",
    "# Define the destination directory\n",
    "destination_directory = \"../../data/raw\"\n",
    "\n",
    "# Define headers for datasets that are missing them\n",
    "headers_dict = {\n",
    "    \"kazanova/sentiment140\": [\"target\", \"ids\", \"date\", \"flag\", \"user\", \"text\"],\n",
    "    \"yasserh/twitter-tweets-sentiment-dataset\": [],\n",
    "    \"saurabhshahane/twitter-sentiment-dataset\": []\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset 'kazanova/sentiment140' files moved and renamed to: ../../data/raw\n",
      "Dataset 'yasserh/twitter-tweets-sentiment-dataset' files moved and renamed to: ../../data/raw\n",
      "Dataset 'saurabhshahane/twitter-sentiment-dataset' files moved and renamed to: ../../data/raw\n"
     ]
    }
   ],
   "source": [
    "# Download and move datasets\n",
    "dataset_files = download_and_move_datasets(dataset_ids, destination_directory)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "### Dataset: kazanova/sentiment140 ###\n",
      "\n",
      "File: ../../data/raw\\kazanova_sentiment140_training.1600000.processed.noemoticon.csv\n",
      "DataFrame Overview:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1600000 entries, 0 to 1599999\n",
      "Data columns (total 6 columns):\n",
      " #   Column  Non-Null Count    Dtype \n",
      "---  ------  --------------    ----- \n",
      " 0   target  1600000 non-null  int64 \n",
      " 1   ids     1600000 non-null  int64 \n",
      " 2   date    1600000 non-null  object\n",
      " 3   flag    1600000 non-null  object\n",
      " 4   user    1600000 non-null  object\n",
      " 5   text    1600000 non-null  object\n",
      "dtypes: int64(2), object(4)\n",
      "memory usage: 73.2+ MB\n",
      "None\n",
      "\n",
      "First 5 Rows:\n",
      "   target         ids                          date      flag  \\\n",
      "0       0  1467810369  Mon Apr 06 22:19:45 PDT 2009  NO_QUERY   \n",
      "1       0  1467810672  Mon Apr 06 22:19:49 PDT 2009  NO_QUERY   \n",
      "2       0  1467810917  Mon Apr 06 22:19:53 PDT 2009  NO_QUERY   \n",
      "3       0  1467811184  Mon Apr 06 22:19:57 PDT 2009  NO_QUERY   \n",
      "4       0  1467811193  Mon Apr 06 22:19:57 PDT 2009  NO_QUERY   \n",
      "\n",
      "              user                                               text  \n",
      "0  _TheSpecialOne_  @switchfoot http://twitpic.com/2y1zl - Awww, t...  \n",
      "1    scotthamilton  is upset that he can't update his Facebook by ...  \n",
      "2         mattycus  @Kenichan I dived many times for the ball. Man...  \n",
      "3          ElleCTF    my whole body feels itchy and like its on fire   \n",
      "4           Karoli  @nationwideclass no, it's not behaving at all....  \n",
      "Dataset with headers saved to: ../../data/raw\\kazanova_sentiment140_training.1600000.processed.noemoticon_with_headers.csv\n",
      "\n",
      "### Dataset: yasserh/twitter-tweets-sentiment-dataset ###\n",
      "\n",
      "File: ../../data/raw\\yasserh_twitter-tweets-sentiment-dataset_Tweets.csv\n",
      "DataFrame Overview:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 27481 entries, 0 to 27480\n",
      "Data columns (total 4 columns):\n",
      " #   Column         Non-Null Count  Dtype \n",
      "---  ------         --------------  ----- \n",
      " 0   textID         27481 non-null  object\n",
      " 1   text           27480 non-null  object\n",
      " 2   selected_text  27480 non-null  object\n",
      " 3   sentiment      27481 non-null  object\n",
      "dtypes: object(4)\n",
      "memory usage: 858.9+ KB\n",
      "None\n",
      "\n",
      "First 5 Rows:\n",
      "       textID                                               text  \\\n",
      "0  cb774db0d1                I`d have responded, if I were going   \n",
      "1  549e992a42      Sooo SAD I will miss you here in San Diego!!!   \n",
      "2  088c60f138                          my boss is bullying me...   \n",
      "3  9642c003ef                     what interview! leave me alone   \n",
      "4  358bd9e861   Sons of ****, why couldn`t they put them on t...   \n",
      "\n",
      "                         selected_text sentiment  \n",
      "0  I`d have responded, if I were going   neutral  \n",
      "1                             Sooo SAD  negative  \n",
      "2                          bullying me  negative  \n",
      "3                       leave me alone  negative  \n",
      "4                        Sons of ****,  negative  \n",
      "Dataset with headers saved to: ../../data/raw\\yasserh_twitter-tweets-sentiment-dataset_Tweets_with_headers.csv\n",
      "\n",
      "### Dataset: saurabhshahane/twitter-sentiment-dataset ###\n",
      "\n",
      "File: ../../data/raw\\saurabhshahane_twitter-sentiment-dataset_Twitter_Data.csv\n",
      "DataFrame Overview:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 162980 entries, 0 to 162979\n",
      "Data columns (total 2 columns):\n",
      " #   Column      Non-Null Count   Dtype  \n",
      "---  ------      --------------   -----  \n",
      " 0   clean_text  162976 non-null  object \n",
      " 1   category    162973 non-null  float64\n",
      "dtypes: float64(1), object(1)\n",
      "memory usage: 2.5+ MB\n",
      "None\n",
      "\n",
      "First 5 Rows:\n",
      "                                          clean_text  category\n",
      "0  when modi promised âminimum government maxim...      -1.0\n",
      "1  talk all the nonsense and continue all the dra...       0.0\n",
      "2  what did just say vote for modi  welcome bjp t...       1.0\n",
      "3  asking his supporters prefix chowkidar their n...       1.0\n",
      "4  answer who among these the most powerful world...       1.0\n",
      "Dataset with headers saved to: ../../data/raw\\saurabhshahane_twitter-sentiment-dataset_Twitter_Data_with_headers.csv\n"
     ]
    }
   ],
   "source": [
    "# Inspect the datasets with custom headers\n",
    "inspect_datasets(dataset_files, headers_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directory Structure of e:\\2_LEARNING_BKU\\2_File_2\\K22_HK242\\CO3117_Machine_Learning\\Main\\data\\raw\n",
      "[Dir] data\n",
      "  [Dir] final\n",
      "    [File] a.txt\n",
      "    [File] final_clean.csv\n",
      "    [File] final_clean_no_duplicates.csv\n",
      "    [File] final_clean_no_neutral.csv\n",
      "    [File] final_clean_no_neutral_no_duplicates.csv\n",
      "  [Dir] processed\n",
      "    [File] a.txt\n",
      "    [File] df1_with_text_clean.csv\n",
      "    [File] df2_with_text_clean.csv\n",
      "    [File] df3_with_text_clean.csv\n",
      "    [File] processed_data.csv\n",
      "    [File] sample_with_text_clean.csv\n",
      "  [Dir] raw\n",
      "    [File] a.txt\n",
      "    [File] data_download.ipynb\n",
      "    [File] kazanova_sentiment140_training.1600000.processed.noemoticon.csv\n",
      "    [File] kazanova_sentiment140_training.1600000.processed.noemoticon_with_headers.csv\n",
      "    [File] saurabhshahane_twitter-sentiment-dataset_Twitter_Data.csv\n",
      "    [File] saurabhshahane_twitter-sentiment-dataset_Twitter_Data_with_headers.csv\n",
      "    [File] yasserh_twitter-tweets-sentiment-dataset_Tweets.csv\n",
      "    [File] yasserh_twitter-tweets-sentiment-dataset_Tweets_with_headers.csv\n",
      "  [File] README.md\n",
      "  [Dir] store\n",
      "    [File] final_clean.csv\n",
      "    [File] final_clean_no_neutral.csv\n",
      "[File] environment.yml\n",
      "[File] LICENSE\n",
      "[File] Logger.py\n",
      "[Dir] models\n",
      "  [Dir] experiments\n",
      "    [File] a.py\n",
      "  [File] README.md\n",
      "  [Dir] trained\n",
      "    [File] a.py\n",
      "[Dir] notebooks\n",
      "  [Dir] assignment1\n",
      "    [File] exploration.ipynb\n",
      "    [File] submission.ipynb\n",
      "  [Dir] assignment2\n",
      "    [File] exploration.ipynb\n",
      "    [File] submission.ipynb\n",
      "  [Dir] final_project\n",
      "    [File] submission_ass1.ipynb\n",
      "    [File] submission_ass2.ipynb\n",
      "[Dir] other\n",
      "  [File] clean_requirement.py\n",
      "  [File] conda.txt\n",
      "  [File] main.py\n",
      "  [File] mem.py\n",
      "  [File] requirements.txt\n",
      "[File] output.png\n",
      "[File] README.md\n",
      "[Dir] reports\n",
      "  [Dir] figures\n",
      "    [File] a.txt\n",
      "  [Dir] final_project\n",
      "    [File] a.txt\n",
      "[Dir] src\n",
      "  [Dir] data\n",
      "    [File] make_dataset.py\n",
      "    [File] preprocess.py\n",
      "    [File] process.ipynb\n",
      "    [File] __init__.py\n",
      "    [Dir] __pycache__\n",
      "      [File] make_dataset.cpython-312.pyc\n",
      "      [File] preprocess.cpython-312.pyc\n",
      "  [Dir] features\n",
      "    [File] build_features_utils.py\n",
      "    [File] example.ipynb\n",
      "    [File] __init__.py\n",
      "    [Dir] __pycache__\n",
      "      [File] build_features.cpython-312.pyc\n",
      "      [File] build_features_utils.cpython-312.pyc\n",
      "      [File] __init__.cpython-312.pyc\n",
      "  [Dir] models\n",
      "    [File] best_decision_tree_bert.pkl\n",
      "    [File] best_decision_tree_count.pkl\n",
      "    [File] best_decision_tree_glove.pkl\n",
      "    [File] best_decision_tree_word2vec.pkl\n",
      "    [File] models_utils.py\n",
      "    [File] predict_model.ipynb\n",
      "    [Dir] store\n",
      "      [File] best_decision_tree_full_count.pkl\n",
      "    [File] train_model.ipynb\n",
      "    [File] __init__.py\n",
      "    [Dir] __pycache__\n",
      "      [File] models_utils.cpython-312.pyc\n",
      "      [File] __init__.cpython-312.pyc\n",
      "  [Dir] visualization\n",
      "    [File] visualize.py\n",
      "    [File] visualize_example.ipynb\n",
      "    [File] __init__.py\n",
      "    [Dir] __pycache__\n",
      "      [File] visualize.cpython-311.pyc\n",
      "      [File] __init__.cpython-311.pyc\n",
      "[Dir] tests\n",
      "  [File] test_data.py\n",
      "  [File] test_models.py\n",
      "  [File] __init__.py\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "def visualize_directory_structure(path, indent=0):\n",
    "    \"\"\"\n",
    "    Recursively prints the directory structure starting from the given path.\n",
    "    \"\"\"\n",
    "    for item in os.listdir(path):\n",
    "        if item == \".git\" or item == \".gitignore\":\n",
    "            continue\n",
    "        item_path = os.path.join(path, item)\n",
    "        if os.path.isdir(item_path):\n",
    "            print(\"  \" * indent + f\"[Dir] {item}\")\n",
    "            visualize_directory_structure(item_path, indent + 1)\n",
    "        else:\n",
    "            print(\"  \" * indent + f\"[File] {item}\")\n",
    "\n",
    "# Get the current working directory\n",
    "current_directory = os.getcwd()\n",
    "\n",
    "print(\"Directory Structure of\", current_directory)\n",
    "visualize_directory_structure(\"E:\\\\2_LEARNING_BKU\\\\2_File_2\\\\K22_HK242\\\\CO3117_Machine_Learning\\\\Main\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_env_test",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
