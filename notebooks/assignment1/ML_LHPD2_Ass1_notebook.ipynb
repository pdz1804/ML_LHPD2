{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-04T11:15:04.601873Z",
     "iopub.status.busy": "2025-02-04T11:15:04.601488Z",
     "iopub.status.idle": "2025-02-04T11:15:08.697313Z",
     "shell.execute_reply": "2025-02-04T11:15:08.696219Z",
     "shell.execute_reply.started": "2025-02-04T11:15:04.601825Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!pip install xgboost gensim tqdm hmmlearn pgmpy sklearn-crfsuite transformers tensorflow keras keras-tuner\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-04T11:15:08.698960Z",
     "iopub.status.busy": "2025-02-04T11:15:08.698610Z",
     "iopub.status.idle": "2025-02-04T11:15:08.706146Z",
     "shell.execute_reply": "2025-02-04T11:15:08.705153Z",
     "shell.execute_reply.started": "2025-02-04T11:15:08.698924Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Standard Library Imports\n",
    "import os\n",
    "import sys\n",
    "import pickle\n",
    "import joblib\n",
    "import logging\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from statistics import mean\n",
    "\n",
    "# Visualization Libraries\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Machine Learning Libraries\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, KFold, GridSearchCV\n",
    "from sklearn.preprocessing import MaxAbsScaler, MinMaxScaler, StandardScaler\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, roc_auc_score, log_loss, hinge_loss\n",
    "from sklearn.tree import DecisionTreeClassifier, plot_tree\n",
    "from sklearn.linear_model import Perceptron, LogisticRegression\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import GaussianNB, MultinomialNB\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "from pgmpy.models import BayesianNetwork\n",
    "from pgmpy.estimators import MaximumLikelihoodEstimator\n",
    "from pgmpy.inference import VariableElimination\n",
    "\n",
    "import nltk\n",
    "import re\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from collections import defaultdict, Counter\n",
    "from hmmlearn import hmm\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Specialized Libraries\n",
    "import xgboost as xgb\n",
    "import hmmlearn.hmm\n",
    "from hmmlearn.hmm import GaussianHMM\n",
    "from sklearn_crfsuite import CRF\n",
    "\n",
    "# Natural Language Processing (NLP) Libraries\n",
    "import gensim.downloader as api\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "# Progress Bar\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import keras_tuner as kt\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.models import load_model\n",
    "from tqdm import tqdm\n",
    "\n",
    "import keras\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras_tuner import RandomSearch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-04T11:15:08.708334Z",
     "iopub.status.busy": "2025-02-04T11:15:08.708080Z",
     "iopub.status.idle": "2025-02-04T11:15:08.729363Z",
     "shell.execute_reply": "2025-02-04T11:15:08.728330Z",
     "shell.execute_reply.started": "2025-02-04T11:15:08.708312Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "## Options\n",
    "pd.set_option(\"max_colwidth\", None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-04T11:15:08.731416Z",
     "iopub.status.busy": "2025-02-04T11:15:08.731059Z",
     "iopub.status.idle": "2025-02-04T11:15:08.746164Z",
     "shell.execute_reply": "2025-02-04T11:15:08.745214Z",
     "shell.execute_reply.started": "2025-02-04T11:15:08.731381Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class MyLogger:\n",
    "    \"\"\"\n",
    "        Initializes the MyLogger with a specified log file for logging messages.\n",
    "        \n",
    "        :param log_file: str, path to the log file (default: 'app.log')\n",
    "    \"\"\"\n",
    "    def __init__(self, log_file='app.log'):\n",
    "        self.log_file = log_file\n",
    "        self._initialize_logger()\n",
    "\n",
    "    def _initialize_logger(self):\n",
    "        \"\"\"\n",
    "        Initializes the MyLogger with a specified log file for logging messages.\n",
    "        \n",
    "        :param log_file: str, path to the log file (default: 'app.log')\n",
    "        \"\"\"\n",
    "        if os.path.exists(self.log_file):\n",
    "            file_mode = 'a'\n",
    "        else:\n",
    "            file_mode = 'w'\n",
    "\n",
    "        self.logger = logging.getLogger()\n",
    "        self.logger.setLevel(logging.INFO)\n",
    "\n",
    "        file_handler = logging.FileHandler(self.log_file, mode=file_mode, encoding='utf-8')\n",
    "        file_handler.setLevel(logging.INFO)\n",
    "\n",
    "        console_handler = logging.StreamHandler()\n",
    "        console_handler.setLevel(logging.INFO)\n",
    "\n",
    "        formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')\n",
    "        file_handler.setFormatter(formatter)\n",
    "        console_handler.setFormatter(formatter)\n",
    "\n",
    "        if self.logger.hasHandlers():\n",
    "            self.logger.handlers.clear()\n",
    "\n",
    "        self.logger.addHandler(file_handler)\n",
    "        self.logger.addHandler(console_handler)\n",
    "\n",
    "    def log_message(self, message):\n",
    "        \"\"\"\n",
    "        Logs an info-level message to the current log file and console.\n",
    "        \n",
    "        :param message: str, the message to be logged\n",
    "        \"\"\"\n",
    "        self.logger.info(message)\n",
    "\n",
    "    def change_log_file(self, new_log_file):\n",
    "        \"\"\"\n",
    "        Changes the log file path and reinitializes the logger with the new file.\n",
    "        \n",
    "        :param new_log_file: str, the new path to the log file\n",
    "        \"\"\"\n",
    "        self.log_file = new_log_file\n",
    "        self._initialize_logger()\n",
    "\n",
    "logger = MyLogger()\n",
    "\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"true\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Build"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-04T11:15:08.747392Z",
     "iopub.status.busy": "2025-02-04T11:15:08.747163Z",
     "iopub.status.idle": "2025-02-04T11:15:08.770758Z",
     "shell.execute_reply": "2025-02-04T11:15:08.769745Z",
     "shell.execute_reply.started": "2025-02-04T11:15:08.747373Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class FeatureBuilder:\n",
    "    def __init__(self, method=\"tfidf\", save_dir=\"data/processed\", reduce_dim=None, n_components=100):\n",
    "        \"\"\"\n",
    "        Initializes the FeatureBuilder with a specified feature engineering method.\n",
    "        \n",
    "        :param method: str, feature engineering method ('tfidf', 'count', 'word2vec', 'bert', etc.)\n",
    "        :param save_dir: str, directory to save processed features\n",
    "        \"\"\"\n",
    "        self.method = method\n",
    "        self.save_dir = save_dir\n",
    "        self.reduce_dim = reduce_dim\n",
    "        self.n_components = n_components\n",
    "        os.makedirs(save_dir, exist_ok=True)\n",
    "        \n",
    "        # Define models for vectorization\n",
    "        if method == \"tfidf\":\n",
    "            self.vectorizer = TfidfVectorizer(max_features=2000, stop_words=\"english\")\n",
    "        elif method == \"count\":\n",
    "            self.vectorizer = CountVectorizer(max_features=2000)\n",
    "        elif method == \"binary_count\":\n",
    "            self.vectorizer = CountVectorizer(binary=True, max_features=2000)\n",
    "        elif method == \"word2vec\":\n",
    "            self.word2vec_model = api.load(\"word2vec-google-news-300\")  # Pretrained Google News Word2Vec\n",
    "        elif method == \"glove\":\n",
    "            self.glove_model = api.load(\"glove-wiki-gigaword-100\")  # Pretrained GloVe embeddings\n",
    "        elif method == \"bert\":\n",
    "            self.tokenizer = AutoTokenizer.from_pretrained(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "            self.bert_model = AutoModel.from_pretrained(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "    \n",
    "        # Initialize dimensionality reduction\n",
    "        if self.reduce_dim == \"pca\":\n",
    "            self.reducer = PCA(n_components=self.n_components)\n",
    "        elif self.reduce_dim == \"lda\":\n",
    "            self.reducer = LDA(n_components=min(self.n_components, 1))  # LDA needs class labels, adjust accordingly\n",
    "    \n",
    "    def _get_word2vec_vector(self, doc):\n",
    "        \"\"\"\n",
    "        Extracts the average Word2Vec embedding for a document.\n",
    "\n",
    "        :param doc: str, the document text\n",
    "        :return: np.array, the averaged Word2Vec embedding\n",
    "        \"\"\"\n",
    "        tokens = doc.split()\n",
    "        word_vectors = []\n",
    "        for token in tokens:\n",
    "            if token in self.word2vec_model:  # Access word directly\n",
    "                word_vectors.append(self.word2vec_model[token])  # No need for '.wv'\n",
    "        if word_vectors:\n",
    "            return np.mean(word_vectors, axis=0)\n",
    "        else:\n",
    "            return np.zeros(self.word2vec_model.vector_size)\n",
    "\n",
    "    def _get_glove_vector(self, doc):\n",
    "        \"\"\"\n",
    "        Extracts the average GloVe embedding for a document.\n",
    "\n",
    "        :param doc: str, the document text\n",
    "        :return: np.array, the averaged GloVe embedding\n",
    "        \"\"\"\n",
    "        tokens = doc.split()\n",
    "        word_vectors = []\n",
    "        for token in tokens:\n",
    "            if token in self.glove_model:  # Same for GloVe\n",
    "                word_vectors.append(self.glove_model[token])  # Use directly without '.wv'\n",
    "        if word_vectors:\n",
    "            return np.mean(word_vectors, axis=0)\n",
    "        else:\n",
    "            return np.zeros(self.glove_model.vector_size)\n",
    "\n",
    "    def _get_bert_embedding(self, doc):\n",
    "        \"\"\"\n",
    "        Extracts the BERT embedding for a document.\n",
    "\n",
    "        :param doc: str, the document text\n",
    "        :return: np.array, the BERT embedding\n",
    "        \"\"\"\n",
    "        inputs = self.tokenizer(doc, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n",
    "        with torch.no_grad():\n",
    "            outputs = self.bert_model(**inputs)\n",
    "        return outputs.pooler_output.squeeze(0).numpy()\n",
    "    \n",
    "    def fit(self, texts, labels=None):\n",
    "        \"\"\"\n",
    "        Fits the model to the text data by computing necessary statistics (e.g., vocabulary, embeddings).\n",
    "\n",
    "        :param texts: list, raw text data\n",
    "        :return: None\n",
    "        \"\"\"\n",
    "        if self.method in [\"tfidf\", \"count\", \"binary_count\"]:\n",
    "            self.vectorizer.fit(texts)\n",
    "        elif self.method in [\"word2vec\", \"glove\", \"bert\"]:\n",
    "            pass\n",
    "\n",
    "        # if self.reduce_dim == \"lda\" and labels is not None:\n",
    "        #     features = self.vectorizer.transform(texts).toarray()\n",
    "        #     self.reducer.fit(features, labels)\n",
    "        # elif self.reduce_dim == \"pca\":\n",
    "        #     features = self.vectorizer.transform(texts).toarray()\n",
    "        #     self.reducer.fit(features)\n",
    "\n",
    "    def transform(self, texts, labels=None):\n",
    "        \"\"\"\n",
    "        Transforms new data based on the fitted model.\n",
    "\n",
    "        :param texts: list, raw text data\n",
    "        :return: transformed feature matrix\n",
    "        \"\"\"\n",
    "        if self.method in [\"tfidf\", \"count\", \"binary_count\"]:\n",
    "            # Transform the new data using the fitted vectorizer\n",
    "            features = self.vectorizer.transform(texts).toarray()\n",
    "\n",
    "        elif self.method == \"word2vec\":\n",
    "            # Use the pre-trained Word2Vec model to generate embeddings\n",
    "            word2vec_embeddings = []\n",
    "            for doc in tqdm(texts, desc=\"Processing Word2Vec\", unit=\"document\"):\n",
    "                word2vec_embeddings.append(self._get_word2vec_vector(doc))\n",
    "            features = np.array(word2vec_embeddings)\n",
    "\n",
    "        elif self.method == \"glove\":\n",
    "            # Similar process for GloVe embeddings\n",
    "            glove_embeddings = []\n",
    "            for doc in tqdm(texts, desc=\"Processing GloVe\", unit=\"document\"):\n",
    "                glove_embeddings.append(self._get_glove_vector(doc))\n",
    "            features = np.array(glove_embeddings)\n",
    "\n",
    "        elif self.method == \"bert\":\n",
    "            # Use the pre-trained BERT model to generate embeddings\n",
    "            bert_embeddings = []\n",
    "            for doc in tqdm(texts, desc=\"Processing BERT\", unit=\"document\"):\n",
    "                bert_embeddings.append(self._get_bert_embedding(doc))\n",
    "            features = np.array(bert_embeddings)\n",
    "\n",
    "        # Apply dimensionality reduction if enabled\n",
    "        if self.reduce_dim and features is not None:\n",
    "            if self.reduce_dim == \"lda\" and labels is not None:\n",
    "                # features = self.vectorizer.transform(texts).toarray()\n",
    "                self.reducer.fit(features, labels)\n",
    "            elif self.reduce_dim == \"pca\":\n",
    "                # features = self.vectorizer.transform(texts).toarray()\n",
    "                self.reducer.fit(features)\n",
    "            \n",
    "            features = self.reducer.transform(features)\n",
    "\n",
    "        return features\n",
    "\n",
    "    def fit_transform(self, texts):\n",
    "        \"\"\"\n",
    "        Fits and transforms the text data by first fitting the model and then transforming it.\n",
    "\n",
    "        :param texts: list, raw text data\n",
    "        :return: transformed feature matrix\n",
    "        \"\"\"\n",
    "        self.fit(texts)  # First fit the model (compute parameters)\n",
    "        return self.transform(texts)  # Then transform the data using the fitted model\n",
    "    \n",
    "    def _save_model(self):\n",
    "        \"\"\"Saves the fitted vectorizer/scaler for later use.\"\"\"\n",
    "        # Ensure the directory exists\n",
    "        save_dir = self.save_dir if self.save_dir else \"data/processed\"\n",
    "        os.makedirs(save_dir, exist_ok=True)  # Create directory if it doesn't exist  \n",
    "        \n",
    "        if self.method in [\"tfidf\", \"count\", \"binary_count\"]:\n",
    "            file_path = os.path.join(self.save_dir, f\"{self.method}_vectorizer.pkl\")\n",
    "            with open(file_path, \"wb\") as f:\n",
    "                pickle.dump(self.vectorizer, f)\n",
    "        elif self.method in [\"word2vec\", \"glove\"]:\n",
    "            # Save the Word2Vec or GloVe model\n",
    "            file_path = os.path.join(self.save_dir, f\"{self.method}_model.pkl\")\n",
    "            with open(file_path, \"wb\") as f:\n",
    "                if self.method == \"word2vec\":\n",
    "                    pickle.dump(self.word2vec_model, f)\n",
    "                elif self.method == \"glove\":\n",
    "                    pickle.dump(self.glove_model, f)\n",
    "        elif self.method == \"bert\":\n",
    "            # Save the BERT tokenizer and model\n",
    "            tokenizer_path = os.path.join(self.save_dir, \"bert_tokenizer.pkl\")\n",
    "            model_path = os.path.join(self.save_dir, \"bert_model.pkl\")\n",
    "            with open(tokenizer_path, \"wb\") as f:\n",
    "                pickle.dump(self.tokenizer, f)\n",
    "            with open(model_path, \"wb\") as f:\n",
    "                pickle.dump(self.bert_model, f)\n",
    "                \n",
    "        if self.reduce_dim:\n",
    "            reducer_path = os.path.join(self.save_dir, f\"{self.reduce_dim}_reducer.pkl\")\n",
    "            with open(reducer_path, \"wb\") as f:\n",
    "                pickle.dump(self.reducer, f)\n",
    "    \n",
    "    def _load_model(self):\n",
    "        \"\"\"Loads the previously saved vectorizer/scaler.\"\"\"\n",
    "        # Ensure the directory exists\n",
    "        os.makedirs(self.save_dir, exist_ok=True)\n",
    "        \n",
    "        if self.method in [\"tfidf\", \"count\", \"binary_count\"]:\n",
    "            file_path = os.path.join(self.save_dir, f\"{self.method}_vectorizer.pkl\")\n",
    "            if not os.path.exists(file_path):\n",
    "                raise FileNotFoundError(f\"No saved model found at {file_path}. Run `fit_transform` first.\")\n",
    "            with open(file_path, \"rb\") as f:\n",
    "                self.vectorizer = pickle.load(f)\n",
    "        elif self.method in [\"word2vec\", \"glove\"]:\n",
    "            file_path = os.path.join(self.save_dir, f\"{self.method}_model.pkl\")\n",
    "            if not os.path.exists(file_path):\n",
    "                raise FileNotFoundError(f\"No saved model found at {file_path}. Run `fit_transform` first.\")\n",
    "            with open(file_path, \"rb\") as f:\n",
    "                self.word2vec_model = pickle.load(f)\n",
    "        elif self.method == \"bert\":\n",
    "            tokenizer_path = os.path.join(self.save_dir, \"bert_tokenizer.pkl\")\n",
    "            model_path = os.path.join(self.save_dir, \"bert_model.pkl\")\n",
    "            if not os.path.exists(tokenizer_path) or not os.path.exists(model_path):\n",
    "                raise FileNotFoundError(f\"No saved BERT model found at {tokenizer_path} or {model_path}. Run `fit_transform` first.\")\n",
    "            with open(tokenizer_path, \"rb\") as f:\n",
    "                self.tokenizer = pickle.load(f)\n",
    "            with open(model_path, \"rb\") as f:\n",
    "                self.bert_model = pickle.load(f)\n",
    "        \n",
    "        if self.reduce_dim:\n",
    "            reducer_path = os.path.join(self.save_dir, f\"{self.reduce_dim}_reducer.pkl\")\n",
    "            with open(reducer_path, \"rb\") as f:\n",
    "                self.reducer = pickle.load(f)\n",
    "\n",
    "def build_vector_for_text(df_sampled, feature_methods, project_root):\n",
    "    \"\"\"\n",
    "    Builds feature vectors for text data by extracting features using specified methods and splitting into train/test sets.\n",
    "    \n",
    "    This function processes a sampled DataFrame containing text data, applies various feature extraction methods,\n",
    "    and returns feature dictionaries for training and testing sets along with their corresponding target labels.\n",
    "    \n",
    "    :param df_sampled: pandas.DataFrame, sampled DataFrame with 'text_clean' and 'target' columns\n",
    "    :param feature_methods: list, list of feature extraction methods to apply (e.g., 'tfidf', 'word2vec')\n",
    "    :param project_root: str, path to the project root directory for saving processed data\n",
    "    :return: tuple, (X_train_features_dict, X_test_features_dict, y_train, y_test)\n",
    "             - X_train_features_dict: dict, dictionary mapping methods to training feature DataFrames\n",
    "             - X_test_features_dict: dict, dictionary mapping methods to testing feature DataFrames\n",
    "             - y_train: pandas.Series, target labels for the training set\n",
    "             - y_test: pandas.Series, target labels for the testing set\n",
    "    \"\"\"\n",
    "    X_train_features_dict = {}\n",
    "    X_test_features_dict = {}\n",
    "\n",
    "    # 🔹 Step 1: First, split the DataFrame before feature extraction (to maintain X-y matching)\n",
    "    df_train, df_test = train_test_split(df_sampled, test_size=0.2, random_state=42, stratify=df_sampled[\"target\"])\n",
    "\n",
    "    # Extract y_train and y_test **before feature extraction** to ensure data alignment\n",
    "    y_train = df_train[\"target\"].reset_index(drop=True)\n",
    "    y_test = df_test[\"target\"].reset_index(drop=True)\n",
    "\n",
    "    logger.log_message(\"\\n🔎 Running feature extraction...\\n\")\n",
    "    for method in tqdm(feature_methods, desc=\"Feature Extraction Progress\"):\n",
    "        logger.log_message(f\"\\n🔍 Processing feature extraction using: {method}...\")\n",
    "\n",
    "        try:\n",
    "            # Initialize FeatureBuilder for the current method\n",
    "            feature_builder = FeatureBuilder(\n",
    "                method=method,\n",
    "                save_dir=os.path.join(project_root, \"processed\"),\n",
    "                reduce_dim=None,\n",
    "                n_components=50\n",
    "            )\n",
    "\n",
    "            # 🔹 Step 2: Extract features separately for train and test sets\n",
    "            feature_builder.fit(df_sampled[\"text_clean\"].tolist())\n",
    "            X_train = feature_builder.transform(df_train[\"text_clean\"].tolist())\n",
    "            X_test = feature_builder.transform(df_test[\"text_clean\"].tolist()) \n",
    "\n",
    "            # Ensure feature matrices are DataFrames\n",
    "            X_train_features_dict[method] = pd.DataFrame(X_train)\n",
    "            X_test_features_dict[method] = pd.DataFrame(X_test)\n",
    "\n",
    "            logger.log_message(f\"✅ {method} - Train shape: {X_train.shape}, Test shape: {X_test.shape}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            logger.log_message(f\"❌ Error with {method}: {e}. Skipping this method.\")\n",
    "\n",
    "    return X_train_features_dict, X_test_features_dict, y_train, y_test\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_population(num_features, population_size):\n",
    "    \"\"\"Creates an initial population of binary feature selectors.\"\"\"\n",
    "    return np.random.randint(2, size=(population_size, num_features))\n",
    "\n",
    "def fitness_function(features, X_train, y_train):\n",
    "    \"\"\"Evaluates the fitness of a feature selection candidate.\"\"\"\n",
    "    selected_features = [i for i, f in enumerate(features) if f == 1]\n",
    "    if not selected_features:  # Avoid empty feature sets\n",
    "        return 0\n",
    "\n",
    "    X_train_selected = X_train[:, selected_features]\n",
    "\n",
    "    nb_model = GaussianNB(var_smoothing=1e-8)\n",
    "    try:\n",
    "        scores = cross_val_score(nb_model, X_train_selected, y_train, cv=5)\n",
    "        return np.mean(scores)\n",
    "    except ValueError as e:\n",
    "        logger.log_message(f\"Error during cross-validation: {e}\")\n",
    "        return 0\n",
    "\n",
    "def crossover(parent1, parent2):\n",
    "    \"\"\"Performs single-point crossover.\"\"\"\n",
    "    point = np.random.randint(1, len(parent1) - 1)\n",
    "    offspring1 = np.concatenate((parent1[:point], parent2[point:]))\n",
    "    offspring2 = np.concatenate((parent2[:point], parent1[point:]))\n",
    "    return offspring1, offspring2\n",
    "\n",
    "def mutate(individual, mutation_rate=0.1):\n",
    "    \"\"\"Mutates an individual with a given probability.\"\"\"\n",
    "    for i in range(len(individual)):\n",
    "        if np.random.rand() < mutation_rate:\n",
    "            individual[i] = 1 - individual[i]\n",
    "    return individual\n",
    "\n",
    "def genetic_algorithm(X_train, y_train, X_test, y_test, model_save_path=None, population_size=20, num_generations=100, mutation_rate=0.1, crossover_rate=0.7):\n",
    "    \"\"\"Runs a genetic algorithm to optimize feature selection for Naive Bayes.\"\"\"\n",
    "    # Check if the model already exists\n",
    "    if os.path.exists(model_save_path):\n",
    "        logger.log_message(f\"🔄 Loading existing model from {model_save_path}...\")\n",
    "        model_algorithm = joblib.load(model_save_path)\n",
    "        return model_algorithm\n",
    "    \n",
    "    num_features = X_train.shape[1]\n",
    "    population = create_population(num_features, population_size)\n",
    "\n",
    "    for generation in range(num_generations):\n",
    "        fitness_scores = [fitness_function(ind, X_train.values, y_train) for ind in population]\n",
    "        \n",
    "        # Normalize fitness scores to avoid division errors\n",
    "        fitness_scores = np.array(fitness_scores)\n",
    "        fitness_scores = np.clip(fitness_scores, 1e-5, None)\n",
    "        \n",
    "        probabilities = fitness_scores / np.sum(fitness_scores)\n",
    "\n",
    "        # Select parents based on probabilities\n",
    "        selected_indices = np.random.choice(np.arange(population_size), size=population_size, p=probabilities)\n",
    "        selected_parents = [population[idx] for idx in selected_indices]\n",
    "\n",
    "        next_generation = []\n",
    "        for j in range(0, population_size, 2):\n",
    "            if np.random.rand() < crossover_rate:\n",
    "                offspring1, offspring2 = crossover(selected_parents[j], selected_parents[j + 1])\n",
    "            else:\n",
    "                offspring1, offspring2 = selected_parents[j], selected_parents[j + 1]\n",
    "            \n",
    "            next_generation.append(mutate(offspring1, mutation_rate))\n",
    "            next_generation.append(mutate(offspring2, mutation_rate))\n",
    "\n",
    "        population = next_generation  # Move to the next generation\n",
    "\n",
    "    # Select the best individual\n",
    "    best_individual = population[np.argmax(fitness_scores)]\n",
    "    selected_features = [i for i, f in enumerate(best_individual) if f == 1]\n",
    "\n",
    "    X_train_selected = X_train.iloc[:, selected_features]\n",
    "    X_test_selected = X_test.iloc[:, selected_features]\n",
    "\n",
    "    logger.log_message(f\"Selected {len(selected_features)} features out of {num_features}\")\n",
    "\n",
    "    # Scaling the selected features\n",
    "    scaler = MinMaxScaler()\n",
    "    X_train_selected = scaler.fit_transform(X_train_selected)\n",
    "    X_test_selected = scaler.transform(X_test_selected)\n",
    "\n",
    "    # Train Naive Bayes with selected features\n",
    "    nb_model = GaussianNB()\n",
    "    nb_model.fit(X_train_selected, y_train)\n",
    "    y_pred = nb_model.predict(X_test_selected)\n",
    "    \n",
    "    # Compute metrics\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred, average='binary')\n",
    "    recall = recall_score(y_test, y_pred, average='binary')\n",
    "    f1 = f1_score(y_test, y_pred, average='binary')\n",
    "    \n",
    "    # ROC AUC can be computed if the model outputs probabilities\n",
    "    # Handle models that do not support `predict_proba`\n",
    "    if hasattr(nb_model, \"predict_proba\"):\n",
    "        logger.log_message(\"Has predict_proba\")\n",
    "        y_prob = nb_model.predict_proba(X_test_selected)[:, 1]\n",
    "        roc_auc = roc_auc_score(y_test, y_prob)\n",
    "    elif hasattr(nb_model, \"decision_function\"):\n",
    "        logger.log_message(\"Has decision_function\")\n",
    "        y_prob = nb_model.decision_function(X_test_selected)\n",
    "        roc_auc = roc_auc_score(y_test, y_prob)\n",
    "    else:\n",
    "        logger.log_message(\"Does not have predict_proba or decision_function\")\n",
    "        roc_auc = \"N/A\"  # Not applicable for models like Perceptron\n",
    "\n",
    "    # Print metrics\n",
    "    logger.log_message(f\"Accuracy: {accuracy:.4f}\")\n",
    "    logger.log_message(f\"Precision: {precision:.4f}\")\n",
    "    logger.log_message(f\"Recall: {recall:.4f}\")\n",
    "    logger.log_message(f\"F1 Score: {f1:.4f}\")\n",
    "    if hasattr(nb_model, \"predict_proba\") or hasattr(nb_model, \"decision_function\"):\n",
    "        logger.log_message(f\"ROC AUC: {roc_auc:.4f}\")\n",
    "    else:\n",
    "        logger.log_message(\"ROC AUC: N/A\")\n",
    "    \n",
    "    # Save the trained model\n",
    "    if model_save_path:\n",
    "        joblib.dump(nb_model, model_save_path)\n",
    "        logger.log_message(f'💾 Model saved to {model_save_path}')\n",
    "\n",
    "# --------------------------------------------------\n",
    "\n",
    "def generate_binary_classification_model(X, y, model_algorithm, hyperparameters, needs_scaled = False, model_save_path=\"best_model.pkl\", img_save_path=None, img_loss_path=None):\n",
    "    \"\"\"\n",
    "    Generating everything required for training and validation of a binary classification model\n",
    "\n",
    "    Args:\n",
    "        - X (Pandas DataFrame): Training features\n",
    "        - y (Pandas DataFrame): Target values\n",
    "        - model_algorithm (object): Model algorithm to train\n",
    "        - hyperparameters (dict): Hyperparameters for tuning\n",
    "        - needs_scaled (Boolean): Whether to scale the dataset\n",
    "        - model_save_path (str): Path to save the best model\n",
    "        - img_save_path (str): Path to save validation performance plot\n",
    "        - img_loss_path (str): Path to save training loss plot\n",
    "    \"\"\"\n",
    "    # Check if the model already exists\n",
    "    if os.path.exists(model_save_path):\n",
    "        logger.log_message(f\"🔄 Loading existing model from {model_save_path}...\")\n",
    "        model_algorithm = joblib.load(model_save_path)\n",
    "        return model_algorithm\n",
    "    \n",
    "    logger.log_message(f\"🚀 Training new model: {model_algorithm.__class__.__name__}...\")\n",
    "    # Performing a scaling on the data if required\n",
    "    if needs_scaled:\n",
    "        \n",
    "        # Instantiating the StandardScaler\n",
    "        scaler = StandardScaler()\n",
    "        \n",
    "        # Performing a fit_transform on the dataset\n",
    "        scaled_features = scaler.fit_transform(X)\n",
    "        \n",
    "        # Transforming the StandardScaler output back into a Pandas DataFrame\n",
    "        X = pd.DataFrame(scaled_features, index = X.index, columns = X.columns)\n",
    "        \n",
    "    # Instantiating a GridSearch object with the inputted model algorithm and hyperparameters\n",
    "    gridsearchcv = GridSearchCV(estimator = model_algorithm,\n",
    "                                param_grid = hyperparameters)\n",
    "    \n",
    "    # Fitting the training data to the GridSearch object\n",
    "    gridsearchcv.fit(X, y)\n",
    "    \n",
    "    # Printing out the best hyperparameters\n",
    "    logger.log_message(f'Best hyperparameters: {gridsearchcv.best_params_}')\n",
    "    \n",
    "    # Instantiating a new model object with the ideal hyperparameters from the GridSearch job\n",
    "    model_algorithm.set_params(**gridsearchcv.best_params_)\n",
    "    \n",
    "    # Creating a container to hold each set of validation metrics\n",
    "    accuracy_scores, roc_auc_scores, f1_scores, precision_scores, recall_scores = [], [], [], [], []\n",
    "    training_losses, validation_losses = [], []\n",
    "    \n",
    "    # Instantiating the K-Fold cross validation object\n",
    "    k_fold = KFold(n_splits = 5)\n",
    "    \n",
    "    logger.log_message(\"\\n🎯 Running K-Fold Cross-Validation...\")\n",
    "    for train_index, val_index in tqdm(k_fold.split(X), total=k_fold.get_n_splits(), desc=\"K-Fold Progress\"):\n",
    "\n",
    "        # Splitting the training set from the validation set for this specific fold\n",
    "        X_train, X_val = X.iloc[train_index, :], X.iloc[val_index, :]\n",
    "        y_train, y_val = y.iloc[train_index], y.iloc[val_index]\n",
    "        \n",
    "        # Fitting the X_train and y_train datasets to the model algorithm\n",
    "        model_algorithm.fit(X_train, y_train)\n",
    "        \n",
    "        # Compute losses\n",
    "        train_loss = get_training_loss(model_algorithm, X_train, y_train)\n",
    "        val_loss = get_training_loss(model_algorithm, X_val, y_val)\n",
    "\n",
    "        training_losses.append(train_loss)\n",
    "        validation_losses.append(val_loss)\n",
    "\n",
    "        # Getting inferential predictions for the validation dataset\n",
    "        val_preds = model_algorithm.predict(X_val)\n",
    "\n",
    "        # Generating validation metrics by comparing the inferential predictions (val_preds) to the actuals (y_val)\n",
    "        val_accuracy = accuracy_score(y_val, val_preds)\n",
    "        val_roc_auc_score = roc_auc_score(y_val, val_preds)\n",
    "        val_f1_score = f1_score(y_val, val_preds)\n",
    "        val_precision_score = precision_score(y_val, val_preds)\n",
    "        val_recall_score = recall_score(y_val, val_preds)\n",
    "        \n",
    "        # Appending the validation scores to the respective validation metric container\n",
    "        accuracy_scores.append(val_accuracy)\n",
    "        roc_auc_scores.append(val_roc_auc_score)\n",
    "        f1_scores.append(val_f1_score)\n",
    "        precision_scores.append(val_precision_score)\n",
    "        recall_scores.append(val_recall_score)\n",
    "        \n",
    "    # Print average validation scores\n",
    "    logger.log_message(f'📊 Average Accuracy: {int(mean(accuracy_scores) * 100)}%')\n",
    "    logger.log_message(f'📊 Average ROC AUC: {int(mean(roc_auc_scores) * 100)}%')\n",
    "    logger.log_message(f'📊 Average F1 Score: {int(mean(f1_scores) * 100)}%')\n",
    "    logger.log_message(f'📊 Average Precision: {int(mean(precision_scores) * 100)}%')\n",
    "    logger.log_message(f'📊 Average Recall: {int(mean(recall_scores) * 100)}%')\n",
    "    \n",
    "    # New added\n",
    "    model_algorithm.fit(X, y)\n",
    "\n",
    "    # Save the trained model\n",
    "    joblib.dump(model_algorithm, model_save_path)\n",
    "    logger.log_message(f'💾 Model saved to {model_save_path}')\n",
    "    \n",
    "    # If img_save_path is provided, plot the validation scores\n",
    "    if img_save_path:\n",
    "        plt.figure(figsize=(10, 6))\n",
    "\n",
    "        # Plot Accuracy\n",
    "        plt.plot(range(1, len(accuracy_scores) + 1), accuracy_scores, label=\"Accuracy\", marker='o')\n",
    "\n",
    "        # Plot ROC AUC\n",
    "        plt.plot(range(1, len(roc_auc_scores) + 1), roc_auc_scores, label=\"ROC AUC\", marker='o')\n",
    "\n",
    "        # Add labels and title\n",
    "        plt.title(\"Validation Performance Across K-Folds\")\n",
    "        plt.xlabel(\"Fold Number\")\n",
    "        plt.ylabel(\"Score\")\n",
    "        plt.legend()\n",
    "\n",
    "        # Save the plot to the specified path\n",
    "        plt.savefig(img_save_path)\n",
    "        plt.close()\n",
    "        logger.log_message(f\"📈 Plot saved to {img_save_path}\")\n",
    "        \n",
    "    # Plot loss curves\n",
    "    if img_loss_path:\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.plot(range(1, len(training_losses) + 1), training_losses, label=\"Training Loss\", marker='o')\n",
    "        plt.plot(range(1, len(validation_losses) + 1), validation_losses, label=\"Validation Loss\", marker='o')\n",
    "        plt.title(\"Training & Validation Loss Across K-Folds\")\n",
    "        plt.xlabel(\"Fold Number\")\n",
    "        plt.ylabel(\"Loss\")\n",
    "        plt.legend()\n",
    "        plt.savefig(img_loss_path)\n",
    "        plt.close()\n",
    "        logger.log_message(f\"📉 Loss plot saved to {img_loss_path}\")\n",
    "    \n",
    "    return model_algorithm\n",
    "\n",
    "def get_training_loss(model, X_train, y_train):\n",
    "    \"\"\"\n",
    "    Compute training loss based on model type.\n",
    "    \"\"\"\n",
    "    # Models that expose their loss during training\n",
    "    if hasattr(model, \"best_score_\"):  # XGBoost\n",
    "        return -model.best_score_\n",
    "\n",
    "    if hasattr(model, \"loss_\"):  # Perceptron (Hinge loss)\n",
    "        return model.loss_\n",
    "\n",
    "    # Probabilistic models (e.g., HMM, Naive Bayes)\n",
    "    if hasattr(model, \"score\"):  \n",
    "        return -model.score(X_train, y_train)  # Negative log-likelihood\n",
    "\n",
    "    # Support Vector Machines (hinge loss)\n",
    "    if isinstance(model, SVC):\n",
    "        y_pred = model.decision_function(X_train)\n",
    "        return np.mean(np.maximum(0, 1 - y_train * y_pred))  # Hinge loss\n",
    "\n",
    "    # Logistic Regression (log loss)\n",
    "    if isinstance(model, LogisticRegression):\n",
    "        y_proba = model.predict_proba(X_train)\n",
    "        return log_loss(y_train, y_proba)\n",
    "\n",
    "    # Decision Tree, Random Forest: No direct loss, use log loss\n",
    "    if isinstance(model, (DecisionTreeClassifier, RandomForestClassifier)):\n",
    "        y_proba = model.predict_proba(X_train)\n",
    "        return log_loss(y_train, y_proba)\n",
    "\n",
    "    return None  # Loss not available\n",
    "\n",
    "# --------------------------------------------------\n",
    "def train_bayes_net(df, model_save_path):\n",
    "    \"\"\"\n",
    "    Trains a Bayesian Network model for sentiment classification or loads an existing model from disk.\n",
    "    \n",
    "    This function either loads a pre-trained Bayesian Network model from the specified path or trains a new one\n",
    "    using text data from a DataFrame. It builds the network structure, fits conditional probability tables (CPTs),\n",
    "    evaluates the model on a test set, and logs performance metrics like accuracy, precision, recall, F1 score,\n",
    "    and ROC AUC (if applicable).\n",
    "    \n",
    "    :param df: pandas.DataFrame, DataFrame containing 'text_clean' (cleaned text) and 'target' (sentiment labels) columns\n",
    "    :param model_save_path: str, path to save or load the trained model\n",
    "    :return: None, logs model performance metrics to the logger\n",
    "    \"\"\"\n",
    "    if os.path.exists(model_save_path):\n",
    "        logger.log_message(\"✅ Model found! Loading...\")\n",
    "        # reader = BIFReader(model_save_path)\n",
    "        # model = reader.get_model()\n",
    "        logger.log_message(\"✅ Model loaded successfully!\")\n",
    "    else:  \n",
    "        df_sampled = df\n",
    "        \n",
    "        vectorizer = CountVectorizer(binary=True, max_features=100) \n",
    "        X = vectorizer.fit_transform(df_sampled['text_clean']).toarray()\n",
    "        y = df_sampled['target'].values\n",
    "        \n",
    "        # Chia dữ liệu thành tập huấn luyện và kiểm tra\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "        \n",
    "        # Chuyển đổi thành DataFrame để sử dụng với pgmpy\n",
    "        feature_names = vectorizer.get_feature_names_out()\n",
    "        train_df = pd.DataFrame(X_train, columns=feature_names)\n",
    "        train_df['target'] = y_train\n",
    "        \n",
    "        # Xây dựng cấu trúc Bayesian Network\n",
    "        # Giả sử mỗi từ phụ thuộc vào 'Sentiment'\n",
    "        edges = [('target', word) for word in feature_names]\n",
    "        model = BayesianNetwork(edges)\n",
    "        \n",
    "        # Học các bảng xác suất có điều kiện (CPT) từ dữ liệu\n",
    "        model.fit(train_df, estimator=MaximumLikelihoodEstimator)\n",
    "\n",
    "        # Suy luận và đánh giá mô hình\n",
    "        inference = VariableElimination(model)\n",
    "        \n",
    "        # joblib.dump(model, model_save_path)\n",
    "        # with open(model_save_path, \"w\") as f:\n",
    "        #     f.write(model.to_bif())\n",
    "        \n",
    "        # logger.log_message(f'💾 Model saved to {model_save_path}')\n",
    "\n",
    "        # Hàm dự đoán sentiment cho tập dữ liệu\n",
    "        def predict_sentiment(model, inference, X, feature_names):\n",
    "            predictions = []\n",
    "            for i in range(X.shape[0]):\n",
    "                evidence = {feature_names[j]: X[i, j] for j in range(len(feature_names))}\n",
    "                result = inference.map_query(variables=['target'], evidence=evidence)\n",
    "                predictions.append(result['target'])\n",
    "            return np.array(predictions)\n",
    "\n",
    "        def predict_sentiment_proba(model, inference, X, feature_names):\n",
    "            proba_predictions = []\n",
    "            for i in range(X.shape[0]):\n",
    "                evidence = {feature_names[j]: X[i, j] for j in range(len(feature_names))}\n",
    "                result = inference.query(variables=['target'], evidence=evidence)\n",
    "                \n",
    "                # Extract probability of target = 1 (assuming binary classification: 0 or 1)\n",
    "                prob_1 = result.values[1]  # Probabilities are stored as an array, index 1 corresponds to class 1\n",
    "                proba_predictions.append(prob_1)\n",
    "            return np.array(proba_predictions)\n",
    "        \n",
    "        # Dự đoán trên tập kiểm tra\n",
    "        y_pred = predict_sentiment(model, inference, X_test, feature_names)\n",
    "\n",
    "        # Đánh giá độ chính xác\n",
    "        # Compute metrics\n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "        precision = precision_score(y_test, y_pred, average='binary')\n",
    "        recall = recall_score(y_test, y_pred, average='binary')\n",
    "        f1 = f1_score(y_test, y_pred, average='binary')\n",
    "        \n",
    "        # ROC AUC can be computed if the model outputs probabilities\n",
    "        # Handle models that do not support `predict_proba`\n",
    "        if hasattr(model, \"predict_proba\"):\n",
    "            logger.log_message(\"Has predict_proba\")\n",
    "            y_prob = model.predict_proba(X_test)[:, 1]  # Take the positive class probabilities\n",
    "            roc_auc = roc_auc_score(y_test, y_prob)\n",
    "        elif hasattr(model, \"decision_function\"):\n",
    "            logger.log_message(\"Has decision_function\")\n",
    "            y_prob = model.decision_function(X_test)\n",
    "            roc_auc = roc_auc_score(y_test, y_prob)\n",
    "        else:\n",
    "            logger.log_message(\"Does not have predict_proba or decision_function\")\n",
    "            y_proba = predict_sentiment_proba(model, inference, X_test, feature_names)\n",
    "            roc_auc = roc_auc_score(y_test, y_proba)\n",
    "            # roc_auc = \"N/A\"  # Not applicable for models like Perceptron\n",
    "\n",
    "        # Print metrics\n",
    "        logger.log_message(\"Model: Bayesian Network\")\n",
    "        logger.log_message(\"-\" * 50)\n",
    "        logger.log_message(f\"Accuracy: {accuracy:.4f}\")\n",
    "        logger.log_message(f\"Precision: {precision:.4f}\")\n",
    "        logger.log_message(f\"Recall: {recall:.4f}\")\n",
    "        logger.log_message(f\"F1 Score: {f1:.4f}\")\n",
    "        if hasattr(model, \"predict_proba\") or hasattr(model, \"decision_function\"):\n",
    "            logger.log_message(f\"ROC AUC: {roc_auc:.4f}\")\n",
    "        else:\n",
    "            if roc_auc != \"N/A\":\n",
    "                logger.log_message(f\"ROC AUC: {roc_auc:.4f}\")\n",
    "            else:\n",
    "                logger.log_message(\"ROC AUC: N/A\")\n",
    "\n",
    "def extract_features(text, word_features):\n",
    "    \"\"\"\n",
    "    Extracts feature indices from text based on a predefined list of word features.\n",
    "    \n",
    "    :param text: str, input text to process\n",
    "    :param word_features: list, list of unique words used as features\n",
    "    :return: numpy.ndarray, array of indices corresponding to words found in word_features\n",
    "    \"\"\"\n",
    "    words = text.split()  # Chuyển văn bản thành danh sách từ\n",
    "    return np.array([word_features.index(word) for word in words if word in word_features])\n",
    "\n",
    "def pad_sequence(seq, max_len):\n",
    "    \"\"\"\n",
    "    Pads or truncates a sequence to a fixed length.\n",
    "    \n",
    "    :param seq: numpy.ndarray, input sequence to pad or truncate\n",
    "    :param max_len: int, desired length of the output sequence\n",
    "    :return: numpy.ndarray, padded or truncated sequence\n",
    "    \"\"\"\n",
    "    if len(seq) >= max_len:\n",
    "        return seq[:max_len]\n",
    "    return np.pad(seq, (0, max_len - len(seq)), mode='constant', constant_values=0)\n",
    "\n",
    "def train_hmm(df, model_save_path):\n",
    "    \"\"\"\n",
    "    Trains a Hidden Markov Model (HMM) for sentiment classification using text data.\n",
    "    \n",
    "    This function preprocesses text data into sequences of feature indices, pads them to a fixed length,\n",
    "    trains a Gaussian HMM, saves the model, and evaluates its performance on a test set with metrics\n",
    "    like accuracy, precision, recall, F1 score, and ROC AUC (if applicable).\n",
    "    \n",
    "    :param df: pandas.DataFrame, DataFrame with 'text_clean' (cleaned text) and 'target' (sentiment labels) columns\n",
    "    :param model_save_path: str, path to save the trained HMM model\n",
    "    :return: None, logs model performance metrics to the logger\n",
    "    \"\"\"\n",
    "    df_sampled = df\n",
    "    \n",
    "    # Tạo tập từ vựng (chỉ lấy 3000 từ phổ biến nhất)\n",
    "    all_words = nltk.FreqDist(word.lower() for text in df_sampled[\"text_clean\"] for word in text.split())\n",
    "    word_features = list(all_words.keys())[:5000]  # Lấy 5000 từ phổ biến nhất\n",
    "\n",
    "    # Chuyển đổi dữ liệu text thành dạng số\n",
    "    X = [extract_features(text, word_features) for text in df_sampled[\"text_clean\"]]\n",
    "    y = df_sampled[\"target\"].values  # Nhãn (0: negative, 1: positive)\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "    \n",
    "    # HMM yêu cầu chuỗi có độ dài giống nhau -> Padding độ dài cố định (50 từ)\n",
    "    max_len = 50\n",
    "    \n",
    "    X_train = np.array([pad_sequence(seq, max_len) for seq in X_train])\n",
    "    X_test = np.array([pad_sequence(seq, max_len) for seq in X_test])\n",
    "\n",
    "    # Huấn luyện HMM cho từng class (pos và neg)\n",
    "    hmm = hmmlearn.hmm.GaussianHMM(n_components=2, covariance_type=\"diag\", n_iter=100)\n",
    "    hmm.fit(X_train)\n",
    "    \n",
    "    joblib.dump(hmm, model_save_path)\n",
    "    logger.log_message(f'💾 Model saved to {model_save_path}')\n",
    "    \n",
    "    y_pred = hmm.predict(X_test)\n",
    "    \n",
    "    # Compute metrics\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred, average='binary')\n",
    "    recall = recall_score(y_test, y_pred, average='binary')\n",
    "    f1 = f1_score(y_test, y_pred, average='binary')\n",
    "    \n",
    "    # ROC AUC can be computed if the model outputs probabilities\n",
    "    # Handle models that do not support `predict_proba`\n",
    "    if hasattr(hmm, \"predict_proba\"):\n",
    "        logger.log_message(\"Has predict_proba\")\n",
    "        y_prob = hmm.predict_proba(X_test)[:, 1]  # Take the positive class probabilities\n",
    "        roc_auc = roc_auc_score(y_test, y_prob)\n",
    "    elif hasattr(hmm, \"decision_function\"):\n",
    "        logger.log_message(\"Has decision_function\")\n",
    "        y_prob = hmm.decision_function(X_test)\n",
    "        roc_auc = roc_auc_score(y_test, y_prob)\n",
    "    else:\n",
    "        logger.log_message(\"Does not have predict_proba or decision_function\")\n",
    "        roc_auc = \"N/A\"  # Not applicable for models like Perceptron\n",
    "\n",
    "    # Print metrics\n",
    "    logger.log_message(\"Model: HMM\")\n",
    "    logger.log_message(\"-\" * 50)\n",
    "    logger.log_message(f\"Accuracy: {accuracy:.4f}\")\n",
    "    logger.log_message(f\"Precision: {precision:.4f}\")\n",
    "    logger.log_message(f\"Recall: {recall:.4f}\")\n",
    "    logger.log_message(f\"F1 Score: {f1:.4f}\")\n",
    "    if hasattr(hmm, \"predict_proba\") or hasattr(hmm, \"decision_function\"):\n",
    "        logger.log_message(f\"ROC AUC: {roc_auc:.4f}\")\n",
    "    else:\n",
    "        logger.log_message(\"ROC AUC: N/A\")\n",
    "    \n",
    "def train_graphical_model(df, model_name, model_save_path):\n",
    "    \"\"\"\n",
    "    Trains a graphical model (HMM or Bayesian Network) based on the specified model name.\n",
    "    \n",
    "    :param df: pandas.DataFrame, DataFrame with 'text_clean' and 'target' columns\n",
    "    :param model_name: str, name of the model to train ('hmm' or 'bayesnet')\n",
    "    :param model_save_path: str, path to save the trained model\n",
    "    :return: None, delegates training to the appropriate function\n",
    "    \"\"\"\n",
    "    if model_name == \"hmm\":\n",
    "        train_hmm(df, model_save_path)\n",
    "    elif model_name == \"bayesnet\":\n",
    "        train_bayes_net(df, model_save_path)\n",
    "\n",
    "# --------------------------------------------------\n",
    "\n",
    "def train_cnn_lstm(texts, labels, vocab_size=10000, max_length=500, embedding_dim=300, num_trials=10, epochs=20):\n",
    "    \"\"\"\n",
    "    Trains a CNN-LSTM sentiment analysis model on given text data.\n",
    "\n",
    "    Parameters:\n",
    "        texts (list): List of sentences (raw text).\n",
    "        labels (list): List of binary sentiment labels (0 for negative, 1 for positive).\n",
    "        vocab_size (int): Size of vocabulary for tokenization.\n",
    "        max_length (int): Maximum sequence length for padding.\n",
    "        embedding_dim (int): Dimension of the word embedding layer.\n",
    "        num_trials (int): Number of hyperparameter tuning trials.\n",
    "        epochs (int): Number of training epochs.\n",
    "\n",
    "    Returns:\n",
    "        Trained Keras model with the best hyperparameters.\n",
    "    \"\"\"\n",
    "    # **Step 1: Text Preprocessing**\n",
    "    tokenizer = Tokenizer(num_words=vocab_size, oov_token=\"<OOV>\")\n",
    "    tokenizer.fit_on_texts(texts)\n",
    "    sequences = tokenizer.texts_to_sequences(texts)\n",
    "    X_data = pad_sequences(sequences, maxlen=max_length, padding=\"pre\")\n",
    "    y_data = np.array(labels)  # Convert labels to NumPy array\n",
    "\n",
    "    # **Step 2: Split Data for Training & Testing**\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_data, y_data, test_size=0.2, random_state=42)\n",
    "\n",
    "    # **Step 3: Build Model Function**\n",
    "    def build_model(hp):\n",
    "        \"\"\"\n",
    "        Builds a CNN-LSTM hybrid model with tunable hyperparameters for binary classification.\n",
    "        \n",
    "        This function constructs a Keras Sequential model with an embedding layer, two CNN blocks \n",
    "        (Conv1D, BatchNormalization, MaxPooling1D), a Bidirectional LSTM layer, and dense layers. \n",
    "        Hyperparameters such as filter sizes, kernel sizes, LSTM units, dense units, dropout rate, \n",
    "        and learning rate are tuned using the Keras Tuner HyperParameters object.\n",
    "        \n",
    "        :param hp: keras.tuner.HyperParameters, object containing hyperparameter choices for tuning\n",
    "        :return: keras.Model, compiled CNN-LSTM model ready for training\n",
    "        \"\"\"\n",
    "        model = keras.Sequential()\n",
    "\n",
    "        # **Embedding Layer**\n",
    "        model.add(layers.Embedding(\n",
    "            input_dim=vocab_size, \n",
    "            output_dim=embedding_dim, \n",
    "            input_length=max_length\n",
    "        ))\n",
    "\n",
    "        # **CNN Block 1**\n",
    "        model.add(layers.Conv1D(\n",
    "            filters=hp.Int('filters_1', min_value=64, max_value=256, step=64),\n",
    "            kernel_size=hp.Choice('kernel_size_1', values=[3, 5, 7]),\n",
    "            activation=\"relu\",\n",
    "            padding=\"same\"\n",
    "        ))\n",
    "        model.add(layers.BatchNormalization())\n",
    "        model.add(layers.MaxPooling1D(pool_size=2))\n",
    "\n",
    "        # **CNN Block 2**\n",
    "        model.add(layers.Conv1D(\n",
    "            filters=hp.Int('filters_2', min_value=128, max_value=512, step=128),\n",
    "            kernel_size=hp.Choice('kernel_size_2', values=[3, 5]),\n",
    "            activation=\"relu\",\n",
    "            padding=\"same\"\n",
    "        ))\n",
    "        model.add(layers.BatchNormalization())\n",
    "        model.add(layers.MaxPooling1D(pool_size=2))\n",
    "\n",
    "        # **Bidirectional LSTM Layer**\n",
    "        model.add(layers.Bidirectional(layers.LSTM(\n",
    "            units=hp.Int('lstm_units', min_value=64, max_value=256, step=64),\n",
    "            activation=\"tanh\",\n",
    "            return_sequences=False\n",
    "        )))\n",
    "\n",
    "        # **Fully Connected Layer**\n",
    "        model.add(layers.Dense(\n",
    "            units=hp.Int('dense_units', min_value=128, max_value=512, step=128),\n",
    "            activation=\"relu\"\n",
    "        ))\n",
    "        model.add(layers.Dropout(rate=hp.Float('dropout', min_value=0.3, max_value=0.6, step=0.1)))\n",
    "\n",
    "        # **Output Layer**\n",
    "        model.add(layers.Dense(1, activation=\"sigmoid\"))\n",
    "\n",
    "        # **Compile Model**\n",
    "        model.compile(\n",
    "            optimizer=keras.optimizers.Adam(hp.Choice('learning_rate', values=[5e-4, 1e-4, 5e-5, 1e-5])),\n",
    "            loss=\"binary_crossentropy\",\n",
    "            metrics=[\"accuracy\"]\n",
    "        )\n",
    "\n",
    "        return model\n",
    "\n",
    "    # **Step 4: Initialize Keras Tuner**\n",
    "    tuner = kt.RandomSearch(\n",
    "        build_model,\n",
    "        objective=\"val_accuracy\",\n",
    "        max_trials=num_trials,\n",
    "        executions_per_trial=1,\n",
    "        directory=\"tuner_results\",\n",
    "        project_name=\"cnn_lstm_tuning\"\n",
    "    )\n",
    "\n",
    "    logger.log_message(\"\\n🔍 Running Hyperparameter Tuning...\")\n",
    "    tuner.search(X_train, y_train, epochs=5, validation_data=(X_test, y_test), batch_size=32, validation_split=0.2, verbose=1)\n",
    "\n",
    "    # **Step 5: Retrieve Best Model**\n",
    "    best_hps = tuner.get_best_hyperparameters(num_trials=1)[0]\n",
    "    best_model = tuner.hypermodel.build(best_hps)\n",
    "\n",
    "    # **Step 6: Final Training with Best Model**\n",
    "    logger.log_message(\"\\n🚀 Training Final Model...\")\n",
    "    history = best_model.fit(X_train, y_train, epochs=epochs, validation_data=(X_test, y_test), batch_size=32, validation_split=0.2, verbose=1)\n",
    "    \n",
    "    # **Step 7: Predict on Validation Set**\n",
    "    y_pred_prob = best_model.predict(X_test)\n",
    "    y_pred = (y_pred_prob > 0.5).astype(int)\n",
    "\n",
    "    # **Step 8: Compute Metrics**\n",
    "    precision = precision_score(y_test, y_pred)\n",
    "    recall = recall_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "    roc_auc = roc_auc_score(y_test, y_pred_prob)\n",
    "\n",
    "    # **Step 9: Store Results**\n",
    "    results = {\n",
    "        \"loss\": history.history[\"loss\"],\n",
    "        \"val_loss\": history.history[\"val_loss\"],\n",
    "        \"accuracy\": history.history[\"accuracy\"],\n",
    "        \"val_accuracy\": history.history[\"val_accuracy\"],\n",
    "        \"precision\": precision,\n",
    "        \"recall\": recall,\n",
    "        \"f1_score\": f1,\n",
    "        \"roc_auc\": roc_auc\n",
    "    }\n",
    "\n",
    "    logger.log_message(f'🔹 loss: {history.history[\"loss\"][-1]}')\n",
    "    logger.log_message(f'🔹 val_loss: {history.history[\"val_loss\"][-1]}')\n",
    "    logger.log_message(f'🔹 accuracy: {history.history[\"accuracy\"][-1]}')\n",
    "    logger.log_message(f'🔹 val_accuracy: {history.history[\"val_accuracy\"][-1]}')\n",
    "    logger.log_message(f'🔹 precision: {precision}')\n",
    "    logger.log_message(f'🔹 recall: {recall}')\n",
    "    logger.log_message(f'🔹 f1_score: {f1}')\n",
    "    logger.log_message(f'🔹 roc_auc: {roc_auc}')\n",
    "\n",
    "    # **Step 10: Save the Best Model**\n",
    "    best_model.save(\"best_cnn_lstm.keras\")\n",
    "\n",
    "    logger.log_message(\"\\n✅ Model Training and Save Complete!\")\n",
    "    \n",
    "    return best_model, results\n",
    "\n",
    "# --------------------------------------------------\n",
    "\n",
    "def train_general_model(df, doc_lst, label_lst, model_name_lst, feature_methods, model_dict, param_dict, X_train_features_dict, X_test_features_dict, y_train, y_test):\n",
    "    \"\"\"\n",
    "    Trains multiple machine learning models using various feature extraction methods and logs the process.\n",
    "    \n",
    "    This function iterates over a list of model names and feature extraction methods, training either \n",
    "    specialized models (CNN, LSTM, HMM, BayesNet) or general models (including Genetic Algorithm) \n",
    "    based on the provided data and configurations. It handles exceptions and logs progress and errors.\n",
    "    \n",
    "    :param df: pandas.DataFrame, DataFrame containing text data (used for graphical models)\n",
    "    :param doc_lst: list, list of text documents (used for CNN/LSTM models)\n",
    "    :param label_lst: list, list of corresponding labels (used for CNN/LSTM models)\n",
    "    :param model_name_lst: list, list of model names to train (e.g., 'cnn', 'lstm', 'hmm', 'bayesnet', 'GA', etc.)\n",
    "    :param feature_methods: list, list of feature extraction methods (e.g., 'tfidf', 'word2vec')\n",
    "    :param model_dict: dict, dictionary mapping model names to their corresponding class/function\n",
    "    :param param_dict: dict, dictionary mapping model names to their hyperparameter settings\n",
    "    :param X_train_features_dict: dict, dictionary of training feature matrices for each method\n",
    "    :param X_test_features_dict: dict, dictionary of testing feature matrices for each method\n",
    "    :param y_train: numpy.ndarray or pandas.Series, training labels\n",
    "    :param y_test: numpy.ndarray or pandas.Series, testing labels\n",
    "    :return: None, logs training progress and results\n",
    "    \"\"\"\n",
    "    logger.log_message(\"\\n🔎 Running feature extraction and model training loop...\\n\")\n",
    "    \n",
    "    for model_name in model_name_lst:\n",
    "        logger.log_message(f\"\\n🚀 Training {model_name} models...\\n\")\n",
    "\n",
    "        try:\n",
    "            if model_name == \"cnn\" or model_name == \"lstm\":\n",
    "                train_cnn_lstm(doc_lst, label_lst)\n",
    "                \n",
    "            elif model_name == \"hmm\" or model_name == \"bayesnet\":\n",
    "                train_graphical_model(\n",
    "                    df, \n",
    "                    model_name, \n",
    "                    model_save_path=f\"best_{model_name}.pkl\"\n",
    "                )\n",
    "                \n",
    "            else:\n",
    "                for method in feature_methods:\n",
    "                    logger.log_message(f\"🔎 Training with Method: {method}...\")\n",
    "                    \n",
    "                    if model_name == \"GA\":\n",
    "                        genetic_algorithm(\n",
    "                            X_train_features_dict[method], \n",
    "                            y_train, \n",
    "                            X_test_features_dict[method], \n",
    "                            y_test, \n",
    "                            model_save_path=f\"best_{model_name}_{method}.pkl\"\n",
    "                        )\n",
    "                    \n",
    "                    else:\n",
    "                        model_api = model_dict[model_name]()\n",
    "                        model_params = param_dict[model_name]\n",
    "                        \n",
    "                        generate_binary_classification_model(\n",
    "                            X=X_train_features_dict[method], \n",
    "                            y=y_train, \n",
    "                            model_algorithm=model_api, \n",
    "                            hyperparameters=model_params, \n",
    "                            needs_scaled=False, \n",
    "                            model_save_path=f\"best_{model_name}_{method}.pkl\",\n",
    "                            img_save_path=f\"best_{model_name}_{method}.png\",\n",
    "                            img_loss_path=f\"best_{model_name}_{method}_loss.png\"\n",
    "                        )\n",
    "                        \n",
    "        except Exception as e:\n",
    "            logger.log_message(f\"❌ Error with {method}: {e}\")\n",
    "\n",
    "# --------------------------------------------------  \n",
    "\n",
    "def predict_general_model(model_names, feature_methods, X_test_features_dict, y_test, output_dir):# Predict for each model\n",
    "    \"\"\"\n",
    "    Makes predictions using pre-trained models and evaluates their performance on test data.\n",
    "    \n",
    "    This function iterates over a list of model names and feature extraction methods, loads the \n",
    "    corresponding pre-trained models, generates predictions on the test set, and computes \n",
    "    performance metrics (accuracy, precision, recall, F1 score, and ROC AUC if applicable). \n",
    "    It handles both deep learning models (e.g., CNN) and traditional machine learning models, \n",
    "    skipping models that were already tested during training (e.g., GA, HMM, BayesNet, LSTM).\n",
    "    \n",
    "    :param model_names: list, list of model names to predict with (e.g., 'cnn', 'GA', 'hmm', etc.)\n",
    "    :param feature_methods: list, list of feature extraction methods (e.g., 'tfidf', 'word2vec')\n",
    "    :param X_test_features_dict: dict, dictionary of test feature matrices for each method\n",
    "    :param y_test: numpy.ndarray or pandas.Series, true labels for the test set\n",
    "    :param output_dir: str, directory path where pre-trained models are stored\n",
    "    :return: None, logs prediction results and performance metrics\n",
    "    \"\"\"\n",
    "    for model_name in model_names:\n",
    "        if model_name in [\"GA\", \"hmm\", \"bayesnet\", \"lstm\"]:\n",
    "            logger.log_message(f\"Already trained and tested model: {model_name}\")\n",
    "            continue\n",
    "        \n",
    "        for method in feature_methods:\n",
    "            logger.log_message(f\"🔎 Predicting with Model: {model_name}, Method: {method}...\")\n",
    "            \n",
    "            try:\n",
    "                if model_name in [\"cnn\"]:\n",
    "                    # Load the saved deep learning model\n",
    "                    model_filename = os.path.join(output_dir, f\"best_{model_name}.keras\")\n",
    "                    model = tf.keras.models.load_model(model_filename)\n",
    "\n",
    "                    # Retrieve and reshape features for CNN/LSTM\n",
    "                    X_test_features = np.array(X_test_features_dict[method])\n",
    "                    if model_name == \"lstm\":\n",
    "                        input_shape = (1, X_test_features.shape[1])\n",
    "                        X_test_features = X_test_features.reshape(X_test_features.shape[0], *input_shape)\n",
    "                    else:\n",
    "                        input_shape = (X_test_features.shape[1], 1)\n",
    "                        X_test_features = X_test_features.reshape(-1, X_test_features.shape[1], 1)\n",
    "\n",
    "                    # Make predictions\n",
    "                    y_prob = model.predict(X_test_features).flatten()\n",
    "                    y_pred = (y_prob > 0.5).astype(int)\n",
    "\n",
    "                else:  # Handle Machine Learning models\n",
    "                    # Load the saved model\n",
    "                    model_filename = os.path.join(output_dir, f\"best_{model_name}_{method}.pkl\")\n",
    "                    with open(model_filename, 'rb') as model_file:\n",
    "                        model = joblib.load(model_file)\n",
    "\n",
    "                    # Make predictions\n",
    "                    y_pred = model.predict(X_test_features_dict[method])\n",
    "                    \n",
    "                    # ROC AUC can be computed if the model outputs probabilities\n",
    "                    # Handle models that do not support `predict_proba`\n",
    "                    if hasattr(model, \"predict_proba\"):\n",
    "                        y_prob = model.predict_proba(X_test_features_dict[method])[:, 1]  # Take the positive class probabilities\n",
    "                    elif hasattr(model, \"decision_function\"):\n",
    "                        y_prob = model.decision_function(X_test_features_dict[method])\n",
    "                    else:\n",
    "                        y_prob = None\n",
    "\n",
    "                # Compute metrics\n",
    "                accuracy = accuracy_score(y_test, y_pred)\n",
    "                precision = precision_score(y_test, y_pred, average='binary')\n",
    "                recall = recall_score(y_test, y_pred, average='binary')\n",
    "                f1 = f1_score(y_test, y_pred, average='binary')\n",
    "                roc_auc = roc_auc_score(y_test, y_prob) if y_prob is not None else \"N/A\"\n",
    "\n",
    "                logger.log_message(f\"Model: {model_name}\")\n",
    "                logger.log_message(f\"Method: {method}\")\n",
    "                logger.log_message(\"-\" * 50)\n",
    "                logger.log_message(f\"Accuracy: {accuracy:.4f}\")\n",
    "                logger.log_message(f\"Precision: {precision:.4f}\")\n",
    "                logger.log_message(f\"Recall: {recall:.4f}\")\n",
    "                logger.log_message(f\"F1 Score: {f1:.4f}\")\n",
    "                logger.log_message(f\"ROC AUC: {roc_auc if roc_auc != 'N/A' else 'N/A'}\")\n",
    "                    \n",
    "            except Exception as e:\n",
    "                logger.log_message(f\"❌ Error while predicting for {model_name} with {method}: {e}\")\n",
    "\n",
    "            \n",
    "        logger.log_message(\"%\" * 50)\n",
    "        logger.log_message(\"%\" * 50)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Do here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-04T11:15:08.793700Z",
     "iopub.status.busy": "2025-02-04T11:15:08.793389Z",
     "iopub.status.idle": "2025-02-04T11:15:08.814721Z",
     "shell.execute_reply": "2025-02-04T11:15:08.813810Z",
     "shell.execute_reply.started": "2025-02-04T11:15:08.793679Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Dictionary for models\n",
    "MODEL_DICT = {\n",
    "    \"decision_tree\": DecisionTreeClassifier,\n",
    "    \"perceptron\": Perceptron,\n",
    "    \"mlp\": MLPClassifier,\n",
    "    \"bayesian\": GaussianNB,\n",
    "    \"random_forest\": RandomForestClassifier,\n",
    "    \"xgboost\": xgb.XGBClassifier,\n",
    "    \"svm\": SVC,\n",
    "    \"logistic_regression\": LogisticRegression\n",
    "} \n",
    "\n",
    "# Dictionary for model parameters\n",
    "MODEL_PARAMS = {\n",
    "    \"decision_tree\": {\n",
    "        \"criterion\": [\"gini\", \"entropy\"],\n",
    "        \"max_depth\": [10, 20, 30, 40],\n",
    "        \"min_samples_split\": [2, 5, 10],\n",
    "        \"min_samples_leaf\": [1, 2, 4],\n",
    "        \"max_features\": [\"sqrt\", \"log2\"]\n",
    "    },\n",
    "    \n",
    "    \"perceptron\": {\n",
    "        \"max_iter\": [1000, 2000],\n",
    "        \"tol\": [1e-3, 1e-4],\n",
    "        \"eta0\": [0.001, 0.01, 0.1],\n",
    "        \"penalty\": [None, \"l2\", \"l1\"],\n",
    "        \"alpha\": [0.0001, 0.001, 0.01]\n",
    "    },\n",
    "    \n",
    "    \"mlp\": {\n",
    "        \"hidden_layer_sizes\": [(100,)],\n",
    "        \"activation\": [\"tanh\", \"logistic\"],\n",
    "        \"solver\": [\"sgd\"],\n",
    "        \"alpha\": [0.001, 0.01],\n",
    "        \"batch_size\": [32],\n",
    "        \"max_iter\": [1000],\n",
    "    },\n",
    "    \n",
    "    \"bayesian\": {\n",
    "        \"priors\": [None, [0.5, 0.5], [0.4, 0.6], [0.3, 0.7], [0.2, 0.8], [0.1, 0.9], [0.05, 0.95]],\n",
    "        \"var_smoothing\": [1e-9, 1e-8, 1e-7]\n",
    "    },\n",
    "    \n",
    "    \"random_forest\": {\n",
    "        \"n_estimators\": [100],\n",
    "        \"max_depth\": [10],\n",
    "        \"min_samples_split\": [2, 5],\n",
    "        \"min_samples_leaf\": [1, 2],\n",
    "        \"max_features\": [\"sqrt\", \"log2\"]\n",
    "    },\n",
    "    \n",
    "    \"xgboost\": {\n",
    "        \"n_estimators\": [100, 150],\n",
    "        \"learning_rate\": [0.001, 0.01, 0.1],\n",
    "        \"max_depth\": [10, 15]\n",
    "    },\n",
    "    \n",
    "    \"svm\": {\n",
    "        \"kernel\": [\"linear\"],\n",
    "        \"C\": [0.1, 1],\n",
    "        \"gamma\": [\"scale\", \"auto\"]\n",
    "    },\n",
    "    \n",
    "    \"logistic_regression\": {\n",
    "        \"penalty\": [\"l1\", \"l2\", \"elasticnet\", None],\n",
    "        \"C\": [0.1, 1.0, 10.0],\n",
    "        \"max_iter\": [1000, 2000]\n",
    "    }\n",
    "}\n",
    "\n",
    "# Dictionary for dimensionality reduction methods\n",
    "DIMENSIONALITY_REDUCTION_DICT = {\n",
    "    \"pca\": PCA,\n",
    "    \"lda\": LDA,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-04T11:15:08.817756Z",
     "iopub.status.busy": "2025-02-04T11:15:08.817461Z",
     "iopub.status.idle": "2025-02-04T11:15:14.196529Z",
     "shell.execute_reply": "2025-02-04T11:15:14.195388Z",
     "shell.execute_reply.started": "2025-02-04T11:15:08.817734Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "dataset_path = \"/kaggle/input\"\n",
    "# df = pd.read_csv(f\"{dataset_path}/final_clean_no_neutral_no_duplicates.csv\")\n",
    "df = pd.read_csv(f\"{dataset_path}/final_clean_no_neutral_no_duplicates_v1.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-04T11:15:14.198717Z",
     "iopub.status.busy": "2025-02-04T11:15:14.198436Z",
     "iopub.status.idle": "2025-02-04T11:15:14.400973Z",
     "shell.execute_reply": "2025-02-04T11:15:14.399975Z",
     "shell.execute_reply.started": "2025-02-04T11:15:14.198694Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-04T11:15:14.402082Z",
     "iopub.status.busy": "2025-02-04T11:15:14.401777Z",
     "iopub.status.idle": "2025-02-04T11:15:14.413006Z",
     "shell.execute_reply": "2025-02-04T11:15:14.412151Z",
     "shell.execute_reply.started": "2025-02-04T11:15:14.402057Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-04T11:15:14.414354Z",
     "iopub.status.busy": "2025-02-04T11:15:14.413993Z",
     "iopub.status.idle": "2025-02-04T11:15:14.445532Z",
     "shell.execute_reply": "2025-02-04T11:15:14.444776Z",
     "shell.execute_reply.started": "2025-02-04T11:15:14.414319Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "df[\"target\"] = df[\"target\"].replace(4, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-04T11:15:14.520966Z",
     "iopub.status.busy": "2025-02-04T11:15:14.520689Z",
     "iopub.status.idle": "2025-02-04T11:15:14.524982Z",
     "shell.execute_reply": "2025-02-04T11:15:14.523987Z",
     "shell.execute_reply.started": "2025-02-04T11:15:14.520945Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "output_dir = \"/kaggle/working\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-04T11:15:14.526413Z",
     "iopub.status.busy": "2025-02-04T11:15:14.526048Z",
     "iopub.status.idle": "2025-02-04T11:15:14.540782Z",
     "shell.execute_reply": "2025-02-04T11:15:14.539914Z",
     "shell.execute_reply.started": "2025-02-04T11:15:14.526377Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "feature_methods = [\"count\", \"tfidf\", \"word2vec\", \"glove\"]\n",
    "df_sampled = df.sample(n=1000, random_state=42)\n",
    "# df_sampled = df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_lst = df_sampled[\"text_clean\"].tolist()\n",
    "label_lst = df_sampled[\"target\"].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-04T11:15:14.542209Z",
     "iopub.status.busy": "2025-02-04T11:15:14.541715Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "X_train_features_dict, X_test_features_dict, y_train, y_test = build_vector_for_text(df_sampled, feature_methods, output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "model_name_lst = [\n",
    "    \"decision_tree\", # ok\n",
    "    \"random_forest\", # ok\n",
    "    \"xgboost\", \n",
    "    \"perceptron\", # ok\n",
    "    \"mlp\", # lau but ok\n",
    "    \"lstm\",\n",
    "    \"bayesian\",\n",
    "    \"GA\",\n",
    "    \"hmm\",\n",
    "    \"bayesnet\",\n",
    "    \"logistic_regression\",\n",
    "    \"svm\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train - Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_general_model(df_sampled, doc_lst, label_lst, model_name_lst, feature_methods, MODEL_DICT, MODEL_PARAMS, X_train_features_dict, X_test_features_dict, y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_general_model(model_name_lst, feature_methods, X_test_features_dict, y_test, output_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Remove output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# import shutil\n",
    "# import os\n",
    "\n",
    "# # Path to the working directory\n",
    "# working_dir = '/kaggle/working/'\n",
    "\n",
    "# # Remove all files and subdirectories in the working directory\n",
    "# for filename in os.listdir(working_dir):\n",
    "#     file_path = os.path.join(working_dir, filename)\n",
    "#     if os.path.isfile(file_path):\n",
    "#         os.remove(file_path)\n",
    "#     else:\n",
    "#         shutil.rmtree(file_path)\n",
    "\n",
    "# logger.log_message(\"All outputs cleared from Kaggle working directory.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# import os\n",
    "\n",
    "# # Path to the working directory\n",
    "# working_dir = '/kaggle/working/'\n",
    "\n",
    "# # Loop through files in the working directory\n",
    "# for filename in os.listdir(working_dir):\n",
    "#     if filename.endswith(\".zip\"):  # Check if it's a zip file\n",
    "#         file_path = os.path.join(working_dir, filename)\n",
    "#         os.remove(file_path)  # Remove the zip file\n",
    "#         logger.log_message(f\"Removed: {file_path}\")\n",
    "\n",
    "# logger.log_message(\"✅ All zip files removed from Kaggle working directory.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Zip all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# import shutil\n",
    "# import os\n",
    "# import zipfile\n",
    "\n",
    "# # Define working directory and zip file path\n",
    "# working_dir = \"/kaggle/working/\"\n",
    "# zip_file_path = \"/kaggle/working/output_files.zip\"\n",
    "\n",
    "# # Create a zip archive containing only specific files\n",
    "# with zipfile.ZipFile(zip_file_path, 'w', zipfile.ZIP_DEFLATED) as zipf:\n",
    "#     for filename in os.listdir(working_dir):\n",
    "#         if filename.endswith(\".log\") or filename.endswith(\".png\") or filename.endswith(\".pkl\"):\n",
    "#             file_path = os.path.join(working_dir, filename)\n",
    "#             zipf.write(file_path, filename)  # Save file with its name (without full path)\n",
    "\n",
    "# logger.log_message(f\"✅ Selected outputs zipped into: {zip_file_path}\")"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 6593367,
     "sourceId": 10648520,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30839,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
