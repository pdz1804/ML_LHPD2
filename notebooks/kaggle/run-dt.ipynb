{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-04T11:15:04.601873Z",
     "iopub.status.busy": "2025-02-04T11:15:04.601488Z",
     "iopub.status.idle": "2025-02-04T11:15:08.697313Z",
     "shell.execute_reply": "2025-02-04T11:15:08.696219Z",
     "shell.execute_reply.started": "2025-02-04T11:15:04.601825Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!pip install xgboost gensim tqdm hmmlearn sklearn-crfsuite transformers\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-04T11:15:08.698960Z",
     "iopub.status.busy": "2025-02-04T11:15:08.698610Z",
     "iopub.status.idle": "2025-02-04T11:15:08.706146Z",
     "shell.execute_reply": "2025-02-04T11:15:08.705153Z",
     "shell.execute_reply.started": "2025-02-04T11:15:08.698924Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Standard Library Imports\n",
    "import os\n",
    "import sys\n",
    "import pickle\n",
    "import joblib\n",
    "import logging\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from statistics import mean\n",
    "\n",
    "# Visualization Libraries\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Machine Learning Libraries\n",
    "from sklearn.model_selection import train_test_split, KFold, GridSearchCV\n",
    "from sklearn.preprocessing import MaxAbsScaler, StandardScaler\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, roc_auc_score\n",
    "from sklearn.tree import DecisionTreeClassifier, plot_tree\n",
    "from sklearn.linear_model import Perceptron, LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Specialized Libraries\n",
    "import xgboost as xgb\n",
    "import hmmlearn.hmm\n",
    "from sklearn_crfsuite import CRF\n",
    "\n",
    "# Natural Language Processing (NLP) Libraries\n",
    "import gensim.downloader as api\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "# Progress Bar\n",
    "from tqdm import tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-04T11:15:08.708334Z",
     "iopub.status.busy": "2025-02-04T11:15:08.708080Z",
     "iopub.status.idle": "2025-02-04T11:15:08.729363Z",
     "shell.execute_reply": "2025-02-04T11:15:08.728330Z",
     "shell.execute_reply.started": "2025-02-04T11:15:08.708312Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "## Options\n",
    "pd.set_option(\"max_colwidth\", None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-04T11:15:08.731416Z",
     "iopub.status.busy": "2025-02-04T11:15:08.731059Z",
     "iopub.status.idle": "2025-02-04T11:15:08.746164Z",
     "shell.execute_reply": "2025-02-04T11:15:08.745214Z",
     "shell.execute_reply.started": "2025-02-04T11:15:08.731381Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class MyLogger:\n",
    "    def __init__(self, log_file='app.log'):\n",
    "        self.log_file = log_file\n",
    "        self._initialize_logger()\n",
    "\n",
    "    def _initialize_logger(self):\n",
    "        if os.path.exists(self.log_file):\n",
    "            file_mode = 'a'\n",
    "        else:\n",
    "            file_mode = 'w'\n",
    "\n",
    "        self.logger = logging.getLogger()\n",
    "        self.logger.setLevel(logging.INFO)\n",
    "\n",
    "        file_handler = logging.FileHandler(self.log_file, mode=file_mode, encoding='utf-8')\n",
    "        file_handler.setLevel(logging.INFO)\n",
    "\n",
    "        console_handler = logging.StreamHandler()\n",
    "        console_handler.setLevel(logging.INFO)\n",
    "\n",
    "        formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')\n",
    "        file_handler.setFormatter(formatter)\n",
    "        console_handler.setFormatter(formatter)\n",
    "\n",
    "        if self.logger.hasHandlers():\n",
    "            self.logger.handlers.clear()\n",
    "\n",
    "        self.logger.addHandler(file_handler)\n",
    "        self.logger.addHandler(console_handler)\n",
    "\n",
    "    def log_message(self, message):\n",
    "        self.logger.info(message)\n",
    "\n",
    "    def change_log_file(self, new_log_file):\n",
    "        self.log_file = new_log_file\n",
    "        self._initialize_logger()\n",
    "\n",
    "logger = MyLogger()\n",
    "\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"true\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Build"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-04T11:15:08.747392Z",
     "iopub.status.busy": "2025-02-04T11:15:08.747163Z",
     "iopub.status.idle": "2025-02-04T11:15:08.770758Z",
     "shell.execute_reply": "2025-02-04T11:15:08.769745Z",
     "shell.execute_reply.started": "2025-02-04T11:15:08.747373Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class FeatureBuilder:\n",
    "    def __init__(self, method=\"tfidf\", save_dir=\"data/processed\", reduce_dim=None, n_components=100):\n",
    "        self.method = method\n",
    "        self.save_dir = save_dir\n",
    "        self.reduce_dim = reduce_dim\n",
    "        self.n_components = n_components\n",
    "        os.makedirs(save_dir, exist_ok=True)\n",
    "        \n",
    "        if method == \"tfidf\":\n",
    "            self.vectorizer = TfidfVectorizer(max_features=500)\n",
    "        elif method == \"count\":\n",
    "            self.vectorizer = CountVectorizer(max_features=500)\n",
    "        elif method == \"binary_count\":\n",
    "            self.vectorizer = CountVectorizer(binary=True, max_features=500)\n",
    "        elif method == \"word2vec\":\n",
    "            self.word2vec_model = api.load(\"word2vec-google-news-300\")  # Pretrained Google News Word2Vec\n",
    "        elif method == \"glove\":\n",
    "            self.glove_model = api.load(\"glove-wiki-gigaword-100\")  # Pretrained GloVe embeddings\n",
    "        elif method == \"bert\":\n",
    "            self.tokenizer = AutoTokenizer.from_pretrained(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "            self.bert_model = AutoModel.from_pretrained(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "    \n",
    "        if self.reduce_dim == \"pca\":\n",
    "            self.reducer = PCA(n_components=self.n_components)\n",
    "        elif self.reduce_dim == \"lda\":\n",
    "            self.reducer = LDA(n_components=min(self.n_components, 1))  # LDA needs class labels, adjust accordingly\n",
    "    \n",
    "    def _get_word2vec_vector(self, doc):\n",
    "        tokens = doc.split()\n",
    "        word_vectors = []\n",
    "        for token in tokens:\n",
    "            if token in self.word2vec_model:  # Access word directly\n",
    "                word_vectors.append(self.word2vec_model[token])  # No need for '.wv'\n",
    "        if word_vectors:\n",
    "            return np.mean(word_vectors, axis=0)\n",
    "        else:\n",
    "            return np.zeros(self.word2vec_model.vector_size)\n",
    "\n",
    "    def _get_glove_vector(self, doc):\n",
    "        tokens = doc.split()\n",
    "        word_vectors = []\n",
    "        for token in tokens:\n",
    "            if token in self.glove_model:  # Same for GloVe\n",
    "                word_vectors.append(self.glove_model[token])  # Use directly without '.wv'\n",
    "        if word_vectors:\n",
    "            return np.mean(word_vectors, axis=0)\n",
    "        else:\n",
    "            return np.zeros(self.glove_model.vector_size)\n",
    "\n",
    "\n",
    "    def _get_bert_embedding(self, doc):\n",
    "        inputs = self.tokenizer(doc, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n",
    "        with torch.no_grad():\n",
    "            outputs = self.bert_model(**inputs)\n",
    "        return outputs.pooler_output.squeeze(0).numpy()\n",
    "    \n",
    "    def fit(self, texts, labels=None):\n",
    "        if self.method in [\"tfidf\", \"count\", \"binary_count\"]:\n",
    "            self.vectorizer.fit(texts)\n",
    "        elif self.method in [\"word2vec\", \"glove\", \"bert\"]:\n",
    "            pass\n",
    "\n",
    "\n",
    "    def transform(self, texts, labels=None):\n",
    "        if self.method in [\"tfidf\", \"count\", \"binary_count\"]:\n",
    "            features = self.vectorizer.transform(texts).toarray()\n",
    "\n",
    "        elif self.method == \"word2vec\":\n",
    "            word2vec_embeddings = []\n",
    "            for doc in tqdm(texts, desc=\"Processing Word2Vec\", unit=\"document\"):\n",
    "                word2vec_embeddings.append(self._get_word2vec_vector(doc))\n",
    "            features = np.array(word2vec_embeddings)\n",
    "\n",
    "        elif self.method == \"glove\":\n",
    "            glove_embeddings = []\n",
    "            for doc in tqdm(texts, desc=\"Processing GloVe\", unit=\"document\"):\n",
    "                glove_embeddings.append(self._get_glove_vector(doc))\n",
    "            features = np.array(glove_embeddings)\n",
    "\n",
    "        elif self.method == \"bert\":\n",
    "            bert_embeddings = []\n",
    "            for doc in tqdm(texts, desc=\"Processing BERT\", unit=\"document\"):\n",
    "                bert_embeddings.append(self._get_bert_embedding(doc))\n",
    "            features = np.array(bert_embeddings)\n",
    "\n",
    "        if self.reduce_dim and features is not None:\n",
    "            if self.reduce_dim == \"lda\" and labels is not None:\n",
    "                self.reducer.fit(features, labels)\n",
    "            elif self.reduce_dim == \"pca\":\n",
    "                self.reducer.fit(features)\n",
    "            \n",
    "            features = self.reducer.transform(features)\n",
    "\n",
    "        return features\n",
    "\n",
    "    def fit_transform(self, texts):\n",
    "        self.fit(texts) \n",
    "        return self.transform(texts) \n",
    "    \n",
    "    def _save_model(self):\n",
    "        save_dir = self.save_dir if self.save_dir else \"data/processed\"\n",
    "        os.makedirs(save_dir, exist_ok=True) \n",
    "        \n",
    "        if self.method in [\"tfidf\", \"count\", \"binary_count\"]:\n",
    "            file_path = os.path.join(self.save_dir, f\"{self.method}_vectorizer.pkl\")\n",
    "            with open(file_path, \"wb\") as f:\n",
    "                pickle.dump(self.vectorizer, f)\n",
    "        elif self.method in [\"word2vec\", \"glove\"]:\n",
    "            file_path = os.path.join(self.save_dir, f\"{self.method}_model.pkl\")\n",
    "            with open(file_path, \"wb\") as f:\n",
    "                if self.method == \"word2vec\":\n",
    "                    pickle.dump(self.word2vec_model, f)\n",
    "                elif self.method == \"glove\":\n",
    "                    pickle.dump(self.glove_model, f)\n",
    "        elif self.method == \"bert\":\n",
    "            tokenizer_path = os.path.join(self.save_dir, \"bert_tokenizer.pkl\")\n",
    "            model_path = os.path.join(self.save_dir, \"bert_model.pkl\")\n",
    "            with open(tokenizer_path, \"wb\") as f:\n",
    "                pickle.dump(self.tokenizer, f)\n",
    "            with open(model_path, \"wb\") as f:\n",
    "                pickle.dump(self.bert_model, f)\n",
    "                \n",
    "        if self.reduce_dim:\n",
    "            reducer_path = os.path.join(self.save_dir, f\"{self.reduce_dim}_reducer.pkl\")\n",
    "            with open(reducer_path, \"wb\") as f:\n",
    "                pickle.dump(self.reducer, f)\n",
    "    \n",
    "    def _load_model(self):\n",
    "        os.makedirs(self.save_dir, exist_ok=True)\n",
    "        \n",
    "        if self.method in [\"tfidf\", \"count\", \"binary_count\"]:\n",
    "            file_path = os.path.join(self.save_dir, f\"{self.method}_vectorizer.pkl\")\n",
    "            if not os.path.exists(file_path):\n",
    "                raise FileNotFoundError(f\"No saved model found at {file_path}. Run `fit_transform` first.\")\n",
    "            with open(file_path, \"rb\") as f:\n",
    "                self.vectorizer = pickle.load(f)\n",
    "        elif self.method in [\"word2vec\", \"glove\"]:\n",
    "            file_path = os.path.join(self.save_dir, f\"{self.method}_model.pkl\")\n",
    "            if not os.path.exists(file_path):\n",
    "                raise FileNotFoundError(f\"No saved model found at {file_path}. Run `fit_transform` first.\")\n",
    "            with open(file_path, \"rb\") as f:\n",
    "                self.word2vec_model = pickle.load(f)\n",
    "        elif self.method == \"bert\":\n",
    "            tokenizer_path = os.path.join(self.save_dir, \"bert_tokenizer.pkl\")\n",
    "            model_path = os.path.join(self.save_dir, \"bert_model.pkl\")\n",
    "            if not os.path.exists(tokenizer_path) or not os.path.exists(model_path):\n",
    "                raise FileNotFoundError(f\"No saved BERT model found at {tokenizer_path} or {model_path}. Run `fit_transform` first.\")\n",
    "            with open(tokenizer_path, \"rb\") as f:\n",
    "                self.tokenizer = pickle.load(f)\n",
    "            with open(model_path, \"rb\") as f:\n",
    "                self.bert_model = pickle.load(f)\n",
    "        \n",
    "        if self.reduce_dim:\n",
    "            reducer_path = os.path.join(self.save_dir, f\"{self.reduce_dim}_reducer.pkl\")\n",
    "            with open(reducer_path, \"rb\") as f:\n",
    "                self.reducer = pickle.load(f)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-04T11:15:08.772285Z",
     "iopub.status.busy": "2025-02-04T11:15:08.771937Z",
     "iopub.status.idle": "2025-02-04T11:15:08.792503Z",
     "shell.execute_reply": "2025-02-04T11:15:08.791616Z",
     "shell.execute_reply.started": "2025-02-04T11:15:08.772239Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def generate_binary_classification_model(X, y, model_algorithm, hyperparameters, needs_scaled = False, model_save_path=\"best_model.pkl\", img_save_path=None):\n",
    "    if os.path.exists(model_save_path):\n",
    "        print(f\"üîÑ Loading existing model from {model_save_path}...\")\n",
    "        model_algorithm = joblib.load(model_save_path)\n",
    "        return model_algorithm\n",
    "    \n",
    "    print(f\"üöÄ Training new model: {model_algorithm.__class__.__name__}...\")\n",
    "    logger.log_message(f\"üöÄ Training new model: {model_algorithm.__class__.__name__}...\")\n",
    "    if needs_scaled:\n",
    "        scaler = StandardScaler()\n",
    "        scaled_features = scaler.fit_transform(X)\n",
    "        X = pd.DataFrame(scaled_features, index = X.index, columns = X.columns)\n",
    "        \n",
    "    gridsearchcv = GridSearchCV(estimator = model_algorithm,\n",
    "                                param_grid = hyperparameters)\n",
    "    gridsearchcv.fit(X, y)\n",
    "    \n",
    "    logger.log_message(f'Best hyperparameters: {gridsearchcv.best_params_}')\n",
    "    \n",
    "    model_algorithm.set_params(**gridsearchcv.best_params_)\n",
    "    \n",
    "    accuracy_scores, roc_auc_scores, f1_scores = [], [], []\n",
    "    \n",
    "    k_fold = KFold(n_splits = 5)\n",
    "    \n",
    "    for train_index, val_index in tqdm(k_fold.split(X), total=k_fold.get_n_splits(), desc=\"K-Fold Progress\"):\n",
    "        X_train, X_val = X.iloc[train_index, :], X.iloc[val_index, :]\n",
    "        y_train, y_val = y.iloc[train_index], y.iloc[val_index]\n",
    "        model_algorithm.fit(X_train, y_train)\n",
    "\n",
    "        val_preds = model_algorithm.predict(X_val)\n",
    "\n",
    "        val_accuracy = accuracy_score(y_val, val_preds)\n",
    "        val_roc_auc_score = roc_auc_score(y_val, val_preds)\n",
    "        val_f1_score = f1_score(y_val, val_preds)\n",
    "        \n",
    "        accuracy_scores.append(val_accuracy)\n",
    "        roc_auc_scores.append(val_roc_auc_score)\n",
    "        f1_scores.append(val_f1_score)\n",
    "        \n",
    "    logger.log_message(f'üìä Average Accuracy: {int(mean(accuracy_scores) * 100)}%')\n",
    "    logger.log_message(f'üìä Average ROC AUC: {int(mean(roc_auc_scores) * 100)}%')\n",
    "    logger.log_message(f'üìä Average F1 Score: {int(mean(f1_scores) * 100)}%')\n",
    "\n",
    "    joblib.dump(model_algorithm, model_save_path)\n",
    "    logger.log_message(f'üíæ Model saved to {model_save_path}')\n",
    "    \n",
    "    if img_save_path:\n",
    "        plt.figure(figsize=(10, 6))\n",
    "\n",
    "        plt.plot(range(1, len(accuracy_scores) + 1), accuracy_scores, label=\"Accuracy\", marker='o')\n",
    "        plt.plot(range(1, len(roc_auc_scores) + 1), roc_auc_scores, label=\"ROC AUC\", marker='o')\n",
    "\n",
    "        plt.title(\"Validation Performance Across K-Folds\")\n",
    "        plt.xlabel(\"Fold Number\")\n",
    "        plt.ylabel(\"Score\")\n",
    "        plt.legend()\n",
    "\n",
    "        plt.savefig(img_save_path)\n",
    "        plt.close()\n",
    "        print(f\"üìà Plot saved to {img_save_path}\")\n",
    "        logger.log_message(f\"üìà Plot saved to {img_save_path}\")\n",
    "    \n",
    "    return model_algorithm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Do here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-04T11:15:08.793700Z",
     "iopub.status.busy": "2025-02-04T11:15:08.793389Z",
     "iopub.status.idle": "2025-02-04T11:15:08.814721Z",
     "shell.execute_reply": "2025-02-04T11:15:08.813810Z",
     "shell.execute_reply.started": "2025-02-04T11:15:08.793679Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Dictionary for models\n",
    "MODEL_DICT = {\n",
    "    \"decision_tree\": DecisionTreeClassifier,\n",
    "    \"perceptron\": Perceptron,\n",
    "    \"bayesian\": GaussianNB,\n",
    "    \"bayesian_enhanced\": lambda: GaussianNB(var_smoothing=1e-9),\n",
    "    \"random_forest\": RandomForestClassifier,\n",
    "    \"xgboost\": xgb.XGBClassifier,\n",
    "    \"svm\": SVC,\n",
    "    \"max_edge_classifier\": MaxAbsScaler,\n",
    "    \"kernel_functions_svm\": lambda: SVC(kernel='rbf'),\n",
    "    \"soft_margin_svm\": lambda: SVC(C=1.0),\n",
    "    # \"lda\": LDA,\n",
    "    \"logistic_regression\": LogisticRegression,\n",
    "    \"hmm\": lambda: hmmlearn.hmm.GaussianHMM(n_components=3),\n",
    "    \"crf\": CRF,\n",
    "}\n",
    "\n",
    "# Dictionary for model parameters\n",
    "MODEL_PARAMS = {\n",
    "    \"decision_tree\": {\n",
    "        \"criterion\": [\"gini\"],\n",
    "        \"max_depth\": [10, 20],\n",
    "        \"min_samples_split\": [2],\n",
    "        \"min_samples_leaf\": [1],\n",
    "        \"max_features\": [\"sqrt\", \"log2\"]\n",
    "    },\n",
    "    \n",
    "    # \"decision_tree\": {\n",
    "    #     \"criterion\": [\"gini\", \"entropy\"],\n",
    "    #     \"max_depth\": [10, 20, 30, 40],\n",
    "    #     \"min_samples_split\": [2, 5, 10],\n",
    "    #     \"min_samples_leaf\": [1, 2, 4],\n",
    "    #     \"max_features\": [\"auto\", \"sqrt\", \"log2\"]\n",
    "    # },\n",
    "    \n",
    "    \"perceptron\": {\n",
    "        \"max_iter\": [1000, 2000],\n",
    "        \"tol\": [1e-3],\n",
    "        \"eta0\": [0.001],\n",
    "        \"penalty\": [\"l2\"],\n",
    "        \"alpha\": [0.0001, 0.001]\n",
    "    },\n",
    "    \n",
    "    # \"perceptron\": {\n",
    "    #     \"max_iter\": [1000, 2000],\n",
    "    #     \"tol\": [1e-3, 1e-4],\n",
    "    #     \"eta0\": [0.001, 0.01, 0.1],\n",
    "    #     \"penalty\": [None, \"l2\", \"l1\"],\n",
    "    #     \"alpha\": [0.0001, 0.001, 0.01]\n",
    "    # },\n",
    "    \n",
    "    \"bayesian\": {\n",
    "        \"priors\": [None, \"uniform\", \"gaussian\"],\n",
    "        \"var_smoothing\": [1e-9, 1e-8, 1e-7]\n",
    "    },\n",
    "    \n",
    "    \"bayesian_enhanced\": {\n",
    "        \"var_smoothing\": [1e-9]\n",
    "    },\n",
    "    \n",
    "    # \"bayesian_enhanced\": {\n",
    "    #     \"var_smoothing\": [1e-9, 1e-8, 1e-7]\n",
    "    # },\n",
    "    \n",
    "    \"random_forest\": {\n",
    "        \"n_estimators\": [100],\n",
    "        \"max_depth\": [10],\n",
    "        \"min_samples_split\": [2],\n",
    "        \"min_samples_leaf\": [1],\n",
    "        \"max_features\": [\"sqrt\"]\n",
    "    },\n",
    "    \n",
    "    # \"random_forest\": {\n",
    "    #     \"n_estimators\": [50, 100, 200],\n",
    "    #     \"max_depth\": [None, 10, 20, 30],\n",
    "    #     \"min_samples_split\": [2, 5, 10],\n",
    "    #     \"min_samples_leaf\": [1, 2, 4],\n",
    "    #     \"max_features\": [\"auto\", \"sqrt\", \"log2\"],\n",
    "    #     \"bootstrap\": [True, False]\n",
    "    # },\n",
    "    \n",
    "    \"xgboost\": {\n",
    "        \"n_estimators\": [100],\n",
    "        \"learning_rate\": [0.01, 0.1],\n",
    "        \"max_depth\": [6, 10]\n",
    "    },\n",
    "    \n",
    "    # \"xgboost\": {\n",
    "    #     \"n_estimators\": [100, 200, 300],\n",
    "    #     \"learning_rate\": [0.01, 0.1, 0.2],\n",
    "    #     \"max_depth\": [3, 6, 10],\n",
    "    #     \"subsample\": [0.8, 1.0],\n",
    "    #     \"colsample_bytree\": [0.8, 1.0],\n",
    "    #     \"gamma\": [0, 0.1, 0.2]\n",
    "    # },\n",
    "    \n",
    "    \"svm\": {\n",
    "        \"kernel\": [\"linear\"],\n",
    "        \"C\": [0.1, 0.001]\n",
    "    },\n",
    "    \n",
    "    # \"svm\": {\n",
    "    #     \"kernel\": [\"linear\", \"rbf\", \"poly\"],\n",
    "    #     \"C\": [0.1, 1, 10, 100],\n",
    "    #     \"gamma\": [0.1, 0.01, \"scale\"],\n",
    "    #     \"degree\": [2, 3, 4]\n",
    "    # },\n",
    "    \n",
    "    \"max_edge_classifier\": {\n",
    "        \"scaler\": [\"maxabs\", \"standard\"]\n",
    "    },\n",
    "    \n",
    "    \"kernel_functions_svm\": {\n",
    "        \"kernel\": [\"rbf\", \"poly\"],\n",
    "        \"C\": [1.0, 10.0, 100.0],\n",
    "        \"gamma\": [\"scale\", \"auto\"]\n",
    "    },\n",
    "    \n",
    "    \"soft_margin_svm\": {\n",
    "        \"C\": [0.1, 1.0, 10.0]\n",
    "    },\n",
    "    \n",
    "    \"lda\": {\n",
    "        \"n_components\": [2, 3, 4, 5],\n",
    "        \"solver\": [\"svd\", \"lsqr\", \"eigen\"],\n",
    "        \"shrinkage\": [\"auto\", None]\n",
    "    },\n",
    "    \n",
    "    \"logistic_regression\": {\n",
    "        \"penalty\": [\"l2\"],\n",
    "        \"C\": [0.1, 1.0],\n",
    "        \"max_iter\": [1000, 2000]\n",
    "    },\n",
    "    \n",
    "    # \"logistic_regression\": {\n",
    "    #     \"penalty\": [\"l1\", \"l2\", \"elasticnet\", None],\n",
    "    #     \"C\": [0.1, 1.0, 10.0],\n",
    "    #     \"solver\": [\"liblinear\", \"lbfgs\", \"saga\"],\n",
    "    #     \"max_iter\": [1000, 2000]\n",
    "    # },\n",
    "    \n",
    "    \n",
    "    \"hmm\": {\n",
    "        \"n_components\": [2, 3, 4],\n",
    "        \"covariance_type\": [\"diag\", \"full\", \"tied\"],\n",
    "        \"n_iter\": [100, 200],\n",
    "        \"init_params\": [\"c\", \"s\", \"cs\"],\n",
    "        \"params\": [\"c\", \"t\", \"ct\"]\n",
    "    },\n",
    "    \n",
    "    \"crf\": {\n",
    "        \"algorithm\": [\"lbfgs\", \"newton-cg\", \"liblinear\"],\n",
    "        \"max_iterations\": [100, 200],\n",
    "        \"penalty\": [\"l2\", \"elasticnet\"],\n",
    "        \"dual\": [True, False],\n",
    "        \"tol\": [1e-4, 1e-3],\n",
    "    }\n",
    "}\n",
    "\n",
    "# Dictionary for dimensionality reduction methods\n",
    "DIMENSIONALITY_REDUCTION_DICT = {\n",
    "    \"pca\": PCA,\n",
    "    \"lda\": LDA,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-04T11:15:08.817756Z",
     "iopub.status.busy": "2025-02-04T11:15:08.817461Z",
     "iopub.status.idle": "2025-02-04T11:15:14.196529Z",
     "shell.execute_reply": "2025-02-04T11:15:14.195388Z",
     "shell.execute_reply.started": "2025-02-04T11:15:08.817734Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "dataset_path = \"/kaggle/input/tweets-clean-posneg-v1\"\n",
    "df = pd.read_csv(f\"{dataset_path}/final_clean_no_neutral_no_duplicates.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-04T11:15:14.198717Z",
     "iopub.status.busy": "2025-02-04T11:15:14.198436Z",
     "iopub.status.idle": "2025-02-04T11:15:14.400973Z",
     "shell.execute_reply": "2025-02-04T11:15:14.399975Z",
     "shell.execute_reply.started": "2025-02-04T11:15:14.198694Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-04T11:15:14.402082Z",
     "iopub.status.busy": "2025-02-04T11:15:14.401777Z",
     "iopub.status.idle": "2025-02-04T11:15:14.413006Z",
     "shell.execute_reply": "2025-02-04T11:15:14.412151Z",
     "shell.execute_reply.started": "2025-02-04T11:15:14.402057Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-04T11:15:14.414354Z",
     "iopub.status.busy": "2025-02-04T11:15:14.413993Z",
     "iopub.status.idle": "2025-02-04T11:15:14.445532Z",
     "shell.execute_reply": "2025-02-04T11:15:14.444776Z",
     "shell.execute_reply.started": "2025-02-04T11:15:14.414319Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "df[\"target\"] = df[\"target\"].replace(4, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-04T11:15:14.446550Z",
     "iopub.status.busy": "2025-02-04T11:15:14.446320Z",
     "iopub.status.idle": "2025-02-04T11:15:14.519888Z",
     "shell.execute_reply": "2025-02-04T11:15:14.519161Z",
     "shell.execute_reply.started": "2025-02-04T11:15:14.446530Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# df_sampled = df.sample(n=100000, random_state=42)\n",
    "df_sampled = df\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    df_sampled[\"text_clean\"], df_sampled[\"target\"], test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "logger.log_message(f\"Training set size: {len(X_train)}\")\n",
    "logger.log_message(f\"Test set size: {len(X_test)}\")\n",
    "logger.log_message(f\"Training labels size: {len(y_train)}\")\n",
    "logger.log_message(f\"Test labels size: {len(y_test)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-04T11:15:14.520966Z",
     "iopub.status.busy": "2025-02-04T11:15:14.520689Z",
     "iopub.status.idle": "2025-02-04T11:15:14.524982Z",
     "shell.execute_reply": "2025-02-04T11:15:14.523987Z",
     "shell.execute_reply.started": "2025-02-04T11:15:14.520945Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# feature_methods = [\"count\", \"word2vec\", \"glove\", \"bert\"]\n",
    "feature_methods = [\"word2vec\", \"glove\"]\n",
    "X_train_features_dict = {}\n",
    "X_test_features_dict = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-04T11:15:14.526413Z",
     "iopub.status.busy": "2025-02-04T11:15:14.526048Z",
     "iopub.status.idle": "2025-02-04T11:15:14.540782Z",
     "shell.execute_reply": "2025-02-04T11:15:14.539914Z",
     "shell.execute_reply.started": "2025-02-04T11:15:14.526377Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "output_dir = \"/kaggle/working\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-04T11:15:14.542209Z",
     "iopub.status.busy": "2025-02-04T11:15:14.541715Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "print(\"\\nüîé Running feature extraction...\\n\")\n",
    "logger.log_message(\"\\nüîé Running feature extraction...\\n\")\n",
    "\n",
    "for method in tqdm(feature_methods, desc=\"Feature Extraction Progress\"):\n",
    "    print(f\"\\nüîç Processing feature extraction using: {method}...\")\n",
    "\n",
    "    try:\n",
    "        # Initialize FeatureBuilder\n",
    "        feature_builder = FeatureBuilder(method=method, save_dir=os.path.join(output_dir, \"processed\"), reduce_dim=\"pca\", n_components=50)\n",
    "\n",
    "        # Fit and transform training data\n",
    "        X_train_features = feature_builder.fit_transform(X_train.tolist())\n",
    "        print(f\"‚úÖ {method} - X_train_features shape: {X_train_features.shape}\")\n",
    "        \n",
    "        X_train_features_dict[method] = pd.DataFrame(X_train_features)\n",
    "\n",
    "        X_test_features_dict[method] = feature_builder.transform(X_test.tolist())\n",
    "        print(f\"‚úÖ {method} - X_test_features shape: {X_test_features_dict[method].shape}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error with {method}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "model_names = [\"decision_tree\", \"logistic_regression\", \"random_forest\", \"xgboost\", \"perceptron\", \"bayesian_enhanced\", \"svm\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "output_dir = \"/kaggle/working\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "print(\"\\nüîé Running model training loop...\\n\")\n",
    "logger.log_message(\"\\nüîé Running feature extraction...\\n\")\n",
    "\n",
    "for model_name in model_names:\n",
    "    for method in tqdm(feature_methods, desc=\"Feature Extraction Progress\"):\n",
    "        print(f\"Model: {model_name}\")\n",
    "        print(f\"üîç Method: {method}...\")\n",
    "        logger.log_message(f\"Model: {model_name}\")\n",
    "        logger.log_message(f\"üîç Method: {method}...\")\n",
    "    \n",
    "        try:\n",
    "            # Retrieve Decision Tree model and hyperparameters\n",
    "            algorithm = MODEL_DICT[model_name]()\n",
    "            params = MODEL_PARAMS[model_name]\n",
    "    \n",
    "            # Train or load model\n",
    "            trained_model = generate_binary_classification_model(\n",
    "                X=X_train_features_dict[method], \n",
    "                y=y_train, \n",
    "                model_algorithm=algorithm, \n",
    "                hyperparameters=params, \n",
    "                needs_scaled=False, \n",
    "                model_save_path=f\"best_{model_name}_{method}.pkl\",\n",
    "                img_save_path=f\"best_{model_name}_{method}.png\"\n",
    "            )\n",
    "    \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error with {method}: {e}\")\n",
    "            logger.log_message(f\"‚ùå Error with {method}: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Predict for each model\n",
    "for model_name in model_names:\n",
    "    for method in feature_methods:\n",
    "        # Load the saved model\n",
    "        model_filename = os.path.join(output_dir, f\"best_{model_name}_{method}.pkl\")\n",
    "        with open(model_filename, 'rb') as model_file:\n",
    "            model = joblib.load(model_file)\n",
    "\n",
    "        # Make predictions\n",
    "        y_pred = model.predict(X_test_features_dict[method])\n",
    "\n",
    "        # Compute metrics\n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "        precision = precision_score(y_test, y_pred, average='binary')\n",
    "        recall = recall_score(y_test, y_pred, average='binary')\n",
    "        f1 = f1_score(y_test, y_pred, average='binary')\n",
    "        \n",
    "        # ROC AUC can be computed if the model outputs probabilities\n",
    "        # Handle models that do not support `predict_proba`\n",
    "        if hasattr(model, \"predict_proba\"):\n",
    "            y_prob = model.predict_proba(X_test_features_dict[method])[:, 1]  # Take the positive class probabilities\n",
    "            roc_auc = roc_auc_score(y_test, y_prob)\n",
    "        else:\n",
    "            roc_auc = \"N/A\"  # Not applicable for models like Perceptron\n",
    "\n",
    "        # Print metrics\n",
    "        print(f\"Model: {model_name}\")\n",
    "        print(f\"Method: {method}\")\n",
    "        print(\"-\" * 50)\n",
    "        print(f\"Accuracy: {accuracy:.4f}\")\n",
    "        print(f\"Precision: {precision:.4f}\")\n",
    "        print(f\"Recall: {recall:.4f}\")\n",
    "        print(f\"F1 Score: {f1:.4f}\")\n",
    "        if hasattr(model, \"predict_proba\"):\n",
    "            print(f\"ROC AUC: {roc_auc:.4f}\")\n",
    "        else:\n",
    "            print(\"ROC AUC: N/A\")\n",
    "        print(\"-\" * 50)\n",
    "\n",
    "        logger.log_message(f\"Model: {model_name}\")\n",
    "        logger.log_message(f\"Method: {method}\")\n",
    "        logger.log_message(\"-\" * 50)\n",
    "        logger.log_message(f\"Accuracy: {accuracy:.4f}\")\n",
    "        logger.log_message(f\"Precision: {precision:.4f}\")\n",
    "        logger.log_message(f\"Recall: {recall:.4f}\")\n",
    "        logger.log_message(f\"F1 Score: {f1:.4f}\")\n",
    "        \n",
    "        if hasattr(model, \"predict_proba\"):\n",
    "            logger.log_message(f\"ROC AUC: {roc_auc:.4f}\")\n",
    "        else:\n",
    "            logger.log_message(\"ROC AUC: N/A\")\n",
    "        logger.log_message(\"-\" * 50)\n",
    "        \n",
    "    print(\"%\" * 50)\n",
    "    logger.log_message(\"%\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Remove output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# import shutil\n",
    "# import os\n",
    "\n",
    "# # Path to the working directory\n",
    "# working_dir = '/kaggle/working/'\n",
    "\n",
    "# # Remove all files and subdirectories in the working directory\n",
    "# for filename in os.listdir(working_dir):\n",
    "#     file_path = os.path.join(working_dir, filename)\n",
    "#     if os.path.isfile(file_path):\n",
    "#         os.remove(file_path)\n",
    "#     else:\n",
    "#         shutil.rmtree(file_path)\n",
    "\n",
    "# print(\"All outputs cleared from Kaggle working directory.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# import os\n",
    "\n",
    "# # Path to the working directory\n",
    "# working_dir = '/kaggle/working/'\n",
    "\n",
    "# # Loop through files in the working directory\n",
    "# for filename in os.listdir(working_dir):\n",
    "#     if filename.endswith(\".zip\"):  # Check if it's a zip file\n",
    "#         file_path = os.path.join(working_dir, filename)\n",
    "#         os.remove(file_path)  # Remove the zip file\n",
    "#         print(f\"Removed: {file_path}\")\n",
    "\n",
    "# print(\"‚úÖ All zip files removed from Kaggle working directory.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Zip all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# import shutil\n",
    "# import os\n",
    "# import zipfile\n",
    "\n",
    "# # Define working directory and zip file path\n",
    "# working_dir = \"/kaggle/working/\"\n",
    "# zip_file_path = \"/kaggle/working/output_files.zip\"\n",
    "\n",
    "# # Create a zip archive containing only specific files\n",
    "# with zipfile.ZipFile(zip_file_path, 'w', zipfile.ZIP_DEFLATED) as zipf:\n",
    "#     for filename in os.listdir(working_dir):\n",
    "#         if filename.endswith(\".log\") or filename.endswith(\".png\") or filename.endswith(\".pkl\"):\n",
    "#             file_path = os.path.join(working_dir, filename)\n",
    "#             zipf.write(file_path, filename)  # Save file with its name (without full path)\n",
    "\n",
    "# print(f\"‚úÖ Selected outputs zipped into: {zip_file_path}\")"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 6593367,
     "sourceId": 10648520,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30839,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
