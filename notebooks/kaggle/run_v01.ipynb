{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-04T11:15:04.601873Z",
     "iopub.status.busy": "2025-02-04T11:15:04.601488Z",
     "iopub.status.idle": "2025-02-04T11:15:08.697313Z",
     "shell.execute_reply": "2025-02-04T11:15:08.696219Z",
     "shell.execute_reply.started": "2025-02-04T11:15:04.601825Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!pip install xgboost gensim tqdm hmmlearn sklearn-crfsuite transformers tensorflow keras keras-tuner\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-04T11:15:08.698960Z",
     "iopub.status.busy": "2025-02-04T11:15:08.698610Z",
     "iopub.status.idle": "2025-02-04T11:15:08.706146Z",
     "shell.execute_reply": "2025-02-04T11:15:08.705153Z",
     "shell.execute_reply.started": "2025-02-04T11:15:08.698924Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Standard Library Imports\n",
    "import os\n",
    "import sys\n",
    "import pickle\n",
    "import joblib\n",
    "import logging\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from statistics import mean\n",
    "\n",
    "# Visualization Libraries\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Machine Learning Libraries\n",
    "from sklearn.model_selection import train_test_split, KFold, GridSearchCV\n",
    "from sklearn.preprocessing import MaxAbsScaler, StandardScaler\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, roc_auc_score, log_loss, hinge_loss\n",
    "from sklearn.tree import DecisionTreeClassifier, plot_tree\n",
    "from sklearn.linear_model import Perceptron, LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Specialized Libraries\n",
    "import xgboost as xgb\n",
    "import hmmlearn.hmm\n",
    "from sklearn_crfsuite import CRF\n",
    "\n",
    "# Natural Language Processing (NLP) Libraries\n",
    "import gensim.downloader as api\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "# Progress Bar\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import keras_tuner as kt\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.models import load_model\n",
    "from tqdm import tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-04T11:15:08.708334Z",
     "iopub.status.busy": "2025-02-04T11:15:08.708080Z",
     "iopub.status.idle": "2025-02-04T11:15:08.729363Z",
     "shell.execute_reply": "2025-02-04T11:15:08.728330Z",
     "shell.execute_reply.started": "2025-02-04T11:15:08.708312Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "## Options\n",
    "pd.set_option(\"max_colwidth\", None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-04T11:15:08.731416Z",
     "iopub.status.busy": "2025-02-04T11:15:08.731059Z",
     "iopub.status.idle": "2025-02-04T11:15:08.746164Z",
     "shell.execute_reply": "2025-02-04T11:15:08.745214Z",
     "shell.execute_reply.started": "2025-02-04T11:15:08.731381Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class MyLogger:\n",
    "    def __init__(self, log_file='app.log'):\n",
    "        self.log_file = log_file\n",
    "        self._initialize_logger()\n",
    "\n",
    "    def _initialize_logger(self):\n",
    "        if os.path.exists(self.log_file):\n",
    "            file_mode = 'a'\n",
    "        else:\n",
    "            file_mode = 'w'\n",
    "\n",
    "        self.logger = logging.getLogger()\n",
    "        self.logger.setLevel(logging.INFO)\n",
    "\n",
    "        file_handler = logging.FileHandler(self.log_file, mode=file_mode, encoding='utf-8')\n",
    "        file_handler.setLevel(logging.INFO)\n",
    "\n",
    "        console_handler = logging.StreamHandler()\n",
    "        console_handler.setLevel(logging.INFO)\n",
    "\n",
    "        formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')\n",
    "        file_handler.setFormatter(formatter)\n",
    "        console_handler.setFormatter(formatter)\n",
    "\n",
    "        if self.logger.hasHandlers():\n",
    "            self.logger.handlers.clear()\n",
    "\n",
    "        self.logger.addHandler(file_handler)\n",
    "        self.logger.addHandler(console_handler)\n",
    "\n",
    "    def log_message(self, message):\n",
    "        self.logger.info(message)\n",
    "\n",
    "    def change_log_file(self, new_log_file):\n",
    "        self.log_file = new_log_file\n",
    "        self._initialize_logger()\n",
    "\n",
    "logger = MyLogger()\n",
    "\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"true\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Build"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-04T11:15:08.747392Z",
     "iopub.status.busy": "2025-02-04T11:15:08.747163Z",
     "iopub.status.idle": "2025-02-04T11:15:08.770758Z",
     "shell.execute_reply": "2025-02-04T11:15:08.769745Z",
     "shell.execute_reply.started": "2025-02-04T11:15:08.747373Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class FeatureBuilder:\n",
    "    def __init__(self, method=\"tfidf\", save_dir=\"data/processed\", reduce_dim=None, n_components=100):\n",
    "        \"\"\"\n",
    "        Initializes the FeatureBuilder with a specified feature engineering method.\n",
    "        \n",
    "        :param method: str, feature engineering method ('tfidf', 'count', 'word2vec', 'bert', etc.)\n",
    "        :param save_dir: str, directory to save processed features\n",
    "        \"\"\"\n",
    "        self.method = method\n",
    "        self.save_dir = save_dir\n",
    "        self.reduce_dim = reduce_dim\n",
    "        self.n_components = n_components\n",
    "        os.makedirs(save_dir, exist_ok=True)\n",
    "        \n",
    "        # Define models for vectorization\n",
    "        if method == \"tfidf\":\n",
    "            self.vectorizer = TfidfVectorizer(max_features=2000, stop_words=\"english\")\n",
    "        elif method == \"count\":\n",
    "            self.vectorizer = CountVectorizer(max_features=2000)\n",
    "        elif method == \"binary_count\":\n",
    "            self.vectorizer = CountVectorizer(binary=True, max_features=2000)\n",
    "        elif method == \"word2vec\":\n",
    "            self.word2vec_model = api.load(\"word2vec-google-news-300\")  # Pretrained Google News Word2Vec\n",
    "        elif method == \"glove\":\n",
    "            self.glove_model = api.load(\"glove-wiki-gigaword-100\")  # Pretrained GloVe embeddings\n",
    "        elif method == \"bert\":\n",
    "            self.tokenizer = AutoTokenizer.from_pretrained(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "            self.bert_model = AutoModel.from_pretrained(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "    \n",
    "        # Initialize dimensionality reduction\n",
    "        if self.reduce_dim == \"pca\":\n",
    "            self.reducer = PCA(n_components=self.n_components)\n",
    "        elif self.reduce_dim == \"lda\":\n",
    "            self.reducer = LDA(n_components=min(self.n_components, 1))  # LDA needs class labels, adjust accordingly\n",
    "    \n",
    "    def _get_word2vec_vector(self, doc):\n",
    "        \"\"\"\n",
    "        Extracts the average Word2Vec embedding for a document.\n",
    "\n",
    "        :param doc: str, the document text\n",
    "        :return: np.array, the averaged Word2Vec embedding\n",
    "        \"\"\"\n",
    "        tokens = doc.split()\n",
    "        word_vectors = []\n",
    "        for token in tokens:\n",
    "            if token in self.word2vec_model:  # Access word directly\n",
    "                word_vectors.append(self.word2vec_model[token])  # No need for '.wv'\n",
    "        if word_vectors:\n",
    "            return np.mean(word_vectors, axis=0)\n",
    "        else:\n",
    "            return np.zeros(self.word2vec_model.vector_size)\n",
    "\n",
    "    def _get_glove_vector(self, doc):\n",
    "        \"\"\"\n",
    "        Extracts the average GloVe embedding for a document.\n",
    "\n",
    "        :param doc: str, the document text\n",
    "        :return: np.array, the averaged GloVe embedding\n",
    "        \"\"\"\n",
    "        tokens = doc.split()\n",
    "        word_vectors = []\n",
    "        for token in tokens:\n",
    "            if token in self.glove_model:  # Same for GloVe\n",
    "                word_vectors.append(self.glove_model[token])  # Use directly without '.wv'\n",
    "        if word_vectors:\n",
    "            return np.mean(word_vectors, axis=0)\n",
    "        else:\n",
    "            return np.zeros(self.glove_model.vector_size)\n",
    "\n",
    "    def _get_bert_embedding(self, doc):\n",
    "        \"\"\"\n",
    "        Extracts the BERT embedding for a document.\n",
    "\n",
    "        :param doc: str, the document text\n",
    "        :return: np.array, the BERT embedding\n",
    "        \"\"\"\n",
    "        inputs = self.tokenizer(doc, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n",
    "        with torch.no_grad():\n",
    "            outputs = self.bert_model(**inputs)\n",
    "        return outputs.pooler_output.squeeze(0).numpy()\n",
    "    \n",
    "    def fit(self, texts, labels=None):\n",
    "        \"\"\"\n",
    "        Fits the model to the text data by computing necessary statistics (e.g., vocabulary, embeddings).\n",
    "\n",
    "        :param texts: list, raw text data\n",
    "        :return: None\n",
    "        \"\"\"\n",
    "        if self.method in [\"tfidf\", \"count\", \"binary_count\"]:\n",
    "            self.vectorizer.fit(texts)\n",
    "        elif self.method in [\"word2vec\", \"glove\", \"bert\"]:\n",
    "            pass\n",
    "\n",
    "        # if self.reduce_dim == \"lda\" and labels is not None:\n",
    "        #     features = self.vectorizer.transform(texts).toarray()\n",
    "        #     self.reducer.fit(features, labels)\n",
    "        # elif self.reduce_dim == \"pca\":\n",
    "        #     features = self.vectorizer.transform(texts).toarray()\n",
    "        #     self.reducer.fit(features)\n",
    "\n",
    "    def transform(self, texts, labels=None):\n",
    "        \"\"\"\n",
    "        Transforms new data based on the fitted model.\n",
    "\n",
    "        :param texts: list, raw text data\n",
    "        :return: transformed feature matrix\n",
    "        \"\"\"\n",
    "        if self.method in [\"tfidf\", \"count\", \"binary_count\"]:\n",
    "            # Transform the new data using the fitted vectorizer\n",
    "            features = self.vectorizer.transform(texts).toarray()\n",
    "\n",
    "        elif self.method == \"word2vec\":\n",
    "            # Use the pre-trained Word2Vec model to generate embeddings\n",
    "            word2vec_embeddings = []\n",
    "            for doc in tqdm(texts, desc=\"Processing Word2Vec\", unit=\"document\"):\n",
    "                word2vec_embeddings.append(self._get_word2vec_vector(doc))\n",
    "            features = np.array(word2vec_embeddings)\n",
    "\n",
    "        elif self.method == \"glove\":\n",
    "            # Similar process for GloVe embeddings\n",
    "            glove_embeddings = []\n",
    "            for doc in tqdm(texts, desc=\"Processing GloVe\", unit=\"document\"):\n",
    "                glove_embeddings.append(self._get_glove_vector(doc))\n",
    "            features = np.array(glove_embeddings)\n",
    "\n",
    "        elif self.method == \"bert\":\n",
    "            # Use the pre-trained BERT model to generate embeddings\n",
    "            bert_embeddings = []\n",
    "            for doc in tqdm(texts, desc=\"Processing BERT\", unit=\"document\"):\n",
    "                bert_embeddings.append(self._get_bert_embedding(doc))\n",
    "            features = np.array(bert_embeddings)\n",
    "\n",
    "        # Apply dimensionality reduction if enabled\n",
    "        if self.reduce_dim and features is not None:\n",
    "            if self.reduce_dim == \"lda\" and labels is not None:\n",
    "                # features = self.vectorizer.transform(texts).toarray()\n",
    "                self.reducer.fit(features, labels)\n",
    "            elif self.reduce_dim == \"pca\":\n",
    "                # features = self.vectorizer.transform(texts).toarray()\n",
    "                self.reducer.fit(features)\n",
    "            \n",
    "            features = self.reducer.transform(features)\n",
    "\n",
    "        return features\n",
    "\n",
    "    def fit_transform(self, texts):\n",
    "        \"\"\"\n",
    "        Fits and transforms the text data by first fitting the model and then transforming it.\n",
    "\n",
    "        :param texts: list, raw text data\n",
    "        :return: transformed feature matrix\n",
    "        \"\"\"\n",
    "        self.fit(texts)  # First fit the model (compute parameters)\n",
    "        return self.transform(texts)  # Then transform the data using the fitted model\n",
    "    \n",
    "    def _save_model(self):\n",
    "        \"\"\"Saves the fitted vectorizer/scaler for later use.\"\"\"\n",
    "        # Ensure the directory exists\n",
    "        save_dir = self.save_dir if self.save_dir else \"data/processed\"\n",
    "        os.makedirs(save_dir, exist_ok=True)  # Create directory if it doesn't exist  \n",
    "        \n",
    "        if self.method in [\"tfidf\", \"count\", \"binary_count\"]:\n",
    "            file_path = os.path.join(self.save_dir, f\"{self.method}_vectorizer.pkl\")\n",
    "            with open(file_path, \"wb\") as f:\n",
    "                pickle.dump(self.vectorizer, f)\n",
    "        elif self.method in [\"word2vec\", \"glove\"]:\n",
    "            # Save the Word2Vec or GloVe model\n",
    "            file_path = os.path.join(self.save_dir, f\"{self.method}_model.pkl\")\n",
    "            with open(file_path, \"wb\") as f:\n",
    "                if self.method == \"word2vec\":\n",
    "                    pickle.dump(self.word2vec_model, f)\n",
    "                elif self.method == \"glove\":\n",
    "                    pickle.dump(self.glove_model, f)\n",
    "        elif self.method == \"bert\":\n",
    "            # Save the BERT tokenizer and model\n",
    "            tokenizer_path = os.path.join(self.save_dir, \"bert_tokenizer.pkl\")\n",
    "            model_path = os.path.join(self.save_dir, \"bert_model.pkl\")\n",
    "            with open(tokenizer_path, \"wb\") as f:\n",
    "                pickle.dump(self.tokenizer, f)\n",
    "            with open(model_path, \"wb\") as f:\n",
    "                pickle.dump(self.bert_model, f)\n",
    "                \n",
    "        if self.reduce_dim:\n",
    "            reducer_path = os.path.join(self.save_dir, f\"{self.reduce_dim}_reducer.pkl\")\n",
    "            with open(reducer_path, \"wb\") as f:\n",
    "                pickle.dump(self.reducer, f)\n",
    "    \n",
    "    def _load_model(self):\n",
    "        \"\"\"Loads the previously saved vectorizer/scaler.\"\"\"\n",
    "        # Ensure the directory exists\n",
    "        os.makedirs(self.save_dir, exist_ok=True)\n",
    "        \n",
    "        if self.method in [\"tfidf\", \"count\", \"binary_count\"]:\n",
    "            file_path = os.path.join(self.save_dir, f\"{self.method}_vectorizer.pkl\")\n",
    "            if not os.path.exists(file_path):\n",
    "                raise FileNotFoundError(f\"No saved model found at {file_path}. Run `fit_transform` first.\")\n",
    "            with open(file_path, \"rb\") as f:\n",
    "                self.vectorizer = pickle.load(f)\n",
    "        elif self.method in [\"word2vec\", \"glove\"]:\n",
    "            file_path = os.path.join(self.save_dir, f\"{self.method}_model.pkl\")\n",
    "            if not os.path.exists(file_path):\n",
    "                raise FileNotFoundError(f\"No saved model found at {file_path}. Run `fit_transform` first.\")\n",
    "            with open(file_path, \"rb\") as f:\n",
    "                self.word2vec_model = pickle.load(f)\n",
    "        elif self.method == \"bert\":\n",
    "            tokenizer_path = os.path.join(self.save_dir, \"bert_tokenizer.pkl\")\n",
    "            model_path = os.path.join(self.save_dir, \"bert_model.pkl\")\n",
    "            if not os.path.exists(tokenizer_path) or not os.path.exists(model_path):\n",
    "                raise FileNotFoundError(f\"No saved BERT model found at {tokenizer_path} or {model_path}. Run `fit_transform` first.\")\n",
    "            with open(tokenizer_path, \"rb\") as f:\n",
    "                self.tokenizer = pickle.load(f)\n",
    "            with open(model_path, \"rb\") as f:\n",
    "                self.bert_model = pickle.load(f)\n",
    "        \n",
    "        if self.reduce_dim:\n",
    "            reducer_path = os.path.join(self.save_dir, f\"{self.reduce_dim}_reducer.pkl\")\n",
    "            with open(reducer_path, \"rb\") as f:\n",
    "                self.reducer = pickle.load(f)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_cnn_model(hp, input_shape):\n",
    "    model = keras.Sequential()\n",
    "    \n",
    "    # CNN Layer 1\n",
    "    model.add(layers.Conv1D(\n",
    "        filters=hp.Int('filters_1', min_value=32, max_value=128, step=32),\n",
    "        kernel_size=hp.Choice('kernel_size_1', values=[3, 5]),\n",
    "        activation=\"relu\",\n",
    "        input_shape=input_shape\n",
    "    ))\n",
    "    model.add(layers.MaxPooling1D(pool_size=2))\n",
    "    \n",
    "    # CNN Layer 2\n",
    "    model.add(layers.Conv1D(\n",
    "        filters=hp.Int('filters_2', min_value=64, max_value=256, step=64),\n",
    "        kernel_size=hp.Choice('kernel_size_2', values=[3, 5]),\n",
    "        activation=\"relu\"\n",
    "    ))\n",
    "    model.add(layers.MaxPooling1D(pool_size=2))\n",
    "    \n",
    "    model.add(layers.Flatten())\n",
    "\n",
    "    # Dense Layer\n",
    "    model.add(layers.Dense(\n",
    "        units=hp.Int('dense_units', min_value=64, max_value=256, step=64),\n",
    "        activation=\"relu\"\n",
    "    ))\n",
    "    \n",
    "    # Dropout\n",
    "    model.add(layers.Dropout(rate=hp.Float('dropout', min_value=0.2, max_value=0.5, step=0.1)))\n",
    "    \n",
    "    # Output Layer\n",
    "    model.add(layers.Dense(1, activation=\"sigmoid\"))\n",
    "    \n",
    "    # Compile Model\n",
    "    model.compile(\n",
    "        optimizer=keras.optimizers.Adam(hp.Choice('learning_rate', values=[1e-3, 1e-4])),\n",
    "        loss=\"binary_crossentropy\",\n",
    "        metrics=[\"accuracy\"]\n",
    "    )\n",
    "    \n",
    "    return model\n",
    "\n",
    "def build_lstm_model(hp, input_shape):\n",
    "    model = keras.Sequential()\n",
    "    \n",
    "    # First LSTM Layer\n",
    "    model.add(layers.LSTM(\n",
    "        units=hp.Int('lstm_units_1', min_value=64, max_value=256, step=64),\n",
    "        return_sequences=True,\n",
    "        input_shape=input_shape\n",
    "    ))\n",
    "    \n",
    "    # Second LSTM Layer\n",
    "    model.add(layers.LSTM(\n",
    "        units=hp.Int('lstm_units_2', min_value=32, max_value=128, step=32),\n",
    "        return_sequences=False\n",
    "    ))\n",
    "    \n",
    "    # Dense Layer\n",
    "    model.add(layers.Dense(\n",
    "        units=hp.Int('dense_units', min_value=64, max_value=256, step=64),\n",
    "        activation=\"relu\"\n",
    "    ))\n",
    "    \n",
    "    # Dropout\n",
    "    model.add(layers.Dropout(rate=hp.Float('dropout', min_value=0.2, max_value=0.5, step=0.1)))\n",
    "    \n",
    "    # Output Layer\n",
    "    model.add(layers.Dense(1, activation=\"sigmoid\"))\n",
    "    \n",
    "    # Compile Model\n",
    "    model.compile(\n",
    "        optimizer=keras.optimizers.Adam(hp.Choice('learning_rate', values=[1e-3, 1e-4])),\n",
    "        loss=\"binary_crossentropy\",\n",
    "        metrics=[\"accuracy\"]\n",
    "    )\n",
    "    \n",
    "    return model\n",
    "\n",
    "def get_training_loss(model, X_train, y_train):\n",
    "    \"\"\"\n",
    "    Compute training loss based on model type.\n",
    "    \"\"\"\n",
    "    # Models that expose their loss during training\n",
    "    if hasattr(model, \"best_score_\"):  # XGBoost\n",
    "        return -model.best_score_\n",
    "\n",
    "    if hasattr(model, \"loss_\"):  # Perceptron (Hinge loss)\n",
    "        return model.loss_\n",
    "\n",
    "    # Probabilistic models (e.g., HMM, Naive Bayes)\n",
    "    if hasattr(model, \"score\"):  \n",
    "        return -model.score(X_train, y_train)  # Negative log-likelihood\n",
    "\n",
    "    # Support Vector Machines (hinge loss)\n",
    "    if isinstance(model, SVC):\n",
    "        y_pred = model.decision_function(X_train)\n",
    "        return np.mean(np.maximum(0, 1 - y_train * y_pred))  # Hinge loss\n",
    "\n",
    "    # Logistic Regression (log loss)\n",
    "    if isinstance(model, LogisticRegression):\n",
    "        y_proba = model.predict_proba(X_train)\n",
    "        return log_loss(y_train, y_proba)\n",
    "\n",
    "    # Decision Tree, Random Forest: No direct loss, use log loss\n",
    "    if isinstance(model, (DecisionTreeClassifier, RandomForestClassifier)):\n",
    "        y_proba = model.predict_proba(X_train)\n",
    "        return log_loss(y_train, y_proba)\n",
    "\n",
    "    return None  # Loss not available\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-04T11:15:08.772285Z",
     "iopub.status.busy": "2025-02-04T11:15:08.771937Z",
     "iopub.status.idle": "2025-02-04T11:15:08.792503Z",
     "shell.execute_reply": "2025-02-04T11:15:08.791616Z",
     "shell.execute_reply.started": "2025-02-04T11:15:08.772239Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def generate_binary_classification_model(X, y, model_algorithm, hyperparameters, needs_scaled = False, model_save_path=\"best_model.pkl\", img_save_path=None, img_loss_path=None):\n",
    "    if os.path.exists(model_save_path):\n",
    "        print(f\"üîÑ Loading existing model from {model_save_path}...\")\n",
    "        model_algorithm = joblib.load(model_save_path)\n",
    "        return model_algorithm\n",
    "    \n",
    "    print(f\"üöÄ Training new model: {model_algorithm.__class__.__name__}...\")\n",
    "    logger.log_message(f\"üöÄ Training new model: {model_algorithm.__class__.__name__}...\")\n",
    "    if needs_scaled:\n",
    "        scaler = StandardScaler()\n",
    "        scaled_features = scaler.fit_transform(X)\n",
    "        X = pd.DataFrame(scaled_features, index = X.index, columns = X.columns)\n",
    "        \n",
    "    gridsearchcv = GridSearchCV(estimator = model_algorithm,\n",
    "                                param_grid = hyperparameters)\n",
    "    gridsearchcv.fit(X, y)\n",
    "    \n",
    "    logger.log_message(f'Best hyperparameters: {gridsearchcv.best_params_}')\n",
    "    \n",
    "    model_algorithm.set_params(**gridsearchcv.best_params_)\n",
    "    \n",
    "    # Creating a container to hold each set of validation metrics\n",
    "    accuracy_scores, roc_auc_scores, f1_scores, precision_scores, recall_scores = [], [], [], [], []\n",
    "    training_losses, validation_losses = [], []\n",
    "    \n",
    "    k_fold = KFold(n_splits = 5)\n",
    "    \n",
    "    for train_index, val_index in tqdm(k_fold.split(X), total=k_fold.get_n_splits(), desc=\"K-Fold Progress\"):\n",
    "        X_train, X_val = X.iloc[train_index, :], X.iloc[val_index, :]\n",
    "        y_train, y_val = y.iloc[train_index], y.iloc[val_index]\n",
    "        model_algorithm.fit(X_train, y_train)\n",
    "        \n",
    "        train_loss = get_training_loss(model_algorithm, X_train, y_train)\n",
    "        val_loss = get_training_loss(model_algorithm, X_val, y_val)\n",
    "\n",
    "        training_losses.append(train_loss)\n",
    "        validation_losses.append(val_loss)\n",
    "\n",
    "        val_preds = model_algorithm.predict(X_val)\n",
    "\n",
    "        val_accuracy = accuracy_score(y_val, val_preds)\n",
    "        val_roc_auc_score = roc_auc_score(y_val, val_preds)\n",
    "        val_f1_score = f1_score(y_val, val_preds)\n",
    "        val_precision_score = precision_score(y_val, val_preds)\n",
    "        val_recall_score = recall_score(y_val, val_preds)\n",
    "        \n",
    "        accuracy_scores.append(val_accuracy)\n",
    "        roc_auc_scores.append(val_roc_auc_score)\n",
    "        f1_scores.append(val_f1_score)\n",
    "        precision_scores.append(val_precision_score)\n",
    "        recall_scores.append(val_recall_score)\n",
    "        \n",
    "    logger.log_message(f'üìä Average Accuracy: {int(mean(accuracy_scores) * 100)}%')\n",
    "    logger.log_message(f'üìä Average ROC AUC: {int(mean(roc_auc_scores) * 100)}%')\n",
    "    logger.log_message(f'üìä Average F1 Score: {int(mean(f1_scores) * 100)}%')\n",
    "    logger.log_message(f'üìä Average Precision: {int(mean(precision_scores) * 100)}%')\n",
    "    logger.log_message(f'üìä Average Recall: {int(mean(recall_scores) * 100)}%')\n",
    "\n",
    "    joblib.dump(model_algorithm, model_save_path)\n",
    "    logger.log_message(f'üíæ Model saved to {model_save_path}')\n",
    "    \n",
    "    if img_save_path:\n",
    "        plt.figure(figsize=(10, 6))\n",
    "\n",
    "        plt.plot(range(1, len(accuracy_scores) + 1), accuracy_scores, label=\"Accuracy\", marker='o')\n",
    "        plt.plot(range(1, len(roc_auc_scores) + 1), roc_auc_scores, label=\"ROC AUC\", marker='o')\n",
    "\n",
    "        plt.title(\"Validation Performance Across K-Folds\")\n",
    "        plt.xlabel(\"Fold Number\")\n",
    "        plt.ylabel(\"Score\")\n",
    "        plt.legend()\n",
    "\n",
    "        plt.savefig(img_save_path)\n",
    "        plt.close()\n",
    "        print(f\"üìà Plot saved to {img_save_path}\")\n",
    "        logger.log_message(f\"üìà Plot saved to {img_save_path}\")\n",
    "        \n",
    "    if img_loss_path:\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.plot(range(1, len(training_losses) + 1), training_losses, label=\"Training Loss\", marker='o')\n",
    "        plt.plot(range(1, len(validation_losses) + 1), validation_losses, label=\"Validation Loss\", marker='o')\n",
    "        plt.title(\"Training & Validation Loss Across K-Folds\")\n",
    "        plt.xlabel(\"Fold Number\")\n",
    "        plt.ylabel(\"Loss\")\n",
    "        plt.legend()\n",
    "        plt.savefig(img_loss_path)\n",
    "        plt.close()\n",
    "        logger.log_message(f\"üìâ Loss plot saved to {img_loss_path}\")\n",
    "    \n",
    "    return model_algorithm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Do here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-04T11:15:08.793700Z",
     "iopub.status.busy": "2025-02-04T11:15:08.793389Z",
     "iopub.status.idle": "2025-02-04T11:15:08.814721Z",
     "shell.execute_reply": "2025-02-04T11:15:08.813810Z",
     "shell.execute_reply.started": "2025-02-04T11:15:08.793679Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Dictionary for models\n",
    "MODEL_DICT = {\n",
    "    \"decision_tree\": DecisionTreeClassifier,\n",
    "    \"perceptron\": Perceptron,\n",
    "    \"bayesian\": GaussianNB,\n",
    "    \"random_forest\": RandomForestClassifier,\n",
    "    \"xgboost\": xgb.XGBClassifier,\n",
    "    \"svm\": SVC,\n",
    "    \"logistic_regression\": LogisticRegression,\n",
    "    \"crf\": lambda: CRF(\n",
    "        algorithm=\"lbfgs\",  # Default: lbfgs\n",
    "        max_iterations=100,  # Limit iterations to prevent overfitting\n",
    "        all_possible_transitions=True\n",
    "    )\n",
    "} \n",
    "\n",
    "# Dictionary for model parameters\n",
    "MODEL_PARAMS = {\n",
    "    \"decision_tree\": {\n",
    "        \"criterion\": [\"gini\", \"entropy\"],\n",
    "        \"max_depth\": [10, 20, 30, 40],\n",
    "        \"min_samples_split\": [2, 5, 10],\n",
    "        \"min_samples_leaf\": [1, 2, 4],\n",
    "        \"max_features\": [\"sqrt\", \"log2\"]\n",
    "    },\n",
    "    \n",
    "    \"perceptron\": {\n",
    "        \"max_iter\": [1000, 2000],\n",
    "        \"tol\": [1e-3, 1e-4],\n",
    "        \"eta0\": [0.001, 0.01, 0.1],\n",
    "        \"penalty\": [None, \"l2\", \"l1\"],\n",
    "        \"alpha\": [0.0001, 0.001, 0.01]\n",
    "    },\n",
    "    \n",
    "    \"bayesian\": {\n",
    "        \"priors\": [None, [0.5, 0.5], [0.4, 0.6], [0.3, 0.7], [0.2, 0.8], [0.1, 0.9], [0.05, 0.95]],\n",
    "        \"var_smoothing\": [1e-9, 1e-8, 1e-7]\n",
    "    },\n",
    "    \n",
    "    \"random_forest\": {\n",
    "        \"n_estimators\": [100, 150],\n",
    "        \"max_depth\": [10],\n",
    "        \"min_samples_split\": [2, 5],\n",
    "        \"min_samples_leaf\": [1, 2],\n",
    "        \"max_features\": [\"sqrt\", \"log2\"]\n",
    "    },\n",
    "    \n",
    "    \"svm\": {\n",
    "        \"kernel\": [\"linear\"],\n",
    "        \"C\": [0.1, 1],\n",
    "        \"gamma\": [\"scale\", \"auto\"]\n",
    "    },\n",
    "    \n",
    "    \"logistic_regression\": {\n",
    "        \"penalty\": [\"l1\", \"l2\", \"elasticnet\", None],\n",
    "        \"C\": [0.1, 1.0, 10.0],\n",
    "        \"max_iter\": [1000, 2000]\n",
    "    }\n",
    "}\n",
    "\n",
    "# Dictionary for dimensionality reduction methods\n",
    "DIMENSIONALITY_REDUCTION_DICT = {\n",
    "    \"pca\": PCA,\n",
    "    \"lda\": LDA,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-04T11:15:08.817756Z",
     "iopub.status.busy": "2025-02-04T11:15:08.817461Z",
     "iopub.status.idle": "2025-02-04T11:15:14.196529Z",
     "shell.execute_reply": "2025-02-04T11:15:14.195388Z",
     "shell.execute_reply.started": "2025-02-04T11:15:08.817734Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "dataset_path = \"/kaggle/input/tweets-clean-posneg-v1\"\n",
    "df = pd.read_csv(f\"{dataset_path}/final_clean_no_neutral_no_duplicates.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-04T11:15:14.198717Z",
     "iopub.status.busy": "2025-02-04T11:15:14.198436Z",
     "iopub.status.idle": "2025-02-04T11:15:14.400973Z",
     "shell.execute_reply": "2025-02-04T11:15:14.399975Z",
     "shell.execute_reply.started": "2025-02-04T11:15:14.198694Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-04T11:15:14.402082Z",
     "iopub.status.busy": "2025-02-04T11:15:14.401777Z",
     "iopub.status.idle": "2025-02-04T11:15:14.413006Z",
     "shell.execute_reply": "2025-02-04T11:15:14.412151Z",
     "shell.execute_reply.started": "2025-02-04T11:15:14.402057Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-04T11:15:14.414354Z",
     "iopub.status.busy": "2025-02-04T11:15:14.413993Z",
     "iopub.status.idle": "2025-02-04T11:15:14.445532Z",
     "shell.execute_reply": "2025-02-04T11:15:14.444776Z",
     "shell.execute_reply.started": "2025-02-04T11:15:14.414319Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "df[\"target\"] = df[\"target\"].replace(4, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-04T11:15:14.446550Z",
     "iopub.status.busy": "2025-02-04T11:15:14.446320Z",
     "iopub.status.idle": "2025-02-04T11:15:14.519888Z",
     "shell.execute_reply": "2025-02-04T11:15:14.519161Z",
     "shell.execute_reply.started": "2025-02-04T11:15:14.446530Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# df_sampled = df.sample(n=100000, random_state=42)\n",
    "df_sampled = df\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    df_sampled[\"text_clean\"], df_sampled[\"target\"], test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "logger.log_message(f\"Training set size: {len(X_train)}\")\n",
    "logger.log_message(f\"Test set size: {len(X_test)}\")\n",
    "logger.log_message(f\"Training labels size: {len(y_train)}\")\n",
    "logger.log_message(f\"Test labels size: {len(y_test)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-04T11:15:14.520966Z",
     "iopub.status.busy": "2025-02-04T11:15:14.520689Z",
     "iopub.status.idle": "2025-02-04T11:15:14.524982Z",
     "shell.execute_reply": "2025-02-04T11:15:14.523987Z",
     "shell.execute_reply.started": "2025-02-04T11:15:14.520945Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# feature_methods = [\"count\", \"word2vec\", \"glove\", \"bert\"]\n",
    "feature_methods = [\"count\", \"tfidf\", \"word2vec\", \"glove\"]\n",
    "X_train_features_dict = {}\n",
    "X_test_features_dict = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-04T11:15:14.526413Z",
     "iopub.status.busy": "2025-02-04T11:15:14.526048Z",
     "iopub.status.idle": "2025-02-04T11:15:14.540782Z",
     "shell.execute_reply": "2025-02-04T11:15:14.539914Z",
     "shell.execute_reply.started": "2025-02-04T11:15:14.526377Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "output_dir = \"/kaggle/working\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-04T11:15:14.542209Z",
     "iopub.status.busy": "2025-02-04T11:15:14.541715Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "print(\"\\nüîé Running feature extraction...\\n\")\n",
    "logger.log_message(\"\\nüîé Running feature extraction...\\n\")\n",
    "\n",
    "for method in tqdm(feature_methods, desc=\"Feature Extraction Progress\"):\n",
    "    print(f\"\\nüîç Processing feature extraction using: {method}...\")\n",
    "\n",
    "    try:\n",
    "        # Initialize FeatureBuilder\n",
    "        feature_builder = FeatureBuilder(method=method, save_dir=os.path.join(output_dir, \"processed\"), reduce_dim=\"pca\", n_components=50)\n",
    "\n",
    "        # Fit and transform training data\n",
    "        X_train_features = feature_builder.fit_transform(X_train.tolist())\n",
    "        print(f\"‚úÖ {method} - X_train_features shape: {X_train_features.shape}\")\n",
    "        \n",
    "        X_train_features_dict[method] = pd.DataFrame(X_train_features)\n",
    "\n",
    "        X_test_features_dict[method] = feature_builder.transform(X_test.tolist())\n",
    "        print(f\"‚úÖ {method} - X_test_features shape: {X_test_features_dict[method].shape}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error with {method}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "model_names = [\"decision_tree\", \"logistic_regression\", \"random_forest\", \"perceptron\", \"bayesian\", \"svm\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "output_dir = \"/kaggle/working\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "print(\"\\nüîé Running model training loop...\\n\")\n",
    "logger.log_message(\"\\nüîé Running feature extraction...\\n\")\n",
    "\n",
    "results = {}\n",
    "for model_name in model_names:\n",
    "    for method in tqdm(feature_methods, desc=\"Feature Extraction Progress\"):\n",
    "        print(f\"Model: {model_name}\")\n",
    "        print(f\"üîç Method: {method}...\")\n",
    "        logger.log_message(f\"Model: {model_name}\")\n",
    "        logger.log_message(f\"üîç Method: {method}...\")\n",
    "        \n",
    "        if model_name == \"cnn\" or model_name == \"lstm\":\n",
    "            try:\n",
    "                # Retrieve feature matrices\n",
    "                X_train_features = np.array(X_train_features_dict[method])\n",
    "                X_test_features = np.array(X_test_features_dict[method])\n",
    "\n",
    "                # Ensure the data has a shape compatible with CNN\n",
    "                X_train_features = X_train_features.reshape(-1, X_train_features.shape[1], 1)\n",
    "                X_test_features = X_test_features.reshape(-1, X_test_features.shape[1], 1)\n",
    "                \n",
    "                # Get input shape dynamically\n",
    "                input_shape = (X_train_features.shape[1], 1)\n",
    "                \n",
    "                # Select model dynamically\n",
    "                if model_name == \"cnn\":\n",
    "                    build_model = lambda hp: build_cnn_model(hp, input_shape)\n",
    "                    project_name = f\"cnn_tuning_{method}\"\n",
    "                elif model_name == \"lstm\":\n",
    "                    build_model = lambda hp: build_lstm_model(hp, input_shape)\n",
    "                    project_name = f\"lstm_tuning_{method}\"\n",
    "                else:\n",
    "                    continue  # Skip unknown models\n",
    "\n",
    "                # Initialize Keras Tuner\n",
    "                tuner = kt.RandomSearch(\n",
    "                    build_model,\n",
    "                    objective=\"val_accuracy\",\n",
    "                    max_trials=3,  # Number of different models to try\n",
    "                    executions_per_trial=1,\n",
    "                    directory=\"tuner_results\",\n",
    "                    project_name=f\"cnn_tuning_{method}\"\n",
    "                )\n",
    "\n",
    "                # Perform hyperparameter search\n",
    "                tuner.search(X_train_features, y_train, epochs=10, batch_size=32, validation_split=0.2, verbose=1)\n",
    "\n",
    "                # Get the best hyperparameters\n",
    "                best_hps = tuner.get_best_hyperparameters(num_trials=1)[0]\n",
    "                best_model = tuner.hypermodel.build(best_hps)\n",
    "\n",
    "                # Train the best model\n",
    "                history = best_model.fit(X_train_features, y_train, epochs=10, batch_size=32, validation_split=0.2, verbose=1)\n",
    "\n",
    "                # Predict on validation set\n",
    "                y_pred_prob = best_model.predict(X_test_features)\n",
    "                y_pred = (y_pred_prob > 0.5).astype(int)\n",
    "\n",
    "                # Compute additional metrics\n",
    "                precision = precision_score(y_test, y_pred)\n",
    "                recall = recall_score(y_test, y_pred)\n",
    "                f1 = f1_score(y_test, y_pred)\n",
    "                roc_auc = roc_auc_score(y_test, y_pred_prob)\n",
    "                \n",
    "                # Store results\n",
    "                results[f\"{model_name}_{method}\"] = {\n",
    "                    \"loss\": history.history[\"loss\"],\n",
    "                    \"val_loss\": history.history[\"val_loss\"],\n",
    "                    \"accuracy\": history.history[\"accuracy\"],\n",
    "                    \"val_accuracy\": history.history[\"val_accuracy\"],\n",
    "                    \"precision\": precision,\n",
    "                    \"recall\": recall,\n",
    "                    \"f1_score\": f1,\n",
    "                    \"roc_auc\": roc_auc\n",
    "                }\n",
    "\n",
    "                logger.log_message(f'loss: {history.history[\"loss\"]}')\n",
    "                logger.log_message(f'val_loss: {history.history[\"val_loss\"]}')\n",
    "                logger.log_message(f'accuracy: {history.history[\"accuracy\"]}')\n",
    "                logger.log_message(f'val_accuracy: {history.history[\"val_accuracy\"]}')\n",
    "                logger.log_message(f'precision: {precision}')\n",
    "                logger.log_message(f'recall: {recall}')\n",
    "                logger.log_message(f'f1_score: {f1}')\n",
    "                logger.log_message(f'roc_auc: {roc_auc}')\n",
    "\n",
    "                # Save the best model\n",
    "                best_model.save(f\"best_{model_name}_{method}.keras\")\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"‚ùå Error with {method}: {e}\")  \n",
    "        else:\n",
    "            try:\n",
    "                # Retrieve Decision Tree model and hyperparameters\n",
    "                algorithm = MODEL_DICT[model_name]()\n",
    "                params = MODEL_PARAMS[model_name]\n",
    "        \n",
    "                # Train or load model\n",
    "                trained_model = generate_binary_classification_model(\n",
    "                    X=X_train_features_dict[method], \n",
    "                    y=y_train, \n",
    "                    model_algorithm=algorithm, \n",
    "                    hyperparameters=params, \n",
    "                    needs_scaled=False, \n",
    "                    model_save_path=f\"best_{model_name}_{method}.pkl\",\n",
    "                    img_save_path=f\"best_{model_name}_{method}.png\",\n",
    "                    img_loss_path=f\"best_{model_name}_{method}_loss.png\"\n",
    "                )\n",
    "        \n",
    "            except Exception as e:\n",
    "                print(f\"‚ùå Error with {method}: {e}\")\n",
    "                logger.log_message(f\"‚ùå Error with {method}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the number of models and feature extraction methods\n",
    "num_methods = len(results.keys())\n",
    "\n",
    "# Create subplots dynamically based on the number of methods\n",
    "fig, axes = plt.subplots(1, num_methods, figsize=(5 * num_methods, 5), sharey=True)\n",
    "\n",
    "# If only one method, ensure `axes` is iterable\n",
    "if num_methods == 1:\n",
    "    axes = [axes]\n",
    "\n",
    "# Plot Training & Validation Loss for each model-method combination\n",
    "for ax, method_key in zip(axes, results.keys()):\n",
    "    ax.plot(results[method_key][\"loss\"], label=\"Train Loss\", color=\"blue\")\n",
    "    ax.plot(results[method_key][\"val_loss\"], label=\"Val Loss\", linestyle=\"dashed\", color=\"red\")\n",
    "    \n",
    "    ax.set_xlabel(\"Epochs\")\n",
    "    ax.set_ylabel(\"Loss\")\n",
    "    \n",
    "    # Extract model and method names from the key\n",
    "    model_name, method = method_key.split(\"_\", 1)\n",
    "    \n",
    "    ax.set_title(f\"Loss Curve - {model_name.upper()} ({method})\")\n",
    "    ax.legend()\n",
    "\n",
    "# Adjust layout\n",
    "plt.tight_layout()\n",
    "\n",
    "# Save the plot\n",
    "plt.savefig(\"loss_curves.png\")\n",
    "print(\"‚úÖ Loss curves saved as loss_curves.png\")\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Predict for each model\n",
    "for model_name in model_names:\n",
    "    for method in feature_methods:\n",
    "        logger.log_message(f\"üîé Predicting with Model: {model_name}, Method: {method}...\")\n",
    "        \n",
    "        try:\n",
    "            if model_name in [\"cnn\", \"lstm\"]:\n",
    "                # Load the saved deep learning model\n",
    "                model_filename = os.path.join(output_dir, f\"best_{model_name}_{method}.keras\")\n",
    "                model = tf.keras.models.load_model(model_filename)\n",
    "\n",
    "                # Retrieve and reshape features for CNN/LSTM\n",
    "                X_test_features = np.array(X_test_features_dict[method])\n",
    "                if model_name == \"lstm\":\n",
    "                    input_shape = (1, X_test_features.shape[1])\n",
    "                    X_test_features = X_test_features.reshape(X_test_features.shape[0], *input_shape)\n",
    "                else:\n",
    "                    input_shape = (X_test_features.shape[1], 1)\n",
    "                    X_test_features = X_test_features.reshape(-1, X_test_features.shape[1], 1)\n",
    "\n",
    "                # Make predictions\n",
    "                y_prob = model.predict(X_test_features).flatten()\n",
    "                y_pred = (y_prob > 0.5).astype(int)\n",
    "\n",
    "            else:  # Handle Machine Learning models\n",
    "                # Load the saved model\n",
    "                model_filename = os.path.join(output_dir, f\"best_{model_name}_{method}.pkl\")\n",
    "                with open(model_filename, 'rb') as model_file:\n",
    "                    model = joblib.load(model_file)\n",
    "\n",
    "                # Make predictions\n",
    "                y_pred = model.predict(X_test_features_dict[method])\n",
    "                \n",
    "                # ROC AUC can be computed if the model outputs probabilities\n",
    "                # Handle models that do not support `predict_proba`\n",
    "                if hasattr(model, \"predict_proba\"):\n",
    "                    y_prob = model.predict_proba(X_test_features_dict[method])[:, 1]  # Take the positive class probabilities\n",
    "                elif hasattr(model, \"decision_function\"):\n",
    "                    y_prob = model.decision_function(X_test_features_dict[method])\n",
    "                else:\n",
    "                    y_prob = None\n",
    "\n",
    "            # Compute metrics\n",
    "            accuracy = accuracy_score(y_test, y_pred)\n",
    "            precision = precision_score(y_test, y_pred, average='binary')\n",
    "            recall = recall_score(y_test, y_pred, average='binary')\n",
    "            f1 = f1_score(y_test, y_pred, average='binary')\n",
    "            roc_auc = roc_auc_score(y_test, y_prob) if y_prob is not None else \"N/A\"\n",
    "\n",
    "            logger.log_message(f\"Model: {model_name}\")\n",
    "            logger.log_message(f\"Method: {method}\")\n",
    "            logger.log_message(\"-\" * 50)\n",
    "            logger.log_message(f\"Accuracy: {accuracy:.4f}\")\n",
    "            logger.log_message(f\"Precision: {precision:.4f}\")\n",
    "            logger.log_message(f\"Recall: {recall:.4f}\")\n",
    "            logger.log_message(f\"F1 Score: {f1:.4f}\")\n",
    "            logger.log_message(f\"ROC AUC: {roc_auc if roc_auc != 'N/A' else 'N/A'}\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error while predicting for {model_name} with {method}: {e}\")\n",
    "            logger.log_message(f\"‚ùå Error while predicting for {model_name} with {method}: {e}\")\n",
    "\n",
    "        \n",
    "    print(\"%\" * 50)\n",
    "    logger.log_message(\"%\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Remove output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# import shutil\n",
    "# import os\n",
    "\n",
    "# # Path to the working directory\n",
    "# working_dir = '/kaggle/working/'\n",
    "\n",
    "# # Remove all files and subdirectories in the working directory\n",
    "# for filename in os.listdir(working_dir):\n",
    "#     file_path = os.path.join(working_dir, filename)\n",
    "#     if os.path.isfile(file_path):\n",
    "#         os.remove(file_path)\n",
    "#     else:\n",
    "#         shutil.rmtree(file_path)\n",
    "\n",
    "# print(\"All outputs cleared from Kaggle working directory.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# import os\n",
    "\n",
    "# # Path to the working directory\n",
    "# working_dir = '/kaggle/working/'\n",
    "\n",
    "# # Loop through files in the working directory\n",
    "# for filename in os.listdir(working_dir):\n",
    "#     if filename.endswith(\".zip\"):  # Check if it's a zip file\n",
    "#         file_path = os.path.join(working_dir, filename)\n",
    "#         os.remove(file_path)  # Remove the zip file\n",
    "#         print(f\"Removed: {file_path}\")\n",
    "\n",
    "# print(\"‚úÖ All zip files removed from Kaggle working directory.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Zip all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# import shutil\n",
    "# import os\n",
    "# import zipfile\n",
    "\n",
    "# # Define working directory and zip file path\n",
    "# working_dir = \"/kaggle/working/\"\n",
    "# zip_file_path = \"/kaggle/working/output_files.zip\"\n",
    "\n",
    "# # Create a zip archive containing only specific files\n",
    "# with zipfile.ZipFile(zip_file_path, 'w', zipfile.ZIP_DEFLATED) as zipf:\n",
    "#     for filename in os.listdir(working_dir):\n",
    "#         if filename.endswith(\".log\") or filename.endswith(\".png\") or filename.endswith(\".pkl\"):\n",
    "#             file_path = os.path.join(working_dir, filename)\n",
    "#             zipf.write(file_path, filename)  # Save file with its name (without full path)\n",
    "\n",
    "# print(f\"‚úÖ Selected outputs zipped into: {zip_file_path}\")"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 6593367,
     "sourceId": 10648520,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30839,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
