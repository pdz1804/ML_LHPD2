\section{Model Comparison for Sentiment Analysis based on Trained Results}

\subsection{Introduction}

This section presents the comparison of various machine learning models trained for sentiment analysis. The models include traditional classifiers such as \textbf{Logistic Regression}, \textbf{Decision Tree}, \textbf{XGBoost}, \textbf{Random Forest}, \textbf{Perceptron}, and \textbf{Naïve Bayes}, along with more complex models like \textbf{CNN-LSTM}, \textbf{HMM}, and \textbf{Bayesian Networks}. Each model's performance is evaluated using key metrics: \textbf{Accuracy}, \textbf{Precision}, \textbf{Recall}, \textbf{F1-score}, and \textbf{ROC AUC}.

\subsubsection{Model Training and Evaluation Workflow}

To ensure a robust evaluation of sentiment classification models, the following steps are undertaken:

\begin{itemize}
    \item \textbf{Instantiating a GridSearch Object:} The model is initialized with a set of hyperparameters using \textit{GridSearchCV}, allowing an exhaustive search over different hyperparameter combinations to identify the optimal settings.
    
    \item \textbf{Fitting the Training Data:} The training dataset is fed into the model, enabling it to learn patterns that distinguish between different sentiment classes.
    
    \item \textbf{K-Fold Cross-Validation:} To enhance generalization, \textit{K-Fold Cross-Validation} is applied, dividing the dataset into multiple subsets to train and validate the model iteratively.
    
    \item \textbf{Saving the Trained Model:} The best-performing model, based on cross-validation results, is stored for future use, ensuring consistency in later inference stages.
    
    \item \textbf{Testing on a Separate Dataset:} The trained model is evaluated on a test dataset to measure its real-world generalization ability, providing a reliable estimate of performance.
    
    \item \textbf{Logging Performance Metrics:} Key metrics such as \textbf{Accuracy}, \textbf{Precision}, \textbf{Recall}, \textbf{F1-score}, and \textbf{ROC AUC} are recorded to facilitate model comparison.
\end{itemize}

This workflow ensures a structured and reliable methodology for training, validating, and comparing sentiment analysis models, aiding in the selection of the most effective approach.

\subsubsection{Evaluation Metrics Overview}

To assess the performance of sentiment analysis models, we utilize five key evaluation metrics: \textbf{Accuracy}, \textbf{Precision}, \textbf{Recall}, \textbf{F1-score}, and \textbf{ROC AUC}. 

\begin{itemize}
    \item \textbf{Accuracy} measures the overall correctness of the model by calculating the proportion of correctly classified instances. However, it may not always be the best metric when dealing with imbalanced sentiment classes. 
    
    \item \textbf{Precision} quantifies the proportion of correctly predicted positive samples out of all predicted positive samples, which is crucial when minimizing false positives, such as in cases where detecting negative sentiment is critical. 
    
    \item Conversely, \textbf{Recall} indicates how well the model identifies actual positive cases, making it essential for scenarios where missing positive sentiment (e.g., detecting customer dissatisfaction) is more detrimental.  
    
    \item The \textbf{F1-score} provides a balanced measure of both precision and recall, ensuring that the model maintains strong predictive power across both metrics. 
    
    \item Lastly, \textbf{ROC AUC} (Receiver Operating Characteristic - Area Under the Curve) evaluates the model's ability to distinguish between different sentiment classes, providing insight into its overall discriminative power. By considering these metrics, we can determine the best-performing model based on different application needs, balancing between false positives and false negatives.
\end{itemize}

\subsection{Model Performance}

\begin{table}[H]
    \centering
    \caption{Performance Comparison of Sentiment Analysis Models}
    \label{tab:sentiment_model_comparison}
    \resizebox{\textwidth}{!}{%
    \begin{tabular}{|l|l|c|c|c|c|c|}
    \hline
    \textbf{Method} & \textbf{Model} & \textbf{Accuracy} & \textbf{Precision} & \textbf{Recall} & \textbf{F1} & \textbf{ROC AUC} \\ \hline
    
    \textbf{count} & \textbf{Logistic Regression} & \underline{\textbf{0.7557}} & 0.7403 & 0.8078 & \underline{\textbf{0.7726}} & \underline{\textbf{0.8297}} \\ \hline
    
    \textbf{tfidf} & \textbf{Decision Tree} & 0.6300 & 0.5928 & 0.8940 & 0.7129 & 0.6694 \\ \hline
    
    \textbf{count} & \textbf{XGBoost} & 0.7251 & 0.6970 & 0.8247 & 0.7555 & 0.8040 \\ \hline
    
    \textbf{count} & \textbf{Random Forest} & 0.7251 & 0.6970 & 0.8247 & 0.7555 & 0.8040 \\ \hline
    
    \textbf{tfidf} & \textbf{Perceptron (ANN)} & 0.6927 & 0.7345 & 0.6296 & 0.6780 & 0.7737 \\ \hline

    \textbf{count} & \textbf{MLP} & 0.7370 & 0.8126 & 0.7526 & 0.7269 & 0.7801 \\ \hline
    
    \textbf{count} & \textbf{GaussianNB} & 0.7134 & 0.7151 & 0.7350 & 0.7250 & 0.7463 \\ \hline
    
    \textbf{count} & \textbf{GaussianNB + GA} & 0.6520 & 0.6860 & 0.6762 & 0.6492 & 0.7056 \\  \hline
    
    \textbf{N/A} & \textbf{CNN-LSTM} & 0.7103 & 0.6740 & 0.8486 & 0.7513 & 0.7891 \\ \hline
    
    \textbf{N/A} & \textbf{HMM} & 0.5141 & 0.5156 & 0.9552 & 0.6697 & 0.4997 \\ \hline
    
    \textbf{N/A} & \textbf{Bayesian Network} & 0.6495 & 0.6364 & 0.7476 & 0.6875 & 0.7143 \\ \hline
    
    \end{tabular}
    }
\end{table}

\subsubsection{Discussion}

\begin{itemize}
    \item \textbf{Embedding Methods:} The choice of embedding methods significantly impacts model performance. 
    \textbf{CountVectorizer} and \textbf{TF-IDF} often yield better results for traditional models as they capture term frequency statistics effectively. 
    In contrast, \textbf{Word2Vec} and \textbf{GloVe} provide dense representations that benefit deep learning models like \textbf{CNN-LSTM}. 
    However, pre-trained embeddings may not always align well with domain-specific datasets, making TF-IDF and CountVectorizer preferable for structured, lexicon-heavy tasks like sentiment classification.

    \item \textbf{Logistic Regression:} The logistic regression model serves as a strong baseline, achieving an accuracy of \textbf{75.57\%}, an F1-score of \textbf{77.26\%}, and the highest ROC AUC of \textbf{82.97\%} among traditional models. It provides balanced performance across all metrics, making it a reliable choice for sentiment classification.

    \item \textbf{Decision Tree:} This model exhibits \textbf{high recall (89.40\%)}, meaning it is effective at capturing positive and negative sentiments. However, its low precision (\textbf{59.28\%}) indicates a high false-positive rate, leading to misclassifications.

    \item \textbf{XGBoost and Random Forest:} Both models deliver \textbf{72.51\% accuracy, 80.40\% ROC AUC, and an F1-score of 75.55\%}. Their ensemble-based decision trees capture complex sentiment patterns better than individual models.

    \item \textbf{Perceptron:} This linear classifier achieves \textbf{69.27\% accuracy}, but struggles with recall (\textbf{62.96\%}), leading to an imbalanced prediction performance.
    
    \item \textbf{MLP (Multi-Layer Perceptron):}  
    The MLP model achieves an accuracy of \textbf{73.70\%}, which is competitive with other traditional models like Logistic Regression and Random Forest. Its precision (\textbf{81.26\%}) is the highest among all models, indicating that it is particularly effective at minimizing false positives. However, its recall (\textbf{75.26\%}) is slightly lower than some other models, suggesting it may miss some positive cases. The MLP's F1-score of \textbf{72.69\%} reflects a good balance between precision and recall, and its ROC AUC of \textbf{78.01\%} shows that it performs well in distinguishing between classes.

    \item \textbf{Naïve Bayes and GA-Optimized Naïve Bayes:} The standard Gaussian Naïve Bayes model achieves \textbf{71.34\% accuracy}, and its GA-optimized variant does not significantly improve performance (\textbf{66.53\% accuracy}).

    \item \textbf{CNN-LSTM:} The deep learning-based CNN-LSTM model achieves \textbf{71.03\% accuracy}, with \textbf{84.86\% recall}, making it useful for capturing contextual sentiment.

    \item \textbf{HMM and Bayesian Network:} These probabilistic models underperform, with the HMM achieving only \textbf{51.41\% accuracy} and a ROC AUC of \textbf{49.97\%}, indicating near-random performance. The Bayesian Network slightly improves to \textbf{64.95\% accuracy} but remains behind tree-based models.

\end{itemize}

\subsubsection{Selecting the Best Model}

\textbf{Our Criteria for Selection the best model:} 

\begin{itemize}
    \item \textbf{Accuracy}: Indicates the overall classification performance of the model.
    \item \textbf{F1-score}: Provides a balance between precision and recall.
    \item \textbf{ROC AUC}: Measures the model’s ability to distinguish between sentiment classes.
\end{itemize}

\begin{table}[H]
\centering
\caption{Best Model Performance Across Key Metrics}
\label{tab:best_model}
\begin{tabular}{|l|l|l|}
\hline
\textbf{Metric} & \textbf{Model} & \textbf{Value} \\ \hline
\textbf{Accuracy} & Logistic Regression (Count) & 0.7557 \\ \hline
\textbf{Precision} & MLP (Count) & 0.8126 \\ \hline
\textbf{Recall} & CNN-LSTM & 0.8486 \\ \hline
\textbf{F1-score} & Logistic Regression (Count) & 0.7726 \\ \hline
\textbf{ROC AUC} & Logistic Regression (Count) & 0.8297 \\ \hline
\end{tabular}
\end{table}

The best-performing model based on accuracy, F1-score, and ROC AUC is \textbf{Logistic Regression} (Count Vectorizer). However, \textbf{CNN-LSTM} achieves the highest recall, making it suitable for applications where recall is the priority. Tree-based models like \textbf{Random Forest} and \textbf{XGBoost} also show strong performance and could be considered when computational efficiency is a concern.

\subsection{Type I and Type II Error Considerations}

Sentiment analysis models must balance two types of errors:

\begin{itemize}
    \item \textbf{Type I Error (False Positives):} Occurs when neutral or negative sentiments are misclassified as positive. This can mislead businesses, causing them to overestimate customer satisfaction. Models with high precision, such as \textbf{Random Forest} and \textbf{Logistic Regression}, help mitigate this issue.
    
    \item \textbf{Type II Error (False Negatives):} Occurs when positive sentiments are misclassified as negative. This can result in missed opportunities for companies to identify positive trends. \textbf{CNN-LSTM}, with its high recall, minimizes this risk by ensuring that positive sentiments are correctly identified.
    
    \item \textbf{Balancing the Errors:} The \textbf{Logistic Regression} model offers a strong trade-off between precision and recall, making it a balanced choice for sentiment classification. On the other hand, \textbf{CNN-LSTM} is more recall-focused, making it suitable for applications where detecting positive sentiment is more critical.
    
    \item \textbf{Impact on Real-World Applications:} Selecting the right model depends on the application's needs. For customer feedback analysis, a high-precision model prevents false alarms about negative sentiments. In contrast, for social media monitoring, a high-recall model ensures no positive trends are overlooked.
\end{itemize}

\section{Comparison with Theory}

This section provides a comparative analysis of sentiment analysis models based on multiple key aspects: feature importance, model behavior, limitations, theoretical expectations, and real-world applicability.

\subsection{Logistic Regression}

\textbf{Feature Importance and Model Behavior Analysis} \\
Logistic Regression is a linear classifier that assigns weights to input features based on their contribution to the classification decision. The model heavily relies on word frequency and importance, making it effective when using feature extraction methods like TF-IDF and Count Vectorization. It performed consistently across datasets, indicating robustness but showing limitations in capturing complex contextual relationships.

\textbf{Limitation Discussion} \\
Despite its efficiency, Logistic Regression assumes linear separability, which limits its ability to handle nuanced sentiment expressions, such as sarcasm or complex multi-word phrases.

\textbf{Comparison with Theory} \\
The model’s high accuracy (75.57\%) and ROC AUC (82.97\%) align with theoretical expectations, proving that simple models can outperform complex ones in structured text classification.

\textbf{Use Case Fit Analysis} \\
Best suited for real-time, scalable applications where interpretability and efficiency are required, such as social media monitoring and customer feedback analysis.

\subsection{Decision Tree}

\textbf{Feature Importance and Model Behavior Analysis} \\
Decision Trees perform recursive partitioning to select the most significant features at each step. The model adapts well to categorical data but is sensitive to noise and class imbalances.

\textbf{Limitation Discussion} \\
It tends to overfit on high-dimensional text data, reducing generalization. The lower precision (59.28\%) indicates frequent misclassification due to splitting on irrelevant features.

\textbf{Comparison with Theory} \\
As expected, Decision Trees struggle with generalization, confirming their known trade-offs between interpretability and robustness.

\textbf{Use Case Fit Analysis} \\
Applicable in controlled environments where feature selection is crucial, but not ideal for large-scale sentiment analysis.

\subsection{XGBoost}

\textbf{Feature Importance and Model Behavior Analysis} \\
XGBoost improves upon Decision Trees by using gradient boosting, assigning greater importance to difficult samples. It effectively captures feature interactions and demonstrated strong predictive power, achieving an F1-score of 82.47\%.

\textbf{Limitation Discussion} \\
Though powerful, XGBoost requires extensive hyperparameter tuning and is computationally expensive, making it less practical for lightweight applications.

\textbf{Comparison with Theory} \\
Empirical results confirm its superiority over single-tree models, but its inability to surpass Logistic Regression in accuracy suggests diminishing returns for feature-rich datasets.

\textbf{Use Case Fit Analysis} \\
Ideal for high-stakes sentiment classification where interpretability is less critical, such as stock market sentiment prediction.

\subsection{Random Forest}

\textbf{Feature Importance and Model Behavior Analysis} \\
Random Forest leverages multiple decision trees to improve stability and reduce overfitting. It exhibited strong generalization, achieving a ROC AUC of 80.40\%.

\textbf{Limitation Discussion} \\
While less prone to overfitting, it remains computationally demanding and lacks the interpretability of simpler models.

\textbf{Comparison with Theory} \\
Its performance aligns with expectations, reinforcing that ensemble methods are powerful but not always necessary.

\textbf{Use Case Fit Analysis} \\
Useful in situations where balanced performance and reliability are needed, such as automated content moderation.

\subsection{Multi-Layer Perceptron (MLP)}

\textbf{Feature Importance and Model Behavior Analysis} \\
MLP captures non-linear patterns using multiple layers of neurons. It performed well in terms of precision (81.26\%) but required extensive tuning.

\textbf{Limitation Discussion} \\
High training time and risk of overfitting limit its real-world application for small-scale tasks.

\textbf{Comparison with Theory} \\
The results support MLP’s theoretical advantages but highlight its sensitivity to data size and tuning complexity.

\textbf{Use Case Fit Analysis} \\
Best for complex sentiment tasks where non-linear interactions are crucial, such as emotion detection in chatbots.

\subsection{CNN-LSTM}

\textbf{Feature Importance and Model Behavior Analysis} \\
CNN-LSTM utilizes convolutional filters to extract features and LSTMs to handle sequential dependencies. It achieved the highest recall (84.86\%) but underperformed in accuracy (71.03\%).

\textbf{Limitation Discussion} \\
Requires substantial computational resources and large datasets to generalize effectively.

\textbf{Comparison with Theory} \\
Its ability to capture context supports deep learning theories, though its practical advantage over traditional methods is limited.

\textbf{Use Case Fit Analysis} \\
Recommended for applications where sequence dependencies are critical, such as long-form sentiment tracking.

\subsection{Naïve Bayes}

\textbf{Feature Importance and Model Behavior Analysis} \\
Naïve Bayes assumes feature independence, making it fast but less adaptable to contextual dependencies. It achieved an accuracy of 71.34\%.

\textbf{Limitation Discussion} \\
The independence assumption limits its effectiveness in real-world language processing.

\textbf{Comparison with Theory} \\
Its performance aligns with theoretical predictions, confirming its viability as a baseline model.

\textbf{Use Case Fit Analysis} \\
Ideal for quick, low-resource sentiment classification tasks, such as spam filtering.

\subsection{Bayesian Networks and Hidden Markov Models (HMM)}

\textbf{Feature Importance and Model Behavior Analysis} \\
These models rely on probabilistic dependencies and temporal patterns. Bayesian Networks achieved an accuracy of 64.95\%, while HMM underperformed at 51.41\%.

\textbf{Limitation Discussion} \\
Complexity and computational inefficiency make them impractical for large-scale applications.

\textbf{Comparison with Theory} \\
The results confirm that these models struggle with high-dimensional text data, reinforcing their limited utility in modern NLP.

\textbf{Use Case Fit Analysis} \\
Better suited for specific niche applications requiring probabilistic inference, such as sentiment evolution tracking over time.

\section{Limitation Discussion}

The evaluation of various models for sentiment analysis on the \textit{Tweets Clean PosNeg v1} dataset using Count Vectorizer embeddings reveals several limitations that impact overall performance.

\subsection{General Limitations}
A primary limitation across all models is their reliance on Count Vectorizer, which captures word frequencies but fails to account for semantic or contextual nuances in tweets. This approach struggles with context-dependent sentiments, such as distinguishing between:

\begin{itemize}
    \item \textit{"I love this product."} (positive sentiment)
    \item \textit{"I love how this product fails."} (negative sentiment)
\end{itemize}

Employing more advanced embeddings, such as Word2Vec, GloVe, or BERT, could address this limitation by capturing deeper semantic relationships.

Another notable limitation is class imbalance sensitivity observed in certain models. For example, HMM (Recall: 0.8522, Precision: 0.5852) and GaussianNB (Recall: 0.7500, Precision: 0.6144) exhibit high recall but low precision, indicating a tendency to overpredict the positive class. This can be problematic when dealing with imbalanced datasets, leading to increased false positives.

Overfitting and poor generalization are evident in models like Decision Tree (Accuracy: 0.6300, ROC AUC: 0.5928) and HMM (Accuracy: 0.6141, ROC AUC: 0.4987), where poor testing performance suggests limited applicability to unseen data.

Finally, computational complexity poses a constraint for certain models. Deep learning approaches such as CNN (Accuracy: 0.7103) and LSTM (Accuracy: 0.7157) require significant computational resources, limiting their practicality in scenarios demanding rapid processing of large tweet volumes.

\subsection{Model-Specific Limitations}

\begin{itemize}
    \item \textbf{Logistic Regression} performs strongly (Accuracy: 0.7557, F1-Score: 0.7726, ROC AUC: 0.8297), but it may struggle with non-linear patterns in tweet data.
    \item \textbf{Decision Tree} (Accuracy: 0.6300, ROC AUC: 0.5928) suffers from overfitting, with unbalanced precision (0.5946) and recall (0.6078).
    \item \textbf{XGBoost} achieves strong performance (Accuracy: 0.7251, ROC AUC: 0.8070), though its slightly lower precision (0.7157) compared to recall (0.7517) suggests a potential bias toward predicting the positive class.
    \item \textbf{Random Forest} offers stability (Accuracy: 0.7231, ROC AUC: 0.8030), but its F1-Score (0.7157) is less competitive than Logistic Regression.
    \item \textbf{MLP} achieves reasonable accuracy (0.7300) and an F1-Score of 0.7260, though its ROC AUC (0.7526) indicates possible overfitting.
    \item \textbf{CNN} and \textbf{LSTM} yield competitive results (Accuracy: 0.7103/0.7157, F1-Score: 0.7513/0.7519), yet their ROC AUC (0.7513/0.7519) is lower than Logistic Regression, and their computational demands are higher.
    \item \textbf{Genetic Algorithm-based model} shows moderate performance (Accuracy: 0.6520, F1-Score: 0.6720, ROC AUC: 0.6800), with an imbalance between precision and recall.
    \item \textbf{HMM} performs poorly (Accuracy: 0.6141, ROC AUC: 0.4987), lacking discriminative power.
    \item \textbf{GaussianNB} (Accuracy: 0.7134, F1-Score: 0.6762) struggles with low precision (0.6144), leading to frequent false positives.
    \item \textbf{Bayesian Network} (Accuracy: 0.6458, F1-Score: 0.6785, ROC AUC: 0.7143) exhibits suboptimal overall performance.
\end{itemize}

\subsection{Conclusion}

Based on the evaluation, \textbf{Logistic Regression with Count Vectorization} demonstrated the best balance of accuracy (75.57\%) and efficiency, making it the optimal choice for most sentiment analysis tasks. XGBoost and Random Forest performed well but were computationally demanding. Deep learning models like CNN-LSTM showed high recall but did not significantly outperform traditional methods in accuracy.

The findings suggest that while complex models have their advantages, traditional machine learning techniques—especially \textbf{Logistic Regression and ensemble methods (XGBoost, Random Forest)}—remain the most practical choices for sentiment classification in structured datasets.

\section{Use-case Fit Analysis of Sentiment Analysis Models}

The performance of various models on the \textit{Tweets Clean PosNeg v1} dataset for sentiment analysis was assessed using key metrics: Accuracy, F1-Score, and ROC AUC. These metrics provide insights into each model's overall accuracy, balance between precision and recall, and ability to distinguish between positive and negative sentiments in tweets.

\subsection{Suitable Models}

\begin{itemize}
    \item \textbf{Logistic Regression} emerges as the most suitable model, achieving the highest accuracy (0.7557), an F1-Score of 0.7726, and an ROC AUC of 0.8297. Its balanced precision (0.7403) and recall (0.7403) ensure reliable classification of both positive and negative tweets, while its low computational complexity makes it well-suited for processing large tweet volumes efficiently.
    \item \textbf{Random Forest} also proves suitable, with an accuracy of 0.7231, an F1-Score of 0.7157, and an ROC AUC of 0.8030. Its stability and robustness against overfitting, compared to Decision Tree, make it a reliable choice.
    \item \textbf{XGBoost} performs well, with an accuracy of 0.7251, an F1-Score of 0.8070, and an ROC AUC of 0.8070. Its high recall (0.7517) ensures that it captures a large proportion of true sentiments, making it effective for comprehensive sentiment analysis.
    \item \textbf{MLP} demonstrates reasonable performance with an accuracy of 0.7300, an F1-Score of 0.7260, and an ROC AUC of 0.7526. Its high precision (0.7290) makes it suitable for applications where minimizing false positives is critical.
    \item \textbf{CNN and LSTM} are viable options, with accuracies of 0.7103 and 0.7157, F1-Scores of 0.7513 and 0.7519, and ROC AUCs of 0.7513 and 0.7519, respectively. LSTM, in particular, is effective for capturing sequential patterns in tweets, as evidenced by its high recall (0.7517).
\end{itemize}

\subsection{Unsuitable Models}

\begin{itemize}
    \item \textbf{HMM} is the least suitable model, with an accuracy of 0.6141, an F1-Score of 0.6977, and an ROC AUC of 0.4987, nearly equivalent to random guessing. Its low precision (0.5852) results in frequent false positives, rendering it unreliable for tweet sentiment classification.
    \item \textbf{Decision Tree} is also unsuitable, with an accuracy of 0.6300, an F1-Score of 0.5946, and an ROC AUC of 0.5928. Its low performance and unbalanced precision-recall indicate overfitting and poor generalization.
    \item \textbf{Genetic Algorithm-based model} is not well-suited, with an accuracy of 0.6520, an F1-Score of 0.6720, and an ROC AUC of 0.6800, underperforming compared to Logistic Regression and XGBoost.
    \item \textbf{GaussianNB}, despite an accuracy of 0.7134, has a low F1-Score of 0.6762 and a precision of 0.6144, leading to excessive false positives and a weaker ROC AUC (0.7143).
    \item \textbf{Bayesian Network} (Accuracy: 0.6458, F1-Score: 0.6785, ROC AUC: 0.7143) fails to deliver competitive performance.
\end{itemize}

\newpage
