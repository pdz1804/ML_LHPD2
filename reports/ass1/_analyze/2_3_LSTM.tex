\subsection{Model: Long Short-Term Memory}

\subsubsection{Introduction}

Long Short-Term Memory (LSTM) networks are a special class of recurrent neural networks designed to capture long-term dependencies in sequential data. By incorporating memory cells and gating mechanisms, LSTMs help alleviate the vanishing or exploding gradient issues commonly found in vanilla RNNs. In this experiment, we integrate a CNN-LSTM pipeline for text-based sentiment classification, leveraging convolutional layers for local feature extraction and LSTM layers for sequential modeling.

\subsubsection{Training Configuration}

The CNN-LSTM training procedure is implemented in the \texttt{train\_cnn\_lstm} function, which handles text tokenization, model building, hyperparameter tuning, and final evaluation. Key configurations include:

\begin{itemize}
    \item \textbf{Vocabulary Size} (\texttt{vocab\_size}): 10{,}000 words, restricting the tokenized vocabulary to the most frequent terms.
    \item \textbf{Sequence Length} (\texttt{max\_length}): 500 tokens per input sequence, with shorter texts padded (or longer texts truncated).
    \item \textbf{Embedding Dimension} (\texttt{embedding\_dim}): 100, defining the size of word embedding vectors.
    \item \textbf{CNN Blocks}: 
    \begin{itemize}
        \item \texttt{filters\_1} and \texttt{filters\_2} chosen from \{64, 128, 192, 256, 384, 512\}.
        \item \texttt{kernel\_size\_1} and \texttt{kernel\_size\_2} chosen from \{3, 5, 7\}, \{3, 5\} respectively.
        \item \textbf{Activation}: ReLU, with max-pooling for dimensionality reduction.
        \item \textbf{BatchNormalization}: Stabilizes intermediate activations.
    \end{itemize}
    \item \textbf{LSTM Layer}:
    \begin{itemize}
        \item \texttt{lstm\_units} chosen from \{64, 128, 192, 256\}.
        \item Bi-directional LSTM configuration for better capture of contextual information.
    \end{itemize}
    \item \textbf{Fully Connected Layer}:
    \begin{itemize}
        \item \texttt{dense\_units} chosen from \{128, 256, 384, 512\} with ReLU activation.
        \item \texttt{dropout} from \{0.3, 0.4, 0.5, 0.6\} to combat overfitting.
    \end{itemize}
    \item \textbf{Optimizer}: Adam with \texttt{learning\_rate} in \{5e-4, 1e-4, 5e-5, 1e-5\}.
    \item \textbf{Epochs}: 10 (default), balanced against computational constraints.
\end{itemize}

We perform Keras Tuner-based random search (\texttt{RandomSearch}) over the hyperparameters to identify an optimal set of configurations, determined by validation accuracy.

\subsubsection{Training and Evaluation Results}

Throughout training, the best hyperparameter set was:

\begin{verbatim}
{
    "filters_1": 64,
    "kernel_size_1": 3,
    "filters_2": 128,
    "kernel_size_2": 3,
    "lstm_units": 64,
    "dense_units": 128,
    "dropout": 0.3,
    "learning_rate": 0.0005
}
\end{verbatim}

Using these final hyperparameters, the model was retrained and evaluated on a withheld test set. The primary performance metrics—accuracy, precision, recall, F1-score, and ROC AUC—are summarized below:

\begin{table}[H]
\centering
\caption{CNN-LSTM Testing Performance}
\label{tab:lstm-testing-metrics}
\begin{tabular}{|l|c|}
\hline
\textbf{Metric} & \textbf{Value} \\ \hline
Accuracy & 0.7103 \\
Precision & 0.6740 \\
Recall & 0.8486 \\
F1-Score & 0.7513 \\
ROC AUC & 0.7891 \\
\hline
\end{tabular}
\end{table}

\subsubsection{Performance Analysis}

\begin{itemize}
    \item \textbf{Accuracy Analysis}: Achieving $\sim71\%$ accuracy suggests the model captures key linguistic cues, though there is room for improvement.
    \item \textbf{Precision and Recall}: The relatively high recall (84.86\%) indicates that the model correctly identifies a substantial fraction of positive cases, but occasionally misclassifies negative samples (precision at 67.40\%).
    \item \textbf{ROC AUC}: The AUC of 0.7891 denotes satisfactory discriminative capability.
    \item \textbf{Model Complexity}: With both convolutional and LSTM components, the model effectively extracts local phrase structures (CNN) and long-range dependencies (LSTM). However, it is more computationally intensive than simpler baselines.
    \item \textbf{Hyperparameter Influence}: Filter sizes (3), lower CNN filter counts, and moderate \texttt{lstm\_units} (64) struck a good balance between underfitting and overfitting, aided by dropout (0.3) to mitigate over-training.
\end{itemize}

\subsubsection{Computational Efficiency Analysis}

Evaluating the computational cost of the CNN-LSTM model is essential for understanding its feasibility in large-scale text classification tasks. We analyze time consumption, memory usage, and hardware requirements based on a dataset containing approximately 500,000 text samples for binary classification.

\textbf{Hardware Configuration}

The experiments were conducted on the following system:\begin{itemize}
\item CPU: Intel Core i7-12700K (12 cores, 20 threads)\item GPU: NVIDIA RTX 3060 (12GB VRAM)\item RAM: 32GB DDR4\end{itemize}

\textbf{Training Time}

The training time per epoch varies depending on batch size and sequence length. Estimated training durations for different configurations are:

\begin{table}[H]
    \centering
    \caption{Estimated Training Time for CNN-LSTM (Single Model)}
    \label{tab:cnn-lstm-training-time}
    \begin{tabular}{|l|c|}
    \hline
    \textbf{Batch Size} & \textbf{Training Time per Epoch (Minutes)} \\
    \hline
    32 & $\approx$ 15 min \\
    \hline
    64 & $\approx$ 12 min \\
    \hline
    128 & $\approx$ 9 min \\
    \hline
    \end{tabular}
\end{table}
    

The entire training process (10 epochs) requires approximately 90-150 minutes, depending on batch size and optimization settings.

\textbf{Memory Consumption}

The model’s memory footprint depends on the feature representation and batch processing:

\begin{itemize}
\item Embedding Layer: Requires approximately 1GB RAM for a vocabulary of 10,000 words with 100-dimensional embeddings.
\item CNN Layers: Adds $\approx$2-3GB RAM overhead due to filter operations and batch normalization.
\item LSTM Layer: Consumes $\approx$3-4GB RAM, influenced by sequence length and bi-directional computation.
\item Total GPU Memory Usage: Approximately 7-9GB VRAM is required for efficient training.
\item Disk Storage: The trained model occupies $\approx$100-200MB, depending on architecture depth.
\end{itemize}

\textbf{Resource Utilization}

\begin{itemize}
\item \textbf{CPU Usage}: 90-100% during data preprocessing and augmentation.
\item \textbf{GPU Usage}: 70-90% utilization during training, depending on batch size.
\item \textbf{Disk I/O}: Moderate, primarily during dataset loading and model checkpointing.
\end{itemize}

\textbf{Summary}The CNN-LSTM model is computationally more expensive than traditional MLPs due to convolutional and sequential processing. However, its memory and processing requirements remain feasible for modern mid-range GPUs. Further optimizations, such as reducing sequence length, adjusting filter sizes, or utilizing quantization techniques, could improve efficiency without significantly impacting performance.

\subsubsection{Conclusion}

By combining CNN layers for local pattern extraction with LSTM units for long-term sequence modeling, the proposed architecture demonstrates competent text classification performance. The best model configuration achieved 71.03\% accuracy and an F1 of 0.7513, making it a robust baseline for tasks involving longer texts or richer linguistic structures. For future improvements, one might explore:
\begin{itemize}
    \item Advanced regularization or fine-tuning strategies (e.g., additional Dropout or data augmentation).
    \item Pre-trained word embeddings (e.g., GloVe, FastText) or transformer-based embedding approaches.
    \item Deeper stacking of LSTM layers, attention mechanisms, or bidirectional LSTMs to enhance context capture.
\end{itemize}

Overall, the CNN-LSTM model successfully balances feature extraction and sequence learning, demonstrating the viability of hybrid architectures in end-to-end text classification.

\newpage