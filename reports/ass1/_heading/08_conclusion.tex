\begin{thebibliography}{99}

    \bibitem{bib1}
    Abu-Mostafa, Y. S., Magdon-Ismail, M., \& Lin, H.-T. (2012). \textit{Learning from data} (Vol. 4). AMLBook. Retrieved from \href{http://work.caltech.edu/telecourse.html}
    
    \bibitem{bib2}
    Bennett, K. P. (1992). Robust linear programming discrimination of two linearly separable sets. \textit{Optimization Methods and Software, 1}.
    
    \bibitem{bib3}
    Bishop, C. M. (2006). \textit{Pattern recognition and machine learning}. Springer. Retrieved from \href{http://users.isr.ist.utl.pt/~wurmd/Livros/school/Bishop%20-%20Pattern%20Recognition%20And%20Machine%20Learning%20-%20Springer%20%202006.pdf}
    
    \bibitem{bib4}
    Boyd, S., \& Vandenberghe, L. (2004). \textit{Convex optimization}. Cambridge University Press. Retrieved from \href{http://stanford.edu/~boyd/cvxbook/}
    
    \bibitem{bib5}
    Cox, D. R. (1958). The regression analysis of binary sequences. \textit{Journal of the Royal Statistical Society. Series B (Methodological)}, 215–242.
    
    \bibitem{bib6}
    Cramer, J. S. (2002). The origins of logistic regression.
    
    \bibitem{bib7}
    Duda, R. O., Hart, P. E., \& Stork, D. G. (2012). \textit{Pattern classification}. John Wiley  \& Sons.
    
    \bibitem{bib8}
    Friedman, J. H., Tibshirani, R.,  \& Hastie, T. (2009). \textit{The elements of statistical learning: Data mining, inference, and prediction}. Springer. Retrieved from \href{https://statweb.stanford.edu/~tibs/}
    
    \bibitem{bib9}
    McCulloch, W. S.,  \& Pitts, W. (1943). A logical calculus of the ideas immanent in nervous activity. \textit{The Bulletin of Mathematical Biophysics, 5}(4), 115–133.
    
    \bibitem{bib10}
    Nesterov, Y. (1983). A method for unconstrained convex minimization problem with the rate of convergence \(O(1/k^2)\). \textit{Soviet Mathematics Doklady, 269}, 543–547.
    
    \bibitem{bib11}
    Ng, A. (n.d.). CS229 lecture notes: Classification and logistic regression. Retrieved from \href{https://datajobs.com/data-science-repo/Generalized-Linear-Models-[Andrew-Ng].pdf
    
    \bibitem{bib12}
    Nguyen, D. (2017). Support vector machine. Retrieved from \href{https://dukn.github.io/MLDL/basicmachinelearning/2017/05/04/SVM/
    
    \bibitem{bib13}
    Rosenblatt, F. (1957). The perceptron, a perceiving and recognizing automaton. Project Para, Cornell Aeronautical Laboratory.
    
    \bibitem{bib14}
    Rosasco, L., De Vito, E. D., Caponnetto, A., Piana, M.,  \& Verri, A. (2004). Are loss functions all the same? \textit{Neural Computation, 16}(5), 1063–1076. Retrieved from \href{http://web.mit.edu/lrosasco/www/publications/loss.pdf}
    
    \bibitem{bib15}
    Schölkopf, B., Smola, A., Williamson, R. C.,  \& Bartlett, P. L. (2000). New support vector algorithms. \textit{Neural Computation, 12}(5), 1207–1245. Retrieved from \href{http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.94.2928&rep=rep1&type=pdf}
    
    \bibitem{bib16}
    Stanford University. (n.d.). CS231n: Convolutional neural networks for visual recognition. Retrieved from \href{http://cs231n.github.io/linear-classify/}
    
    \bibitem{bib17}
    CVXOPT. (n.d.). Retrieved from \href{http://cvxopt.org/
    
    \bibitem{bib18}
    Wikipedia contributors. (n.d.). Hinge loss. In \textit{Wikipedia, The Free Encyclopedia}. Retrieved from \href{https://en.wikipedia.org/wiki/Hinge_loss}
    
    \bibitem{bib19}
    Wikipedia contributors. (n.d.). Kernel method. In \textit{Wikipedia, The Free Encyclopedia}. Retrieved from \href{https://en.wikipedia.org/wiki/Kernel_method}
    
    \bibitem{bib20}
    Chang, C.-C.,  \& Lin, C.-J. (n.d.). LIBSVM: A library for support vector machines. Retrieved from \href{https://www.csie.ntu.edu.tw/~cjlin/libsvm/}
    
    \bibitem{bib21}
    Wikipedia contributors. (n.d.). Newton's method. In \textit{Wikipedia, The Free Encyclopedia}. Retrieved from \href{https://en.wikipedia.org/wiki/Newton's_method}
    
    \bibitem{bib22}
    Frederickson, B. (2016). An interactive tutorial on numerical optimization. Retrieved from \href{http://www.benfrederickson.com/numerical-optimization/}
    
    \bibitem{bib23}
    Scikit-learn contributors. (n.d.). Kernel functions. Retrieved from \href{http://scikit-learn.org/stable/modules/svm.html#svm-kernels}
    
    \bibitem{bib24}
    Scikit-learn contributors. (n.d.). sklearn.svm.SVC. Retrieved from \href{http://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html}
    
    \bibitem{bib25}
    Widrow, B., Hoff, M. E.,  \& others. (1960). Adaptive "Adaline" neuron using chemical "memistors." \textit{Stanford Electronics Labs Technical Report, 1553-2}.
    
    \bibitem{bib26}
    Ng, A. (n.d.). CS229: Machine learning (Lecture notes).
    
    \bibitem{bib27}
    Vu Huu, T. (n.d.). Machine learning co ban. Retrieved from \href{https://machinelearningcoban.com/}

    \bibitem{datasetdefinition}
    Wikipedia contributors. (n.d.). Data set. In \textit{Wikipedia, The Free Encyclopedia}. Retrieved from \href{https://en.wikipedia.org/wiki/Data_set}

    \bibitem{opendata}
    European Data Portal. (n.d.). \textit{Data.europa.eu}. Retrieved from \href{https://data.europa.eu/}

    \bibitem{datasetproperties}
    Various statistical sources. (n.d.). Information on dataset structure and measurement. [No specific retrieval link provided].

    \bibitem{statisticsorigin}
    [Author Unspecified]. (n.d.). Statistical origins and software applications of datasets. [No specific retrieval link provided].

    \bibitem{irisflower}
    Fisher, R. A. (1936). The use of multiple measurements in taxonomic problems. Annals of Eugenics, 7(2), 179–188. Retrieved from \href{https://doi.org/10.1111/j.1469-1809.1936.tb02137.x}

    \bibitem{categoricaldata}
    UCLA Advanced Research Computing. (n.d.). Categorical data analysis datasets. Retrieved from \href{https://stats.idre.ucla.edu}

    \bibitem{robuststatistics}
    University of Cologne. (n.d.). Robust statistics datasets. Retrieved from \href{https://robust.uni-koeln.de/}

    \bibitem{timeseries}
    StatLib. (n.d.). Time series datasets from Chatfield's book. Retrieved from \href{https://lib.stat.cmu.edu/}

    \bibitem{bayesian}
    Gelman, A. (n.d.). Bayesian data analysis datasets. [Archive link not provided].

    \bibitem{gfg_voting}
    GeeksforGeeks. (n.d.). Voting in machine learning. Retrieved from \href{https://www.geeksforgeeks.org/voting-in-machine-learning/}

    \bibitem{soulpage_voting}
    SoulPage. (n.d.). Ensemble voting explained. Retrieved from \href{https://soulpageit.com/ai-glossary/ensemble-voting-explained/}

    \bibitem{aiml_weak_vs_strong}
    AIML. (n.d.). Distinguish between a weak learner vs strong learner. Retrieved from \href{https://aiml.com/distinguish-between-a-weak-learner-vs-strong-learner/}

    \bibitem{gfg_voting_regressor}
    GeeksforGeeks. (n.d.). Voting regressor. Retrieved from \href{https://www.geeksforgeeks.org/voting-regressor/}

    \bibitem{gfg_voting_classifier}
    GeeksforGeeks. (n.d.). Voting classifier. Retrieved from \href{https://www.geeksforgeeks.org/voting-classifier/}

    \bibitem{alpaydin2020}
    Alpaydin, E. (2020). \textit{Introduction to machine learning} (4th ed.). MIT Press.

    \bibitem{hawkins1980}
    Hawkins, D. M. (1980). \textit{Identification of outliers}. Chapman and Hall.

    \bibitem{salgado2020}
    Salgado, C. M., Azevedo, C., Proença, H.,  \& Vieira, S. M. (2020). \textit{Noise versus outliers}. Open Learning Library (MIT).

    \bibitem{qualtrics_sampling}
    Qualtrics. (n.d.). Sampling methods. \textit{Experience Management Research}. Retrieved from \href{https://www.qualtrics.com/en-au/experience-management/research/sampling-methods/}

    \bibitem{byjus_sampling}
    BYJU's. (n.d.). Sampling methods. \textit{Maths Resource}. Retrieved from \href{https://byjus.com/maths/sampling-methods/}

    \bibitem{kearns1988}
    Kearns, M.,  \& Valiant, L. (1988). On the learnability of Boolean formulae. In \textit{Proceedings of the 21st Annual ACM Symposium on Theory of Computing} (pp. 129–133). ACM. Retrieved from \href{https://doi.org/10.1145/62212.62222}

    \bibitem{schapire1990}
    Schapire, R. E. (1990). The strength of weak learnability. \textit{Machine Learning, 5}(2), 197–227. Retrieved from \href{https://doi.org/10.1023/A:1022648800760}

    \bibitem{freund1995}
    Freund, Y.,  \& Schapire, R. E. (1995). A decision-theoretic generalization of on-line learning and an application to boosting. \textit{Journal of Computer and System Sciences, 55}(1), 119–139. Retrieved from \href{https://doi.org/10.1006/jcss.1997.1504}

    \bibitem{freund1996experiments}
    Freund, Y.,  \& Schapire, R. E. (1996). Experiments with a new boosting algorithm. In \textit{Proceedings of the Thirteenth International Conference on Machine Learning} (pp. 148–156). Morgan Kaufmann.

    \bibitem{friedman2001greedy}
    Friedman, J. H. (2001). Greedy function approximation: A gradient boosting machine. \textit{Annals of Statistics, 29}(5), 1189–1232. Retrieved from \href{https://doi.org/10.1214/aos/1013203451}

    \bibitem{Goodfellow}
    Goodfellow, I., Bengio, Y.,  \& Courville, A. (2016). \textit{Deep learning}. MIT Press. Retrieved from \href{http://www.deeplearningbook.org/}

    \bibitem{Olah}
    Olah, C. (2015). Understanding LSTMs. Retrieved from \href{https://colah.github.io/posts/2015-08-Understanding-LSTMs/}

    \bibitem{Sutskever}
    Sutskever, I., Vinyals, O.,  \& Le, Q. V. (2014). Learning phrase representations using RNN encoder–decoder for statistical machine translation. In \textit{Proceedings of NeurIPS}. Retrieved from \href{https://arxiv.org/abs/1406.1078}

    \bibitem{Nielsen}
    Nielsen, M. (2015). \textit{Neural networks and deep learning}. Retrieved from \href{http://neuralnetworksanddeeplearning.com/}

    \bibitem{HechtNielsen}
    Hecht-Nielsen, R. (1989). Theory of the backpropagation neural network. In \textit{IEEE International Conference on Neural Networks}.

    \bibitem{LeCun}
    LeCun, Y., Bengio, Y.,  \& Hinton, G. (2015). Deep learning. \textit{Nature, 521}(7553), 436–444. Retrieved from \href{https://doi.org/10.1038/nature14539}

    \bibitem{Krizhevsky}
    Krizhevsky, A., Sutskever, I.,  \& Hinton, G. E. (2012). ImageNet classification with deep convolutional neural networks. In \textit{Advances in Neural Information Processing Systems}.

    \bibitem{Rumelhart}
    Rumelhart, D. E., Hinton, G. E.,  \& Williams, R. J. (1986). Learning representations by backpropagating errors. \textit{Nature, 323}(6088), 533–536. Retrieved from \href{https://doi.org/10.1038/323533a0}

    \bibitem{Hochreiter}
    Hochreiter, S.,  \& Schmidhuber, J. (1997). Long short-term memory. \textit{Neural Computation, 9}(8), 1735–1780. Retrieved from \href{https://doi.org/10.1162/neco.1997.9.8.1735}

    \bibitem{Cho}
    Cho, K., et al. (2014). Learning phrase representations using RNN encoder–decoder for statistical machine translation. In \textit{Proceedings of EMNLP}.

    \bibitem{MacKay}
    MacKay, D. J. C. (1999). Bayesian model comparison. \textit{Machine Learning, 35}(1), 7–38. Retrieved from \href{https://doi.org/10.1023/A:1007566608920}

    \bibitem{Kingma}
    Kingma, D. P.,  \& Welling, M. (2014). Auto-encoding variational Bayes. In \textit{International Conference on Learning Representations (ICLR)}.

    \bibitem{Vaswani}
    Vaswani, A., et al. (2017). Attention is all you need. In \textit{Advances in Neural Information Processing Systems}. Retrieved from \href{https://arxiv.org/abs/1706.03762}

    \bibitem{Bahdanau}
    Bahdanau, D., Cho, K.,  \& Bengio, Y. (2014). Neural machine translation by jointly learning to align and translate. \textit{arXiv preprint arXiv:1409.0473}. Retrieved from \href{https://arxiv.org/abs/1409.0473}

    \bibitem{Luong}
    Luong, M.-T., Pham, H.,  \& Manning, C. D. (2015). Effective approaches to attention-based neural machine translation. In \textit{Proceedings of EMNLP}. Retrieved from \href{https://arxiv.org/abs/1508.04025}

    \bibitem{Devlin}
    Devlin, J., et al. (2019). BERT: Pre-training of deep bidirectional transformers for language understanding. In \textit{Proceedings of NAACL-HLT}. Retrieved from \href{https://arxiv.org/abs/1810.04805}

    \bibitem{Radford}
    Radford, A., et al. (2018). Improving language understanding by generative pre-training. \textit{OpenAI}. Retrieved from \href{https://www.openai.com/research/language-understanding-generative-pretraining}

    \bibitem{Brown}
    Brown, T., et al. (2020). Language models are few-shot learners. In \textit{Advances in Neural Information Processing Systems}. Retrieved from \href{https://arxiv.org/abs/2005.14165}

    \bibitem{Murphy}
    Murphy, K. P. (2012). \textit{Machine learning: A probabilistic perspective}. MIT Press.

    \bibitem{HintonDistill}
    Hinton, G., Vinyals, O.,  \& Dean, J. (2015). Distilling the knowledge in a neural network. \textit{arXiv preprint arXiv:1503.02531}. Retrieved from \href{https://arxiv.org/abs/1503.02531}

    \bibitem{Jang}
    Jang, E., Gu, S.,  \& Poole, B. (2016). Categorical reparameterization with Gumbel-Softmax. \textit{arXiv preprint arXiv:1611.01144}. Retrieved from \href{https://arxiv.org/abs/1611.01144}

    
\end{thebibliography}
    