\begin{thebibliography}{99}

    \bibitem{bib1}
    \href{http://work.caltech.edu/telecourse.html}{Abu-Mostafa, Yaser S., Malik Magdon-Ismail, and Hsuan-Tien Lin, \textit{Learning from Data}. Vol. 4. New York, NY, USA: AMLBook, 2012.}

    \bibitem{bib2}
    \href{http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.23.3307&rep=rep1&type=pdf}{Bennett, K. P., \textit{Robust Linear Programming Discrimination of Two Linearly Separable Sets}. Optimization Methods and Software, Vol. 1, 1992.}

    \bibitem{bib3}
    \href{http://users.isr.ist.utl.pt/~wurmd/Livros/school/Bishop%20-%20Pattern%20Recognition%20And%20Machine%20Learning%20-%20Springer%20%202006.pdf}{Bishop, Christopher M., \textit{Pattern Recognition and Machine Learning}. Springer, 2006.}

    \bibitem{bib4}
    \href{http://stanford.edu/~boyd/cvxbook/}{Boyd, S., and L. Vandenberghe, \textit{Convex Optimization}. Cambridge University Press, 2004.}

    \bibitem{bib5}
    {Cox, David R., \textit{The Regression Analysis of Binary Sequences}. Journal of the Royal Statistical Society. Series B (Methodological), 1958.}

    \bibitem{bib6}
    {Cramer, Jan Salomon, \textit{The Origins of Logistic Regression}, 2002.}

    \bibitem{bib7}
    {Duda, Richard O., Peter E. Hart, and David G. Stork, \textit{Pattern Classification}. John Wiley \& Sons, 2012.}

    \bibitem{bib8}
    \href{https://statweb.stanford.edu/~tibs/}{Friedman, Jerome H., Robert Tibshirani, and Trevor Hastie, \textit{The Elements of Statistical Learning: Data Mining, Inference, and Prediction}.}

    \bibitem{bib9}
    {McCulloch, W. S., and W. Pitts, \textit{A Logical Calculus of the Ideas Immanent in Nervous Activity}. The Bulletin of Mathematical Biophysics, 1943.}

    \bibitem{bib10}
    {Nesterov, Y., \textit{A Method for Unconstrained Convex Minimization Problem with the Rate of Convergence \(O(1/k^2)\)}. Soviet Math. Dokl., vol. 269, 1983.}

    \bibitem{bib11}
    \href{https://datajobs.com/data-science-repo/Generalized-Linear-Models-[Andrew-Ng].pdf}{Ng, Andrew, \textit{CS229 Lecture Notes: Classification and Logistic Regression}.}

    \bibitem{bib12}
    \href{https://dukn.github.io/MLDL/basicmachinelearning/2017/05/04/SVM/}{Duc Nguyen, \textit{Support Vector Machine}.}

    \bibitem{bib13}
    {Rosenblatt, F., \textit{The Perceptron, a Perceiving and Recognizing Automaton}. Project Para, Cornell Aeronautical Laboratory, 1957.}

    \bibitem{bib14}
    \href{http://web.mit.edu/lrosasco/www/publications/loss.pdf}{Rosasco, L., E. D. De Vito, A. Caponnetto, M. Piana, and A. Verri, \textit{Are Loss Functions All the Same?} Neural Computation, 2004.}

    \bibitem{bib15}
    \href{http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.94.2928&rep=rep1&type=pdf}{Schölkopf, B., A. Smola, R. C. Williamson, and P. L. Bartlett, \textit{New Support Vector Algorithms}. Neural Computation, 2000.}

    \bibitem{bib16}
    \href{http://cs231n.github.io/linear-classify/}{CS231n: Convolutional Neural Networks for Visual Recognition.}

    \bibitem{bib17}
    \href{http://cvxopt.org/}{CVXOPT}

    \bibitem{bib18}
    \href{https://en.wikipedia.org/wiki/Hinge_loss}{Wikipedia, \textit{Hinge Loss}}

    \bibitem{bib19}
    \href{https://en.wikipedia.org/wiki/Kernel_method}{Wikipedia, \textit{Kernel Method}}

    \bibitem{bib20}
    \href{https://www.csie.ntu.edu.tw/~cjlin/libsvm/}{LIBSVM – A Library for Support Vector Machines}

    \bibitem{bib21}
    \href{https://en.wikipedia.org/wiki/Newton's_method}{Wikipedia, \textit{Newton's Method}}

    \bibitem{bib22}
    \href{http://www.benfrederickson.com/numerical-optimization/}{Ben Frederickson, \textit{An Interactive Tutorial on Numerical Optimization}, 2016.}

    \bibitem{bib23}
    \href{http://scikit-learn.org/stable/modules/svm.html#svm-kernels}{Sklearn Kernel Functions}

    \bibitem{bib24}
    \href{http://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html}{\texttt{The Support Vector Machine's API from sklearn, sklearn.svm.SVC}}

    \bibitem{bib25}
    {Widrow, B., et al., \textit{Adaptive "Adaline" Neuron Using Chemical "Memistors"}. Technical Report 1553-2, Stanford Electronics Labs, Stanford, CA, October 1960.}

    \bibitem{bib26}
    Andrew Ng, \textit{CS229: Machine Learning (Lecture Notes)}

    \bibitem{bib27}
    \href{https://machinelearningcoban.com/}{Tiep-Vu Huu, \textit{Machine Learning co ban}}

    \bibitem{datasetdefinition} Wikipedia contributors. "Data set." \textit{Wikipedia, The Free Encyclopedia}, \url{https://en.wikipedia.org/wiki/Data_set}.
    
    \bibitem{opendata} European data portal, \textit{data.europa.eu}.
    
    \bibitem{datasetproperties} Various statistical sources on dataset structure and measurement.
    
    \bibitem{statisticsorigin} Information on the statistical origins and software applications of datasets.
    
    \bibitem{irisflower} Fisher, R.A. \textit{Iris flower data set}, 1936.
    
    \bibitem{categoricaldata} UCLA Advanced Research Computing. \textit{Categorical data analysis datasets}.
    
    \bibitem{robuststatistics} University of Cologne. \textit{Robust statistics datasets}.
    
    \bibitem{timeseries} StatLib. \textit{Time series datasets from Chatfield's book}.
    
    \bibitem{bayesian} Gelman, A. \textit{Bayesian data analysis datasets}.
    
    \bibitem{gfg_voting} GeeksforGeeks. \textit{Voting in Machine Learning}. Available at: \url{https://www.geeksforgeeks.org/voting-in-machine-learning/}
    
    \bibitem{soulpage_voting} SoulPage. \textit{Ensemble Voting Explained}. Available at: \url{https://soulpageit.com/ai-glossary/ensemble-voting-explained/}
    
    \bibitem{aiml_weak_vs_strong} AIML. \textit{Distinguish Between a Weak Learner vs Strong Learner}. Available at: \url{https://aiml.com/distinguish-between-a-weak-learner-vs-strong-learner/}
    
    \bibitem{gfg_voting_regressor} GeeksforGeeks. \textit{Voting Regressor}. Available at: \url{https://www.geeksforgeeks.org/voting-regressor/}
    
    \bibitem{gfg_voting_classifier} GeeksforGeeks. \textit{Voting Classifier}. Available at: \url{https://www.geeksforgeeks.org/voting-classifier/}
    
    \bibitem{alpaydin2020} Alpaydin, E. (2020). \textit{Introduction to Machine Learning}. MIT Press.
    
    \bibitem{hawkins1980} Hawkins, D. (1980). \textit{Identification of Outliers}. Chapman and Hall.
    
    \bibitem{salgado2020} Salgado, C. M., Azevedo, C., Proença, H., and Vieira, S. M. \textit{Noise Versus Outliers}. Open Learning Library (MIT).

    \bibitem{qualtrics_sampling} Qualtrics. \textit{Sampling Methods, Experience Management Research}. Available at: \url{https://www.qualtrics.com/en-au/experience-management/research/sampling-methods/}.
    
    \bibitem{byjus_sampling} BYJU's. \textit{Sampling Methods}. \textit{Maths Resource}. Available at: \url{https://byjus.com/maths/sampling-methods/}.

    \bibitem{kearns1988} Kearns, M., and Valiant, L. \textit{On the learnability of Boolean formulae}, 1988.
    
    \bibitem{schapire1990} Schapire, R. E. \textit{The strength of weak learnability}, 1990.
    
    \bibitem{freund1995} Freund, Y., and Schapire, R. E. \textit{A decision-theoretic generalization of on-line learning and an application to boosting}, 1995.

    \bibitem{freund1996experiments} Y. Freund and R. E. Schapire, \textit{Experiments with a new boosting algorithm}, Proceedings of the Thirteenth International Conference on International Conference on Machine Learning, 1996.

    \bibitem{friedman2001greedy} J. H. Friedman, \textit{Greedy function approximation: A gradient boosting machine}, Annals of Statistics, Vol. 29, No. 5, 2001.

    \bibitem{Goodfellow}
    \href{http://www.deeplearningbook.org/}{I. Goodfellow, Y. Bengio, and A. Courville, \textit{Deep Learning}. MIT Press, 2016.}

    \bibitem{Olah}
    \href{https://colah.github.io/posts/2015-08-Understanding-LSTMs/}{C. Olah, \textit{Understanding LSTMs}, 2015.}

    \bibitem{Sutskever}
    \href{https://arxiv.org/abs/1406.1078}{I. Sutskever, O. Vinyals, and Q. V. Le, \textit{Learning Phrase Representations using RNN Encoder–Decoder for Statistical Machine Translation}, \textit{Proceedings of NeurIPS}, 2014.}

    \bibitem{Nielsen}
    \href{http://neuralnetworksanddeeplearning.com/}{M. Nielsen, \textit{Neural Networks and Deep Learning}, 2015.}

    \bibitem{AbuMostafa}
    \href{http://work.caltech.edu/telecourse.html}{Y. S. Abu-Mostafa, M. Magdon-Ismail, and H.-T. Lin, \textit{Learning from Data}. Vol. 4. AMLBook, 2012.}

    \bibitem{Bishop}
    C. M. Bishop, \textit{Pattern Recognition and Machine Learning}. Springer, 2006.

    \bibitem{HechtNielsen}
    R. Hecht-Nielsen, \textit{Theory of the backpropagation neural network}, IEEE International Conference on Neural Networks, 1989.

    \bibitem{LeCun}
    Y. LeCun, Y. Bengio, and G. Hinton, \textit{Deep learning}, Nature, Vol. 521, No. 7553, 2015.

    \bibitem{Krizhevsky}
    A. Krizhevsky, I. Sutskever, and G. E. Hinton, \textit{ImageNet classification with deep convolutional neural networks}, Advances in Neural Information Processing Systems, 2012.

    \bibitem{Rumelhart}
    D. E. Rumelhart, G. E. Hinton, and R. J. Williams, \textit{Learning representations by backpropagating errors}, Nature, Vol. 323, No. 6088, 1986.

    \bibitem{Hochreiter}
    S. Hochreiter and J. Schmidhuber, “Long short-term memory,” \textit{Neural Computation}, Vol. 9, No. 8, 1997.

    \bibitem{Cho}
    K. Cho, et al., \textit{Learning Phrase Representations using RNN Encoder–Decoder for Statistical Machine Translation}, EMNLP, 2014.

    \bibitem{MacKay}
    D. J. C. MacKay, \textit{Bayesian model comparison}, Machine Learning, Vol. 35, No. 1, 1999.

    \bibitem{Kingma}
    D. P. Kingma and M. Welling, \textit{Auto-Encoding Variational Bayes}, ICLR, 2014.

    \bibitem{Vaswani}
    A. Vaswani, et al., \textit{Attention Is All You Need}, Advances in Neural Information Processing Systems, 2017.

    \bibitem{Bahdanau}
    D. Bahdanau, K. Cho, and Y. Bengio, \textit{Neural Machine Translation by Jointly Learning to Align and Translate}, arXiv preprint arXiv:1409.0473, 2014.
    
    \bibitem{Luong}
    M.-T. Luong, H. Pham, and C. D. Manning, \textit{Effective Approaches to Attention-based Neural Machine Translation}, Proceedings of EMNLP, 2015.
    
    \bibitem{Devlin}
    J. Devlin, et al., \textit{BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding}, Proceedings of NAACL-HLT, 2019.
    
    \bibitem{Radford}
    A. Radford, et al., \textit{Improving Language Understanding by Generative Pre-Training}, OpenAI, 2018.
    
    \bibitem{Brown}
    T. Brown, et al., \textit{Language Models are Few-Shot Learners}, Advances in Neural Information Processing Systems, 2020.
    
    \bibitem{Murphy}
    K. P. Murphy, \textit{Machine Learning: A Probabilistic Perspective}. MIT Press, 2012.
    
    \bibitem{HintonDistill}
    G. Hinton, O. Vinyals, and J. Dean, \textit{Distilling the Knowledge in a Neural Network}, arXiv preprint arXiv:1503.02531, 2015.
    
    \bibitem{Jang}
    E. Jang, S. Gu, and B. Poole, \textit{Categorical Reparameterization with Gumbel-Softmax}, arXiv preprint arXiv:1611.01144, 2016.
    
\end{thebibliography}