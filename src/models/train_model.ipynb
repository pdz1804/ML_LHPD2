{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier, plot_tree\n",
    "from sklearn.linear_model import Perceptron, LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
    "from sklearn.preprocessing import MaxAbsScaler\n",
    "import xgboost as xgb\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import hmmlearn.hmm\n",
    "from sklearn_crfsuite import CRF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Options\n",
    "pd.set_option(\"max_colwidth\", None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "e:\\2_LEARNING_BKU\\2_File_2\\K22_HK242\\CO3117_Machine_Learning\\Main\n"
     ]
    }
   ],
   "source": [
    "# Get the absolute path to the 'src' directory\n",
    "project_root = os.path.abspath(os.path.join(os.getcwd(), \"..\", \"..\"))\n",
    "sys.path.append(project_root)\n",
    "print(project_root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\anaconda3\\envs\\ml_env_test\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from src.features.build_features_utils import *  # Assuming build_features_utils is inside build_features.py\n",
    "from src.models.models_utils import *  # Assuming utils.py exists inside src/models/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionary for models\n",
    "MODEL_DICT = {\n",
    "    \"decision_tree\": DecisionTreeClassifier,\n",
    "    \"perceptron\": Perceptron,\n",
    "    \"bayesian\": GaussianNB,\n",
    "    \"bayesian_enhanced\": lambda: GaussianNB(var_smoothing=1e-9),\n",
    "    \"random_forest\": RandomForestClassifier,\n",
    "    \"xgboost\": xgb.XGBClassifier,\n",
    "    \"svm\": SVC,\n",
    "    \"max_edge_classifier\": MaxAbsScaler,\n",
    "    \"kernel_functions_svm\": lambda: SVC(kernel='rbf'),\n",
    "    \"soft_margin_svm\": lambda: SVC(C=1.0),\n",
    "    # \"lda\": LDA,\n",
    "    \"logistic_regression\": LogisticRegression,\n",
    "    \"hmm\": lambda: hmmlearn.hmm.GaussianHMM(n_components=3),\n",
    "    \"crf\": CRF,\n",
    "}\n",
    "\n",
    "# Dictionary for model parameters\n",
    "MODEL_PARAMS = {\n",
    "    \"decision_tree\": {\n",
    "        \"criterion\": [\"gini\"],\n",
    "        \"max_depth\": [10],\n",
    "        \"min_samples_split\": [2],\n",
    "        \"min_samples_leaf\": [1],\n",
    "        \"max_features\": [\"sqrt\"]\n",
    "    },\n",
    "    \n",
    "    # \"decision_tree\": {\n",
    "    #     \"criterion\": [\"gini\", \"entropy\"],\n",
    "    #     \"max_depth\": [10, 20, 30, 40],\n",
    "    #     \"min_samples_split\": [2, 5, 10],\n",
    "    #     \"min_samples_leaf\": [1, 2, 4],\n",
    "    #     \"max_features\": [\"auto\", \"sqrt\", \"log2\"]\n",
    "    # },\n",
    "    \n",
    "    \"perceptron\": {\n",
    "        \"max_iter\": [1000],\n",
    "        \"tol\": [1e-3],\n",
    "        \"eta0\": [0.001],\n",
    "        \"penalty\": [\"l2\"],\n",
    "        \"alpha\": [0.0001]\n",
    "    },\n",
    "    \n",
    "    # \"perceptron\": {\n",
    "    #     \"max_iter\": [1000, 2000],\n",
    "    #     \"tol\": [1e-3, 1e-4],\n",
    "    #     \"eta0\": [0.001, 0.01, 0.1],\n",
    "    #     \"penalty\": [None, \"l2\", \"l1\"],\n",
    "    #     \"alpha\": [0.0001, 0.001, 0.01]\n",
    "    # },\n",
    "    \n",
    "    \"bayesian\": {\n",
    "        \"priors\": [None, \"uniform\", \"gaussian\"],\n",
    "        \"var_smoothing\": [1e-9, 1e-8, 1e-7]\n",
    "    },\n",
    "    \n",
    "    \"bayesian_enhanced\": {\n",
    "        \"var_smoothing\": [1e-9]\n",
    "    },\n",
    "    \n",
    "    # \"bayesian_enhanced\": {\n",
    "    #     \"var_smoothing\": [1e-9, 1e-8, 1e-7]\n",
    "    # },\n",
    "    \n",
    "    \"random_forest\": {\n",
    "        \"n_estimators\": [100],\n",
    "        \"max_depth\": [10],\n",
    "        \"min_samples_split\": [2],\n",
    "        \"min_samples_leaf\": [1],\n",
    "        \"max_features\": [\"sqrt\"]\n",
    "    },\n",
    "    \n",
    "    # \"random_forest\": {\n",
    "    #     \"n_estimators\": [50, 100, 200],\n",
    "    #     \"max_depth\": [None, 10, 20, 30],\n",
    "    #     \"min_samples_split\": [2, 5, 10],\n",
    "    #     \"min_samples_leaf\": [1, 2, 4],\n",
    "    #     \"max_features\": [\"auto\", \"sqrt\", \"log2\"],\n",
    "    #     \"bootstrap\": [True, False]\n",
    "    # },\n",
    "    \n",
    "    \"xgboost\": {\n",
    "        \"n_estimators\": [100],\n",
    "        \"learning_rate\": [0.01],\n",
    "        \"max_depth\": [10]\n",
    "    },\n",
    "    \n",
    "    # \"xgboost\": {\n",
    "    #     \"n_estimators\": [100, 200, 300],\n",
    "    #     \"learning_rate\": [0.01, 0.1, 0.2],\n",
    "    #     \"max_depth\": [3, 6, 10],\n",
    "    #     \"subsample\": [0.8, 1.0],\n",
    "    #     \"colsample_bytree\": [0.8, 1.0],\n",
    "    #     \"gamma\": [0, 0.1, 0.2]\n",
    "    # },\n",
    "    \n",
    "    \"svm\": {\n",
    "        \"kernel\": [\"linear\"],\n",
    "        \"C\": [0.1]\n",
    "    },\n",
    "    \n",
    "    # \"svm\": {\n",
    "    #     \"kernel\": [\"linear\", \"rbf\", \"poly\"],\n",
    "    #     \"C\": [0.1, 1, 10, 100],\n",
    "    #     \"gamma\": [0.1, 0.01, \"scale\"],\n",
    "    #     \"degree\": [2, 3, 4]\n",
    "    # },\n",
    "    \n",
    "    \"max_edge_classifier\": {\n",
    "        \"scaler\": [\"maxabs\", \"standard\"]\n",
    "    },\n",
    "    \n",
    "    \"kernel_functions_svm\": {\n",
    "        \"kernel\": [\"rbf\", \"poly\"],\n",
    "        \"C\": [1.0, 10.0, 100.0],\n",
    "        \"gamma\": [\"scale\", \"auto\"]\n",
    "    },\n",
    "    \n",
    "    \"soft_margin_svm\": {\n",
    "        \"C\": [0.1, 1.0, 10.0]\n",
    "    },\n",
    "    \n",
    "    \"lda\": {\n",
    "        \"n_components\": [2, 3, 4, 5],\n",
    "        \"solver\": [\"svd\", \"lsqr\", \"eigen\"],\n",
    "        \"shrinkage\": [\"auto\", None]\n",
    "    },\n",
    "    \n",
    "    \"logistic_regression\": {\n",
    "        \"penalty\": [\"l2\"],\n",
    "        \"max_iter\": [1000]\n",
    "    },\n",
    "    \n",
    "    # \"logistic_regression\": {\n",
    "    #     \"penalty\": [\"l1\", \"l2\", \"elasticnet\", None],\n",
    "    #     \"C\": [0.1, 1.0, 10.0],\n",
    "    #     \"solver\": [\"liblinear\", \"lbfgs\", \"saga\"],\n",
    "    #     \"max_iter\": [1000, 2000]\n",
    "    # },\n",
    "    \n",
    "    \n",
    "    \"hmm\": {\n",
    "        \"n_components\": [2, 3, 4],\n",
    "        \"covariance_type\": [\"diag\", \"full\", \"tied\"],\n",
    "        \"n_iter\": [100, 200],\n",
    "        \"init_params\": [\"c\", \"s\", \"cs\"],\n",
    "        \"params\": [\"c\", \"t\", \"ct\"]\n",
    "    },\n",
    "    \n",
    "    \"crf\": {\n",
    "        \"algorithm\": [\"lbfgs\", \"newton-cg\", \"liblinear\"],\n",
    "        \"max_iterations\": [100, 200],\n",
    "        \"penalty\": [\"l2\", \"elasticnet\"],\n",
    "        \"dual\": [True, False],\n",
    "        \"tol\": [1e-4, 1e-3],\n",
    "    }\n",
    "}\n",
    "\n",
    "# Dictionary for dimensionality reduction methods\n",
    "DIMENSIONALITY_REDUCTION_DICT = {\n",
    "    \"pca\": PCA,\n",
    "    \"lda\": LDA,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "dataset_path = os.path.join(project_root, \"data\", \"final\", \"final_clean_no_neutral_no_duplicates.csv\")\n",
    "df = pd.read_csv(dataset_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "      <th>text</th>\n",
       "      <th>text_clean</th>\n",
       "      <th>text_length</th>\n",
       "      <th>text_clean_length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>@switchfoot http://twitpic.com/2y1zl - Awww, that's a bummer.  You shoulda got David Carr of Third Day to do it. ;D</td>\n",
       "      <td>switchfoot awww thats a bummer you shoulda got david carr of third day to do it d</td>\n",
       "      <td>19</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>is upset that he can't update his Facebook by texting it... and might cry as a result  School today also. Blah!</td>\n",
       "      <td>is upset that he cant update his facebook by texting it and might cry a a result school today also blah</td>\n",
       "      <td>21</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>@Kenichan I dived many times for the ball. Managed to save 50%  The rest go out of bounds</td>\n",
       "      <td>kenichan i dived many time for the ball managed to save the rest go out of bound</td>\n",
       "      <td>18</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>my whole body feels itchy and like its on fire</td>\n",
       "      <td>my whole body feel itchy and like it on fire</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>@nationwideclass no, it's not behaving at all. i'm mad. why am i here? because I can't see you all over there.</td>\n",
       "      <td>nationwideclass no it not behaving at all im mad why am i here because i cant see you all over there</td>\n",
       "      <td>21</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   target  \\\n",
       "0     0.0   \n",
       "1     0.0   \n",
       "2     0.0   \n",
       "3     0.0   \n",
       "4     0.0   \n",
       "\n",
       "                                                                                                                  text  \\\n",
       "0  @switchfoot http://twitpic.com/2y1zl - Awww, that's a bummer.  You shoulda got David Carr of Third Day to do it. ;D   \n",
       "1      is upset that he can't update his Facebook by texting it... and might cry as a result  School today also. Blah!   \n",
       "2                            @Kenichan I dived many times for the ball. Managed to save 50%  The rest go out of bounds   \n",
       "3                                                                      my whole body feels itchy and like its on fire    \n",
       "4      @nationwideclass no, it's not behaving at all. i'm mad. why am i here? because I can't see you all over there.    \n",
       "\n",
       "                                                                                                text_clean  \\\n",
       "0                        switchfoot awww thats a bummer you shoulda got david carr of third day to do it d   \n",
       "1  is upset that he cant update his facebook by texting it and might cry a a result school today also blah   \n",
       "2                         kenichan i dived many time for the ball managed to save the rest go out of bound   \n",
       "3                                                             my whole body feel itchy and like it on fire   \n",
       "4     nationwideclass no it not behaving at all im mad why am i here because i cant see you all over there   \n",
       "\n",
       "   text_length  text_clean_length  \n",
       "0           19                 17  \n",
       "1           21                 21  \n",
       "2           18                 17  \n",
       "3           10                 10  \n",
       "4           21                 21  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace target 4 with 1\n",
    "df[\"target\"] = df[\"target\"].replace(4, 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set size: 1600\n",
      "Test set size: 400\n",
      "Training labels size: 1600\n",
      "Test labels size: 400\n"
     ]
    }
   ],
   "source": [
    "# Sample exactly 1,000 random entries from the dataset\n",
    "df_sampled = df.sample(n=2000, random_state=42)\n",
    "\n",
    "# Split dataset into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    df_sampled[\"text_clean\"], df_sampled[\"target\"], test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Print lengths of splits\n",
    "print(f\"Training set size: {len(X_train)}\")\n",
    "print(f\"Test set size: {len(X_test)}\")\n",
    "print(f\"Training labels size: {len(y_train)}\")\n",
    "print(f\"Test labels size: {len(y_test)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of feature extraction methods\n",
    "# feature_methods = [\"tfidf\", \"count\", \"binary_count\", \"word2vec\", \"glove\", \"bert\"]\n",
    "\n",
    "feature_methods = [\"count\", \"word2vec\", \"glove\", \"bert\"]\n",
    "\n",
    "X_train_features_dict = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ”Ž Running feature extraction...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Feature Extraction Progress:   0%|          | 0/4 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ” Processing feature extraction using: count...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Feature Extraction Progress:  25%|â–ˆâ–ˆâ–Œ       | 1/4 [00:04<00:12,  4.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… count - X_train_features shape: (1600, 50)\n",
      "\n",
      "ðŸ” Processing feature extraction using: word2vec...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Word2Vec: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1600/1600 [00:00<00:00, 7001.61document/s]\n",
      "Feature Extraction Progress:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 2/4 [02:18<02:41, 80.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… word2vec - X_train_features shape: (1600, 50)\n",
      "\n",
      "ðŸ” Processing feature extraction using: glove...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing GloVe: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1600/1600 [00:00<00:00, 7966.33document/s]\n",
      "Feature Extraction Progress:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 3/4 [03:31<01:17, 77.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… glove - X_train_features shape: (1600, 50)\n",
      "\n",
      "ðŸ” Processing feature extraction using: bert...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing BERT: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1600/1600 [00:44<00:00, 35.86document/s]\n",
      "Feature Extraction Progress: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [04:25<00:00, 66.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… bert - X_train_features shape: (1600, 50)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nðŸ”Ž Running feature extraction...\\n\")\n",
    "for method in tqdm(feature_methods, desc=\"Feature Extraction Progress\"):\n",
    "    print(f\"\\nðŸ” Processing feature extraction using: {method}...\")\n",
    "\n",
    "    try:\n",
    "        # Initialize FeatureBuilder\n",
    "        feature_builder = FeatureBuilder(method=method, save_dir=os.path.join(project_root, \"data\", \"processed\"), reduce_dim=\"pca\", n_components=50)\n",
    "\n",
    "        # Fit and transform training data\n",
    "        X_train_features = feature_builder.fit_transform(X_train.tolist())\n",
    "        print(f\"âœ… {method} - X_train_features shape: {X_train_features.shape}\")\n",
    "        \n",
    "        X_train_features_dict[method] = pd.DataFrame(X_train_features)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Error with {method}: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model chosen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ”Ž Running feature extraction and model training loop...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Feature Extraction Progress:   0%|          | 0/4 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ” Processing feature extraction using: count...\n",
      "ðŸš€ Training new model: DecisionTreeClassifier...\n",
      "Best hyperparameters: {'criterion': 'gini', 'max_depth': 10, 'max_features': 'sqrt', 'min_samples_leaf': 1, 'min_samples_split': 2}\n",
      "\n",
      "ðŸŽ¯ Running K-Fold Cross-Validation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "K-Fold Progress: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:00<00:00,  9.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“Š Average Accuracy: 54%\n",
      "ðŸ“Š Average ROC AUC: 53%\n",
      "ðŸ“Š Average F1 Score: 56%\n",
      "ðŸ’¾ Model saved to best_decision_tree_count.pkl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Feature Extraction Progress:  25%|â–ˆâ–ˆâ–Œ       | 1/4 [00:02<00:06,  2.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“ˆ Plot saved to best_decision_tree_count.png\n",
      "\n",
      "ðŸ” Processing feature extraction using: word2vec...\n",
      "ðŸš€ Training new model: DecisionTreeClassifier...\n",
      "Best hyperparameters: {'criterion': 'gini', 'max_depth': 10, 'max_features': 'sqrt', 'min_samples_leaf': 1, 'min_samples_split': 2}\n",
      "\n",
      "ðŸŽ¯ Running K-Fold Cross-Validation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "K-Fold Progress: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:00<00:00, 11.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“Š Average Accuracy: 58%\n",
      "ðŸ“Š Average ROC AUC: 58%\n",
      "ðŸ“Š Average F1 Score: 60%\n",
      "ðŸ’¾ Model saved to best_decision_tree_word2vec.pkl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Feature Extraction Progress:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 2/4 [00:03<00:03,  1.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“ˆ Plot saved to best_decision_tree_word2vec.png\n",
      "\n",
      "ðŸ” Processing feature extraction using: glove...\n",
      "ðŸš€ Training new model: DecisionTreeClassifier...\n",
      "Best hyperparameters: {'criterion': 'gini', 'max_depth': 10, 'max_features': 'sqrt', 'min_samples_leaf': 1, 'min_samples_split': 2}\n",
      "\n",
      "ðŸŽ¯ Running K-Fold Cross-Validation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "K-Fold Progress: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:00<00:00, 15.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“Š Average Accuracy: 54%\n",
      "ðŸ“Š Average ROC AUC: 54%\n",
      "ðŸ“Š Average F1 Score: 56%\n",
      "ðŸ’¾ Model saved to best_decision_tree_glove.pkl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Feature Extraction Progress:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 3/4 [00:05<00:01,  1.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“ˆ Plot saved to best_decision_tree_glove.png\n",
      "\n",
      "ðŸ” Processing feature extraction using: bert...\n",
      "ðŸš€ Training new model: DecisionTreeClassifier...\n",
      "Best hyperparameters: {'criterion': 'gini', 'max_depth': 10, 'max_features': 'sqrt', 'min_samples_leaf': 1, 'min_samples_split': 2}\n",
      "\n",
      "ðŸŽ¯ Running K-Fold Cross-Validation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "K-Fold Progress: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:00<00:00, 16.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“Š Average Accuracy: 52%\n",
      "ðŸ“Š Average ROC AUC: 52%\n",
      "ðŸ“Š Average F1 Score: 54%\n",
      "ðŸ’¾ Model saved to best_decision_tree_bert.pkl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Feature Extraction Progress: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:06<00:00,  1.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“ˆ Plot saved to best_decision_tree_bert.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nðŸ”Ž Running feature extraction and model training loop...\\n\")\n",
    "for method in tqdm(feature_methods, desc=\"Feature Extraction Progress\"):\n",
    "    print(f\"\\nðŸ” Processing feature extraction using: {method}...\")\n",
    "\n",
    "    try:\n",
    "        model_name = \"decision_tree\"\n",
    "        \n",
    "        # Retrieve Decision Tree model and hyperparameters\n",
    "        decision_tree_algorithm = MODEL_DICT[model_name]()\n",
    "        decision_tree_params = MODEL_PARAMS[model_name]\n",
    "\n",
    "        # Train or load model\n",
    "        trained_model = generate_binary_classification_model(\n",
    "            X=X_train_features_dict[method], \n",
    "            y=y_train, \n",
    "            model_algorithm=decision_tree_algorithm, \n",
    "            hyperparameters=decision_tree_params, \n",
    "            needs_scaled=False, \n",
    "            model_save_path=f\"best_{model_name}_{method}.pkl\",\n",
    "            img_save_path=f\"best_{model_name}_{method}.png\"\n",
    "        )\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Error with {method}: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ”Ž Running feature extraction and model training loop...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Feature Extraction Progress:   0%|          | 0/4 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ” Processing feature extraction using: count...\n",
      "ðŸš€ Training new model: LogisticRegression...\n",
      "Best hyperparameters: {'max_iter': 1000, 'penalty': 'l2'}\n",
      "\n",
      "ðŸŽ¯ Running K-Fold Cross-Validation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "K-Fold Progress: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:00<00:00, 12.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“Š Average Accuracy: 62%\n",
      "ðŸ“Š Average ROC AUC: 62%\n",
      "ðŸ“Š Average F1 Score: 66%\n",
      "ðŸ’¾ Model saved to best_logistic_regression_count.pkl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Feature Extraction Progress:  25%|â–ˆâ–ˆâ–Œ       | 1/4 [00:02<00:08,  2.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“ˆ Plot saved to best_logistic_regression_count.png\n",
      "\n",
      "ðŸ” Processing feature extraction using: word2vec...\n",
      "ðŸš€ Training new model: LogisticRegression...\n",
      "Best hyperparameters: {'max_iter': 1000, 'penalty': 'l2'}\n",
      "\n",
      "ðŸŽ¯ Running K-Fold Cross-Validation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "K-Fold Progress: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:00<00:00, 14.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“Š Average Accuracy: 69%\n",
      "ðŸ“Š Average ROC AUC: 69%\n",
      "ðŸ“Š Average F1 Score: 72%\n",
      "ðŸ’¾ Model saved to best_logistic_regression_word2vec.pkl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Feature Extraction Progress:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 2/4 [00:04<00:04,  2.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“ˆ Plot saved to best_logistic_regression_word2vec.png\n",
      "\n",
      "ðŸ” Processing feature extraction using: glove...\n",
      "ðŸš€ Training new model: LogisticRegression...\n",
      "Best hyperparameters: {'max_iter': 1000, 'penalty': 'l2'}\n",
      "\n",
      "ðŸŽ¯ Running K-Fold Cross-Validation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "K-Fold Progress: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:00<00:00, 19.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“Š Average Accuracy: 66%\n",
      "ðŸ“Š Average ROC AUC: 66%\n",
      "ðŸ“Š Average F1 Score: 69%\n",
      "ðŸ’¾ Model saved to best_logistic_regression_glove.pkl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Feature Extraction Progress:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 3/4 [00:05<00:01,  1.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“ˆ Plot saved to best_logistic_regression_glove.png\n",
      "\n",
      "ðŸ” Processing feature extraction using: bert...\n",
      "ðŸš€ Training new model: LogisticRegression...\n",
      "Best hyperparameters: {'max_iter': 1000, 'penalty': 'l2'}\n",
      "\n",
      "ðŸŽ¯ Running K-Fold Cross-Validation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "K-Fold Progress: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:00<00:00, 23.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“Š Average Accuracy: 60%\n",
      "ðŸ“Š Average ROC AUC: 59%\n",
      "ðŸ“Š Average F1 Score: 64%\n",
      "ðŸ’¾ Model saved to best_logistic_regression_bert.pkl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Feature Extraction Progress: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:06<00:00,  1.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“ˆ Plot saved to best_logistic_regression_bert.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nðŸ”Ž Running feature extraction and model training loop...\\n\")\n",
    "for method in tqdm(feature_methods, desc=\"Feature Extraction Progress\"):\n",
    "    print(f\"\\nðŸ” Processing feature extraction using: {method}...\")\n",
    "\n",
    "    try:\n",
    "        model_name_lg = \"logistic_regression\"\n",
    "        \n",
    "        # Retrieve Log Reg model and hyperparameters\n",
    "        logreg_algorithm = MODEL_DICT[model_name_lg]()\n",
    "        logreg_params = MODEL_PARAMS[model_name_lg]\n",
    "\n",
    "        # Train or load model\n",
    "        trained_model = generate_binary_classification_model(\n",
    "            X=X_train_features_dict[method], \n",
    "            y=y_train, \n",
    "            model_algorithm=logreg_algorithm, \n",
    "            hyperparameters=logreg_params, \n",
    "            needs_scaled=False, \n",
    "            model_save_path=f\"best_{model_name_lg}_{method}.pkl\",\n",
    "            img_save_path=f\"best_{model_name_lg}_{method}.png\"\n",
    "        )\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Error with {method}: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ”Ž Running feature extraction and model training loop...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Feature Extraction Progress:   0%|          | 0/4 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ” Processing feature extraction using: count...\n",
      "ðŸš€ Training new model: RandomForestClassifier...\n",
      "Best hyperparameters: {'max_depth': 10, 'max_features': 'sqrt', 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 100}\n",
      "\n",
      "ðŸŽ¯ Running K-Fold Cross-Validation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "K-Fold Progress: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:07<00:00,  1.55s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“Š Average Accuracy: 61%\n",
      "ðŸ“Š Average ROC AUC: 60%\n",
      "ðŸ“Š Average F1 Score: 64%\n",
      "ðŸ’¾ Model saved to best_random_forest_count.pkl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Feature Extraction Progress:  25%|â–ˆâ–ˆâ–Œ       | 1/4 [00:18<00:55, 18.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“ˆ Plot saved to best_random_forest_count.png\n",
      "\n",
      "ðŸ” Processing feature extraction using: word2vec...\n",
      "ðŸš€ Training new model: RandomForestClassifier...\n",
      "Best hyperparameters: {'max_depth': 10, 'max_features': 'sqrt', 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 100}\n",
      "\n",
      "ðŸŽ¯ Running K-Fold Cross-Validation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "K-Fold Progress: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:06<00:00,  1.22s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“Š Average Accuracy: 67%\n",
      "ðŸ“Š Average ROC AUC: 67%\n",
      "ðŸ“Š Average F1 Score: 69%\n",
      "ðŸ’¾ Model saved to best_random_forest_word2vec.pkl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Feature Extraction Progress:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 2/4 [00:34<00:34, 17.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“ˆ Plot saved to best_random_forest_word2vec.png\n",
      "\n",
      "ðŸ” Processing feature extraction using: glove...\n",
      "ðŸš€ Training new model: RandomForestClassifier...\n",
      "Best hyperparameters: {'max_depth': 10, 'max_features': 'sqrt', 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 100}\n",
      "\n",
      "ðŸŽ¯ Running K-Fold Cross-Validation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "K-Fold Progress: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:06<00:00,  1.24s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“Š Average Accuracy: 64%\n",
      "ðŸ“Š Average ROC AUC: 64%\n",
      "ðŸ“Š Average F1 Score: 67%\n",
      "ðŸ’¾ Model saved to best_random_forest_glove.pkl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Feature Extraction Progress:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 3/4 [00:49<00:16, 16.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“ˆ Plot saved to best_random_forest_glove.png\n",
      "\n",
      "ðŸ” Processing feature extraction using: bert...\n",
      "ðŸš€ Training new model: RandomForestClassifier...\n",
      "Best hyperparameters: {'max_depth': 10, 'max_features': 'sqrt', 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 100}\n",
      "\n",
      "ðŸŽ¯ Running K-Fold Cross-Validation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "K-Fold Progress: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:06<00:00,  1.33s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“Š Average Accuracy: 58%\n",
      "ðŸ“Š Average ROC AUC: 58%\n",
      "ðŸ“Š Average F1 Score: 64%\n",
      "ðŸ’¾ Model saved to best_random_forest_bert.pkl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Feature Extraction Progress: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [01:05<00:00, 16.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“ˆ Plot saved to best_random_forest_bert.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nðŸ”Ž Running feature extraction and model training loop...\\n\")\n",
    "for method in tqdm(feature_methods, desc=\"Feature Extraction Progress\"):\n",
    "    print(f\"\\nðŸ” Processing feature extraction using: {method}...\")\n",
    "\n",
    "    try:\n",
    "        model_name_rf = \"random_forest\"\n",
    "        \n",
    "        # Retrieve Log Reg model and hyperparameters\n",
    "        rf_algorithm = MODEL_DICT[model_name_rf]()\n",
    "        rf_params = MODEL_PARAMS[model_name_rf]\n",
    "\n",
    "        # Train or load model\n",
    "        trained_model = generate_binary_classification_model(\n",
    "            X=X_train_features_dict[method], \n",
    "            y=y_train, \n",
    "            model_algorithm=rf_algorithm, \n",
    "            hyperparameters=rf_params, \n",
    "            needs_scaled=False, \n",
    "            model_save_path=f\"best_{model_name_rf}_{method}.pkl\",\n",
    "            img_save_path=f\"best_{model_name_rf}_{method}.png\"\n",
    "        )\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Error with {method}: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ”Ž Running feature extraction and model training loop...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Feature Extraction Progress:   0%|          | 0/4 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ” Processing feature extraction using: count...\n",
      "ðŸš€ Training new model: XGBClassifier...\n",
      "Best hyperparameters: {'learning_rate': 0.01, 'max_depth': 10, 'n_estimators': 100}\n",
      "\n",
      "ðŸŽ¯ Running K-Fold Cross-Validation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "K-Fold Progress: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:15<00:00,  3.15s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“Š Average Accuracy: 58%\n",
      "ðŸ“Š Average ROC AUC: 57%\n",
      "ðŸ“Š Average F1 Score: 61%\n",
      "ðŸ’¾ Model saved to best_xgboost_count.pkl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Feature Extraction Progress:  25%|â–ˆâ–ˆâ–Œ       | 1/4 [00:39<01:58, 39.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“ˆ Plot saved to best_xgboost_count.png\n",
      "\n",
      "ðŸ” Processing feature extraction using: word2vec...\n",
      "ðŸš€ Training new model: XGBClassifier...\n",
      "Best hyperparameters: {'learning_rate': 0.01, 'max_depth': 10, 'n_estimators': 100}\n",
      "\n",
      "ðŸŽ¯ Running K-Fold Cross-Validation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "K-Fold Progress: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:21<00:00,  4.35s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“Š Average Accuracy: 64%\n",
      "ðŸ“Š Average ROC AUC: 63%\n",
      "ðŸ“Š Average F1 Score: 66%\n",
      "ðŸ’¾ Model saved to best_xgboost_word2vec.pkl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Feature Extraction Progress:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 2/4 [01:25<01:26, 43.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“ˆ Plot saved to best_xgboost_word2vec.png\n",
      "\n",
      "ðŸ” Processing feature extraction using: glove...\n",
      "ðŸš€ Training new model: XGBClassifier...\n",
      "Best hyperparameters: {'learning_rate': 0.01, 'max_depth': 10, 'n_estimators': 100}\n",
      "\n",
      "ðŸŽ¯ Running K-Fold Cross-Validation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "K-Fold Progress: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:20<00:00,  4.11s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“Š Average Accuracy: 61%\n",
      "ðŸ“Š Average ROC AUC: 61%\n",
      "ðŸ“Š Average F1 Score: 65%\n",
      "ðŸ’¾ Model saved to best_xgboost_glove.pkl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Feature Extraction Progress:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 3/4 [02:14<00:46, 46.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“ˆ Plot saved to best_xgboost_glove.png\n",
      "\n",
      "ðŸ” Processing feature extraction using: bert...\n",
      "ðŸš€ Training new model: XGBClassifier...\n",
      "Best hyperparameters: {'learning_rate': 0.01, 'max_depth': 10, 'n_estimators': 100}\n",
      "\n",
      "ðŸŽ¯ Running K-Fold Cross-Validation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "K-Fold Progress: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:18<00:00,  3.77s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“Š Average Accuracy: 57%\n",
      "ðŸ“Š Average ROC AUC: 56%\n",
      "ðŸ“Š Average F1 Score: 62%\n",
      "ðŸ’¾ Model saved to best_xgboost_bert.pkl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Feature Extraction Progress: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [02:58<00:00, 44.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“ˆ Plot saved to best_xgboost_bert.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nðŸ”Ž Running feature extraction and model training loop...\\n\")\n",
    "for method in tqdm(feature_methods, desc=\"Feature Extraction Progress\"):\n",
    "    print(f\"\\nðŸ” Processing feature extraction using: {method}...\")\n",
    "\n",
    "    try:\n",
    "        model_name_xgb = \"xgboost\"\n",
    "        \n",
    "        # Retrieve Log Reg model and hyperparameters\n",
    "        xgb_algorithm = MODEL_DICT[model_name_xgb]()\n",
    "        xgb_params = MODEL_PARAMS[model_name_xgb]\n",
    "\n",
    "        # Train or load model\n",
    "        trained_model = generate_binary_classification_model(\n",
    "            X=X_train_features_dict[method], \n",
    "            y=y_train, \n",
    "            model_algorithm=xgb_algorithm, \n",
    "            hyperparameters=xgb_params, \n",
    "            needs_scaled=False, \n",
    "            model_save_path=f\"best_{model_name_xgb}_{method}.pkl\",\n",
    "            img_save_path=f\"best_{model_name_xgb}_{method}.png\"\n",
    "        )\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Error with {method}: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ”Ž Running feature extraction and model training loop...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Feature Extraction Progress:   0%|          | 0/4 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ” Processing feature extraction using: count...\n",
      "ðŸš€ Training new model: Perceptron...\n",
      "Best hyperparameters: {'alpha': 0.0001, 'eta0': 0.001, 'max_iter': 1000, 'penalty': 'l2', 'tol': 0.001}\n",
      "\n",
      "ðŸŽ¯ Running K-Fold Cross-Validation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "K-Fold Progress: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:00<00:00, 21.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“Š Average Accuracy: 58%\n",
      "ðŸ“Š Average ROC AUC: 57%\n",
      "ðŸ“Š Average F1 Score: 63%\n",
      "ðŸ’¾ Model saved to best_perceptron_count.pkl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Feature Extraction Progress:  25%|â–ˆâ–ˆâ–Œ       | 1/4 [00:01<00:03,  1.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“ˆ Plot saved to best_perceptron_count.png\n",
      "\n",
      "ðŸ” Processing feature extraction using: word2vec...\n",
      "ðŸš€ Training new model: Perceptron...\n",
      "Best hyperparameters: {'alpha': 0.0001, 'eta0': 0.001, 'max_iter': 1000, 'penalty': 'l2', 'tol': 0.001}\n",
      "\n",
      "ðŸŽ¯ Running K-Fold Cross-Validation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "K-Fold Progress: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:00<00:00, 17.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“Š Average Accuracy: 59%\n",
      "ðŸ“Š Average ROC AUC: 60%\n",
      "ðŸ“Š Average F1 Score: 45%\n",
      "ðŸ’¾ Model saved to best_perceptron_word2vec.pkl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Feature Extraction Progress:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 2/4 [00:02<00:02,  1.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“ˆ Plot saved to best_perceptron_word2vec.png\n",
      "\n",
      "ðŸ” Processing feature extraction using: glove...\n",
      "ðŸš€ Training new model: Perceptron...\n",
      "Best hyperparameters: {'alpha': 0.0001, 'eta0': 0.001, 'max_iter': 1000, 'penalty': 'l2', 'tol': 0.001}\n",
      "\n",
      "ðŸŽ¯ Running K-Fold Cross-Validation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "K-Fold Progress: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:00<00:00, 17.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“Š Average Accuracy: 58%\n",
      "ðŸ“Š Average ROC AUC: 59%\n",
      "ðŸ“Š Average F1 Score: 53%\n",
      "ðŸ’¾ Model saved to best_perceptron_glove.pkl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Feature Extraction Progress:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 3/4 [00:04<00:01,  1.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“ˆ Plot saved to best_perceptron_glove.png\n",
      "\n",
      "ðŸ” Processing feature extraction using: bert...\n",
      "ðŸš€ Training new model: Perceptron...\n",
      "Best hyperparameters: {'alpha': 0.0001, 'eta0': 0.001, 'max_iter': 1000, 'penalty': 'l2', 'tol': 0.001}\n",
      "\n",
      "ðŸŽ¯ Running K-Fold Cross-Validation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "K-Fold Progress: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:00<00:00, 15.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“Š Average Accuracy: 55%\n",
      "ðŸ“Š Average ROC AUC: 55%\n",
      "ðŸ“Š Average F1 Score: 52%\n",
      "ðŸ’¾ Model saved to best_perceptron_bert.pkl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Feature Extraction Progress: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:05<00:00,  1.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“ˆ Plot saved to best_perceptron_bert.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nðŸ”Ž Running feature extraction and model training loop...\\n\")\n",
    "for method in tqdm(feature_methods, desc=\"Feature Extraction Progress\"):\n",
    "    print(f\"\\nðŸ” Processing feature extraction using: {method}...\")\n",
    "\n",
    "    try:\n",
    "        model_name_perceptron = \"perceptron\"\n",
    "        \n",
    "        # Retrieve Log Reg model and hyperparameters\n",
    "        per_algorithm = MODEL_DICT[model_name_perceptron]()\n",
    "        per_params = MODEL_PARAMS[model_name_perceptron]\n",
    "\n",
    "        # Train or load model\n",
    "        trained_model = generate_binary_classification_model(\n",
    "            X=X_train_features_dict[method], \n",
    "            y=y_train, \n",
    "            model_algorithm=per_algorithm, \n",
    "            hyperparameters=per_params, \n",
    "            needs_scaled=False, \n",
    "            model_save_path=f\"best_{model_name_perceptron}_{method}.pkl\",\n",
    "            img_save_path=f\"best_{model_name_perceptron}_{method}.png\"\n",
    "        )\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Error with {method}: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bayesian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ”Ž Running feature extraction and model training loop...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Feature Extraction Progress:   0%|          | 0/4 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ” Processing feature extraction using: count...\n",
      "ðŸš€ Training new model: GaussianNB...\n",
      "Best hyperparameters: {'var_smoothing': 1e-09}\n",
      "\n",
      "ðŸŽ¯ Running K-Fold Cross-Validation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "K-Fold Progress: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:00<00:00, 21.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“Š Average Accuracy: 57%\n",
      "ðŸ“Š Average ROC AUC: 57%\n",
      "ðŸ“Š Average F1 Score: 63%\n",
      "ðŸ’¾ Model saved to best_bayesian_enhanced_count.pkl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Feature Extraction Progress:  25%|â–ˆâ–ˆâ–Œ       | 1/4 [00:01<00:03,  1.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“ˆ Plot saved to best_bayesian_enhanced_count.png\n",
      "\n",
      "ðŸ” Processing feature extraction using: word2vec...\n",
      "ðŸš€ Training new model: GaussianNB...\n",
      "Best hyperparameters: {'var_smoothing': 1e-09}\n",
      "\n",
      "ðŸŽ¯ Running K-Fold Cross-Validation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "K-Fold Progress: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:00<00:00, 23.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“Š Average Accuracy: 57%\n",
      "ðŸ“Š Average ROC AUC: 58%\n",
      "ðŸ“Š Average F1 Score: 50%\n",
      "ðŸ’¾ Model saved to best_bayesian_enhanced_word2vec.pkl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Feature Extraction Progress:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 2/4 [00:02<00:02,  1.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“ˆ Plot saved to best_bayesian_enhanced_word2vec.png\n",
      "\n",
      "ðŸ” Processing feature extraction using: glove...\n",
      "ðŸš€ Training new model: GaussianNB...\n",
      "Best hyperparameters: {'var_smoothing': 1e-09}\n",
      "\n",
      "ðŸŽ¯ Running K-Fold Cross-Validation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "K-Fold Progress: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:00<00:00, 17.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“Š Average Accuracy: 57%\n",
      "ðŸ“Š Average ROC AUC: 58%\n",
      "ðŸ“Š Average F1 Score: 50%\n",
      "ðŸ’¾ Model saved to best_bayesian_enhanced_glove.pkl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Feature Extraction Progress:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 3/4 [00:03<00:01,  1.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“ˆ Plot saved to best_bayesian_enhanced_glove.png\n",
      "\n",
      "ðŸ” Processing feature extraction using: bert...\n",
      "ðŸš€ Training new model: GaussianNB...\n",
      "Best hyperparameters: {'var_smoothing': 1e-09}\n",
      "\n",
      "ðŸŽ¯ Running K-Fold Cross-Validation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "K-Fold Progress: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:00<00:00, 23.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“Š Average Accuracy: 59%\n",
      "ðŸ“Š Average ROC AUC: 59%\n",
      "ðŸ“Š Average F1 Score: 60%\n",
      "ðŸ’¾ Model saved to best_bayesian_enhanced_bert.pkl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Feature Extraction Progress: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:04<00:00,  1.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“ˆ Plot saved to best_bayesian_enhanced_bert.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nðŸ”Ž Running feature extraction and model training loop...\\n\")\n",
    "for method in tqdm(feature_methods, desc=\"Feature Extraction Progress\"):\n",
    "    print(f\"\\nðŸ” Processing feature extraction using: {method}...\")\n",
    "\n",
    "    try:\n",
    "        model_name_bayes = \"bayesian_enhanced\"\n",
    "        \n",
    "        # Retrieve Log Reg model and hyperparameters\n",
    "        bayes_algorithm = MODEL_DICT[model_name_bayes]()\n",
    "        bayes_params = MODEL_PARAMS[model_name_bayes]\n",
    "\n",
    "        # Train or load model\n",
    "        trained_model = generate_binary_classification_model(\n",
    "            X=X_train_features_dict[method], \n",
    "            y=y_train, \n",
    "            model_algorithm=bayes_algorithm, \n",
    "            hyperparameters=bayes_params, \n",
    "            needs_scaled=False, \n",
    "            model_save_path=f\"best_{model_name_bayes}_{method}.pkl\",\n",
    "            img_save_path=f\"best_{model_name_bayes}_{method}.png\"\n",
    "        )\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Error with {method}: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ”Ž Running feature extraction and model training loop...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Feature Extraction Progress:   0%|          | 0/4 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ” Processing feature extraction using: count...\n",
      "ðŸš€ Training new model: SVC...\n",
      "Best hyperparameters: {'C': 0.1, 'kernel': 'linear'}\n",
      "\n",
      "ðŸŽ¯ Running K-Fold Cross-Validation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "K-Fold Progress: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:00<00:00,  5.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“Š Average Accuracy: 61%\n",
      "ðŸ“Š Average ROC AUC: 60%\n",
      "ðŸ“Š Average F1 Score: 67%\n",
      "ðŸ’¾ Model saved to best_svm_count.pkl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Feature Extraction Progress:  25%|â–ˆâ–ˆâ–Œ       | 1/4 [00:02<00:07,  2.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“ˆ Plot saved to best_svm_count.png\n",
      "\n",
      "ðŸ” Processing feature extraction using: word2vec...\n",
      "ðŸš€ Training new model: SVC...\n",
      "Best hyperparameters: {'C': 0.1, 'kernel': 'linear'}\n",
      "\n",
      "ðŸŽ¯ Running K-Fold Cross-Validation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "K-Fold Progress: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:00<00:00,  6.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“Š Average Accuracy: 65%\n",
      "ðŸ“Š Average ROC AUC: 64%\n",
      "ðŸ“Š Average F1 Score: 72%\n",
      "ðŸ’¾ Model saved to best_svm_word2vec.pkl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Feature Extraction Progress:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 2/4 [00:05<00:05,  2.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“ˆ Plot saved to best_svm_word2vec.png\n",
      "\n",
      "ðŸ” Processing feature extraction using: glove...\n",
      "ðŸš€ Training new model: SVC...\n",
      "Best hyperparameters: {'C': 0.1, 'kernel': 'linear'}\n",
      "\n",
      "ðŸŽ¯ Running K-Fold Cross-Validation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "K-Fold Progress: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:00<00:00,  8.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“Š Average Accuracy: 66%\n",
      "ðŸ“Š Average ROC AUC: 66%\n",
      "ðŸ“Š Average F1 Score: 69%\n",
      "ðŸ’¾ Model saved to best_svm_glove.pkl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Feature Extraction Progress:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 3/4 [00:07<00:02,  2.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“ˆ Plot saved to best_svm_glove.png\n",
      "\n",
      "ðŸ” Processing feature extraction using: bert...\n",
      "ðŸš€ Training new model: SVC...\n",
      "Best hyperparameters: {'C': 0.1, 'kernel': 'linear'}\n",
      "\n",
      "ðŸŽ¯ Running K-Fold Cross-Validation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "K-Fold Progress: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:02<00:00,  2.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“Š Average Accuracy: 54%\n",
      "ðŸ“Š Average ROC AUC: 52%\n",
      "ðŸ“Š Average F1 Score: 69%\n",
      "ðŸ’¾ Model saved to best_svm_bert.pkl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Feature Extraction Progress: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:11<00:00,  2.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“ˆ Plot saved to best_svm_bert.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nðŸ”Ž Running feature extraction and model training loop...\\n\")\n",
    "for method in tqdm(feature_methods, desc=\"Feature Extraction Progress\"):\n",
    "    print(f\"\\nðŸ” Processing feature extraction using: {method}...\")\n",
    "\n",
    "    try:\n",
    "        model_name_svm = \"svm\"\n",
    "        \n",
    "        # Retrieve Log Reg model and hyperparameters\n",
    "        svm_algorithm = MODEL_DICT[model_name_svm]()\n",
    "        svm_params = MODEL_PARAMS[model_name_svm]\n",
    "\n",
    "        # Train or load model\n",
    "        trained_model = generate_binary_classification_model(\n",
    "            X=X_train_features_dict[method], \n",
    "            y=y_train, \n",
    "            model_algorithm=svm_algorithm, \n",
    "            hyperparameters=svm_params, \n",
    "            needs_scaled=False, \n",
    "            model_save_path=f\"best_{model_name_svm}_{method}.pkl\",\n",
    "            img_save_path=f\"best_{model_name_svm}_{method}.png\"\n",
    "        )\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Error with {method}: {e}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_env_test",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
