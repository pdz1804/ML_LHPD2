{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorflow in e:\\anaconda3\\envs\\ml_env_test\\lib\\site-packages (2.18.0)Note: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "Requirement already satisfied: keras in e:\\anaconda3\\envs\\ml_env_test\\lib\\site-packages (3.8.0)\n",
      "Requirement already satisfied: tensorflow-intel==2.18.0 in e:\\anaconda3\\envs\\ml_env_test\\lib\\site-packages (from tensorflow) (2.18.0)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in e:\\anaconda3\\envs\\ml_env_test\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (2.1.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in e:\\anaconda3\\envs\\ml_env_test\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=24.3.25 in e:\\anaconda3\\envs\\ml_env_test\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (25.2.10)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in e:\\anaconda3\\envs\\ml_env_test\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (0.6.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in e:\\anaconda3\\envs\\ml_env_test\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (0.2.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in e:\\anaconda3\\envs\\ml_env_test\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (18.1.1)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in e:\\anaconda3\\envs\\ml_env_test\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (3.4.0)\n",
      "Requirement already satisfied: packaging in c:\\users\\user\\appdata\\roaming\\python\\python312\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (24.2)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in e:\\anaconda3\\envs\\ml_env_test\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (5.29.3)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in e:\\anaconda3\\envs\\ml_env_test\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (2.32.3)\n",
      "Requirement already satisfied: setuptools in e:\\anaconda3\\envs\\ml_env_test\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (75.1.0)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\users\\user\\appdata\\roaming\\python\\python312\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (1.16.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in e:\\anaconda3\\envs\\ml_env_test\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (2.5.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in e:\\anaconda3\\envs\\ml_env_test\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (4.12.2)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in e:\\anaconda3\\envs\\ml_env_test\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (1.17.2)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in e:\\anaconda3\\envs\\ml_env_test\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (1.70.0)\n",
      "Requirement already satisfied: tensorboard<2.19,>=2.18 in e:\\anaconda3\\envs\\ml_env_test\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (2.18.0)\n",
      "Requirement already satisfied: numpy<2.1.0,>=1.26.0 in e:\\anaconda3\\envs\\ml_env_test\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (1.26.4)\n",
      "Requirement already satisfied: h5py>=3.11.0 in e:\\anaconda3\\envs\\ml_env_test\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (3.13.0)\n",
      "Requirement already satisfied: ml-dtypes<0.5.0,>=0.4.0 in e:\\anaconda3\\envs\\ml_env_test\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (0.4.1)\n",
      "Requirement already satisfied: rich in e:\\anaconda3\\envs\\ml_env_test\\lib\\site-packages (from keras) (13.9.4)\n",
      "Requirement already satisfied: namex in e:\\anaconda3\\envs\\ml_env_test\\lib\\site-packages (from keras) (0.0.8)\n",
      "Requirement already satisfied: optree in e:\\anaconda3\\envs\\ml_env_test\\lib\\site-packages (from keras) (0.14.0)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in e:\\anaconda3\\envs\\ml_env_test\\lib\\site-packages (from rich->keras) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\user\\appdata\\roaming\\python\\python312\\site-packages (from rich->keras) (2.18.0)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in e:\\anaconda3\\envs\\ml_env_test\\lib\\site-packages (from astunparse>=1.6.0->tensorflow-intel==2.18.0->tensorflow) (0.44.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in e:\\anaconda3\\envs\\ml_env_test\\lib\\site-packages (from markdown-it-py>=2.2.0->rich->keras) (0.1.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in e:\\anaconda3\\envs\\ml_env_test\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.18.0->tensorflow) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in e:\\anaconda3\\envs\\ml_env_test\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.18.0->tensorflow) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in e:\\anaconda3\\envs\\ml_env_test\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.18.0->tensorflow) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in e:\\anaconda3\\envs\\ml_env_test\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.18.0->tensorflow) (2024.12.14)\n",
      "Requirement already satisfied: markdown>=2.6.8 in e:\\anaconda3\\envs\\ml_env_test\\lib\\site-packages (from tensorboard<2.19,>=2.18->tensorflow-intel==2.18.0->tensorflow) (3.7)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in e:\\anaconda3\\envs\\ml_env_test\\lib\\site-packages (from tensorboard<2.19,>=2.18->tensorflow-intel==2.18.0->tensorflow) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in e:\\anaconda3\\envs\\ml_env_test\\lib\\site-packages (from tensorboard<2.19,>=2.18->tensorflow-intel==2.18.0->tensorflow) (3.1.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in e:\\anaconda3\\envs\\ml_env_test\\lib\\site-packages (from werkzeug>=1.0.1->tensorboard<2.19,>=2.18->tensorflow-intel==2.18.0->tensorflow) (2.1.3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution ~cipy (e:\\anaconda3\\envs\\ml_env_test\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~cipy (e:\\anaconda3\\envs\\ml_env_test\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~cipy (e:\\anaconda3\\envs\\ml_env_test\\Lib\\site-packages)\n"
     ]
    }
   ],
   "source": [
    "%pip install tensorflow keras\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: keras-tuner in e:\\anaconda3\\envs\\ml_env_test\\lib\\site-packages (1.4.7)Note: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "Requirement already satisfied: keras in e:\\anaconda3\\envs\\ml_env_test\\lib\\site-packages (from keras-tuner) (3.8.0)\n",
      "Requirement already satisfied: packaging in c:\\users\\user\\appdata\\roaming\\python\\python312\\site-packages (from keras-tuner) (24.2)\n",
      "Requirement already satisfied: requests in e:\\anaconda3\\envs\\ml_env_test\\lib\\site-packages (from keras-tuner) (2.32.3)\n",
      "Requirement already satisfied: kt-legacy in e:\\anaconda3\\envs\\ml_env_test\\lib\\site-packages (from keras-tuner) (1.0.5)\n",
      "Requirement already satisfied: absl-py in e:\\anaconda3\\envs\\ml_env_test\\lib\\site-packages (from keras->keras-tuner) (2.1.0)\n",
      "Requirement already satisfied: numpy in e:\\anaconda3\\envs\\ml_env_test\\lib\\site-packages (from keras->keras-tuner) (1.26.4)\n",
      "Requirement already satisfied: rich in e:\\anaconda3\\envs\\ml_env_test\\lib\\site-packages (from keras->keras-tuner) (13.9.4)\n",
      "Requirement already satisfied: namex in e:\\anaconda3\\envs\\ml_env_test\\lib\\site-packages (from keras->keras-tuner) (0.0.8)\n",
      "Requirement already satisfied: h5py in e:\\anaconda3\\envs\\ml_env_test\\lib\\site-packages (from keras->keras-tuner) (3.13.0)\n",
      "Requirement already satisfied: optree in e:\\anaconda3\\envs\\ml_env_test\\lib\\site-packages (from keras->keras-tuner) (0.14.0)\n",
      "Requirement already satisfied: ml-dtypes in e:\\anaconda3\\envs\\ml_env_test\\lib\\site-packages (from keras->keras-tuner) (0.4.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in e:\\anaconda3\\envs\\ml_env_test\\lib\\site-packages (from requests->keras-tuner) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in e:\\anaconda3\\envs\\ml_env_test\\lib\\site-packages (from requests->keras-tuner) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in e:\\anaconda3\\envs\\ml_env_test\\lib\\site-packages (from requests->keras-tuner) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in e:\\anaconda3\\envs\\ml_env_test\\lib\\site-packages (from requests->keras-tuner) (2024.12.14)\n",
      "Requirement already satisfied: typing-extensions>=4.5.0 in e:\\anaconda3\\envs\\ml_env_test\\lib\\site-packages (from optree->keras->keras-tuner) (4.12.2)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in e:\\anaconda3\\envs\\ml_env_test\\lib\\site-packages (from rich->keras->keras-tuner) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\user\\appdata\\roaming\\python\\python312\\site-packages (from rich->keras->keras-tuner) (2.18.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in e:\\anaconda3\\envs\\ml_env_test\\lib\\site-packages (from markdown-it-py>=2.2.0->rich->keras->keras-tuner) (0.1.2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution ~cipy (e:\\anaconda3\\envs\\ml_env_test\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~cipy (e:\\anaconda3\\envs\\ml_env_test\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~cipy (e:\\anaconda3\\envs\\ml_env_test\\Lib\\site-packages)\n"
     ]
    }
   ],
   "source": [
    "%pip install keras-tuner\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: hmmlearn in e:\\anaconda3\\envs\\ml_env_test\\lib\\site-packages (0.3.3)\n",
      "Requirement already satisfied: numpy>=1.10 in e:\\anaconda3\\envs\\ml_env_test\\lib\\site-packages (from hmmlearn) (1.26.4)\n",
      "Requirement already satisfied: scikit-learn!=0.22.0,>=0.16 in e:\\anaconda3\\envs\\ml_env_test\\lib\\site-packages (from hmmlearn) (1.5.2)\n",
      "Requirement already satisfied: scipy>=0.19 in e:\\anaconda3\\envs\\ml_env_test\\lib\\site-packages (from hmmlearn) (1.13.1)\n",
      "Requirement already satisfied: joblib>=1.2.0 in e:\\anaconda3\\envs\\ml_env_test\\lib\\site-packages (from scikit-learn!=0.22.0,>=0.16->hmmlearn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in e:\\anaconda3\\envs\\ml_env_test\\lib\\site-packages (from scikit-learn!=0.22.0,>=0.16->hmmlearn) (3.5.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution ~cipy (e:\\anaconda3\\envs\\ml_env_test\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~cipy (e:\\anaconda3\\envs\\ml_env_test\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~cipy (e:\\anaconda3\\envs\\ml_env_test\\Lib\\site-packages)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pgmpy in e:\\anaconda3\\envs\\ml_env_test\\lib\\site-packages (0.1.26)\n",
      "Requirement already satisfied: networkx in e:\\anaconda3\\envs\\ml_env_test\\lib\\site-packages (from pgmpy) (3.4.2)\n",
      "Requirement already satisfied: numpy in e:\\anaconda3\\envs\\ml_env_test\\lib\\site-packages (from pgmpy) (1.26.4)\n",
      "Requirement already satisfied: scipy in e:\\anaconda3\\envs\\ml_env_test\\lib\\site-packages (from pgmpy) (1.13.1)\n",
      "Requirement already satisfied: scikit-learn in e:\\anaconda3\\envs\\ml_env_test\\lib\\site-packages (from pgmpy) (1.5.2)\n",
      "Requirement already satisfied: pandas in e:\\anaconda3\\envs\\ml_env_test\\lib\\site-packages (from pgmpy) (2.2.3)\n",
      "Requirement already satisfied: pyparsing in e:\\anaconda3\\envs\\ml_env_test\\lib\\site-packages (from pgmpy) (3.2.0)\n",
      "Requirement already satisfied: torch in e:\\anaconda3\\envs\\ml_env_test\\lib\\site-packages (from pgmpy) (2.5.1)\n",
      "Requirement already satisfied: statsmodels in e:\\anaconda3\\envs\\ml_env_test\\lib\\site-packages (from pgmpy) (0.14.4)\n",
      "Requirement already satisfied: tqdm in e:\\anaconda3\\envs\\ml_env_test\\lib\\site-packages (from pgmpy) (4.66.5)\n",
      "Requirement already satisfied: joblib in e:\\anaconda3\\envs\\ml_env_test\\lib\\site-packages (from pgmpy) (1.4.2)\n",
      "Requirement already satisfied: opt-einsum in e:\\anaconda3\\envs\\ml_env_test\\lib\\site-packages (from pgmpy) (3.4.0)\n",
      "Requirement already satisfied: xgboost in e:\\anaconda3\\envs\\ml_env_test\\lib\\site-packages (from pgmpy) (2.1.1)\n",
      "Requirement already satisfied: google-generativeai in e:\\anaconda3\\envs\\ml_env_test\\lib\\site-packages (from pgmpy) (0.8.4)\n",
      "Requirement already satisfied: google-ai-generativelanguage==0.6.15 in e:\\anaconda3\\envs\\ml_env_test\\lib\\site-packages (from google-generativeai->pgmpy) (0.6.15)\n",
      "Requirement already satisfied: google-api-core in e:\\anaconda3\\envs\\ml_env_test\\lib\\site-packages (from google-generativeai->pgmpy) (2.24.1)\n",
      "Requirement already satisfied: google-api-python-client in e:\\anaconda3\\envs\\ml_env_test\\lib\\site-packages (from google-generativeai->pgmpy) (2.161.0)\n",
      "Requirement already satisfied: google-auth>=2.15.0 in e:\\anaconda3\\envs\\ml_env_test\\lib\\site-packages (from google-generativeai->pgmpy) (2.38.0)\n",
      "Requirement already satisfied: protobuf in e:\\anaconda3\\envs\\ml_env_test\\lib\\site-packages (from google-generativeai->pgmpy) (5.29.3)\n",
      "Requirement already satisfied: pydantic in e:\\anaconda3\\envs\\ml_env_test\\lib\\site-packages (from google-generativeai->pgmpy) (2.10.6)\n",
      "Requirement already satisfied: typing-extensions in e:\\anaconda3\\envs\\ml_env_test\\lib\\site-packages (from google-generativeai->pgmpy) (4.12.2)\n",
      "Requirement already satisfied: proto-plus<2.0.0dev,>=1.22.3 in e:\\anaconda3\\envs\\ml_env_test\\lib\\site-packages (from google-ai-generativelanguage==0.6.15->google-generativeai->pgmpy) (1.26.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\user\\appdata\\roaming\\python\\python312\\site-packages (from pandas->pgmpy) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in e:\\anaconda3\\envs\\ml_env_test\\lib\\site-packages (from pandas->pgmpy) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in e:\\anaconda3\\envs\\ml_env_test\\lib\\site-packages (from pandas->pgmpy) (2023.3)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in e:\\anaconda3\\envs\\ml_env_test\\lib\\site-packages (from scikit-learn->pgmpy) (3.5.0)\n",
      "Requirement already satisfied: patsy>=0.5.6 in e:\\anaconda3\\envs\\ml_env_test\\lib\\site-packages (from statsmodels->pgmpy) (1.0.1)\n",
      "Requirement already satisfied: packaging>=21.3 in c:\\users\\user\\appdata\\roaming\\python\\python312\\site-packages (from statsmodels->pgmpy) (24.2)\n",
      "Requirement already satisfied: filelock in e:\\anaconda3\\envs\\ml_env_test\\lib\\site-packages (from torch->pgmpy) (3.13.1)\n",
      "Requirement already satisfied: setuptools in e:\\anaconda3\\envs\\ml_env_test\\lib\\site-packages (from torch->pgmpy) (75.1.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in e:\\anaconda3\\envs\\ml_env_test\\lib\\site-packages (from torch->pgmpy) (1.13.1)\n",
      "Requirement already satisfied: jinja2 in e:\\anaconda3\\envs\\ml_env_test\\lib\\site-packages (from torch->pgmpy) (3.1.5)\n",
      "Requirement already satisfied: fsspec in e:\\anaconda3\\envs\\ml_env_test\\lib\\site-packages (from torch->pgmpy) (2024.3.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in e:\\anaconda3\\envs\\ml_env_test\\lib\\site-packages (from sympy==1.13.1->torch->pgmpy) (1.3.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\user\\appdata\\roaming\\python\\python312\\site-packages (from tqdm->pgmpy) (0.4.6)\n",
      "Requirement already satisfied: googleapis-common-protos<2.0.dev0,>=1.56.2 in e:\\anaconda3\\envs\\ml_env_test\\lib\\site-packages (from google-api-core->google-generativeai->pgmpy) (1.68.0)\n",
      "Requirement already satisfied: requests<3.0.0.dev0,>=2.18.0 in e:\\anaconda3\\envs\\ml_env_test\\lib\\site-packages (from google-api-core->google-generativeai->pgmpy) (2.32.3)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in e:\\anaconda3\\envs\\ml_env_test\\lib\\site-packages (from google-auth>=2.15.0->google-generativeai->pgmpy) (5.5.2)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in e:\\anaconda3\\envs\\ml_env_test\\lib\\site-packages (from google-auth>=2.15.0->google-generativeai->pgmpy) (0.4.1)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in e:\\anaconda3\\envs\\ml_env_test\\lib\\site-packages (from google-auth>=2.15.0->google-generativeai->pgmpy) (4.9)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\user\\appdata\\roaming\\python\\python312\\site-packages (from python-dateutil>=2.8.2->pandas->pgmpy) (1.16.0)\n",
      "Requirement already satisfied: httplib2<1.dev0,>=0.19.0 in e:\\anaconda3\\envs\\ml_env_test\\lib\\site-packages (from google-api-python-client->google-generativeai->pgmpy) (0.22.0)\n",
      "Requirement already satisfied: google-auth-httplib2<1.0.0,>=0.2.0 in e:\\anaconda3\\envs\\ml_env_test\\lib\\site-packages (from google-api-python-client->google-generativeai->pgmpy) (0.2.0)\n",
      "Requirement already satisfied: uritemplate<5,>=3.0.1 in e:\\anaconda3\\envs\\ml_env_test\\lib\\site-packages (from google-api-python-client->google-generativeai->pgmpy) (4.1.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in e:\\anaconda3\\envs\\ml_env_test\\lib\\site-packages (from jinja2->torch->pgmpy) (2.1.3)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in e:\\anaconda3\\envs\\ml_env_test\\lib\\site-packages (from pydantic->google-generativeai->pgmpy) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.2 in e:\\anaconda3\\envs\\ml_env_test\\lib\\site-packages (from pydantic->google-generativeai->pgmpy) (2.27.2)\n",
      "Requirement already satisfied: grpcio<2.0dev,>=1.33.2 in e:\\anaconda3\\envs\\ml_env_test\\lib\\site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.15->google-generativeai->pgmpy) (1.70.0)\n",
      "Requirement already satisfied: grpcio-status<2.0.dev0,>=1.33.2 in e:\\anaconda3\\envs\\ml_env_test\\lib\\site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.15->google-generativeai->pgmpy) (1.70.0)\n",
      "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in e:\\anaconda3\\envs\\ml_env_test\\lib\\site-packages (from pyasn1-modules>=0.2.1->google-auth>=2.15.0->google-generativeai->pgmpy) (0.6.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in e:\\anaconda3\\envs\\ml_env_test\\lib\\site-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core->google-generativeai->pgmpy) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in e:\\anaconda3\\envs\\ml_env_test\\lib\\site-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core->google-generativeai->pgmpy) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in e:\\anaconda3\\envs\\ml_env_test\\lib\\site-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core->google-generativeai->pgmpy) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in e:\\anaconda3\\envs\\ml_env_test\\lib\\site-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core->google-generativeai->pgmpy) (2024.12.14)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution ~cipy (e:\\anaconda3\\envs\\ml_env_test\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~cipy (e:\\anaconda3\\envs\\ml_env_test\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~cipy (e:\\anaconda3\\envs\\ml_env_test\\Lib\\site-packages)\n"
     ]
    }
   ],
   "source": [
    "%pip install hmmlearn\n",
    "%pip install pgmpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: keras-nlp in e:\\anaconda3\\envs\\ml_env_test\\lib\\site-packages (0.19.1)\n",
      "Requirement already satisfied: keras-hub==0.19.1 in e:\\anaconda3\\envs\\ml_env_test\\lib\\site-packages (from keras-nlp) (0.19.1)\n",
      "Requirement already satisfied: keras>=3.5 in e:\\anaconda3\\envs\\ml_env_test\\lib\\site-packages (from keras-hub==0.19.1->keras-nlp) (3.8.0)\n",
      "Requirement already satisfied: absl-py in e:\\anaconda3\\envs\\ml_env_test\\lib\\site-packages (from keras-hub==0.19.1->keras-nlp) (2.1.0)\n",
      "Requirement already satisfied: numpy in e:\\anaconda3\\envs\\ml_env_test\\lib\\site-packages (from keras-hub==0.19.1->keras-nlp) (1.26.4)\n",
      "Requirement already satisfied: packaging in c:\\users\\user\\appdata\\roaming\\python\\python312\\site-packages (from keras-hub==0.19.1->keras-nlp) (24.2)\n",
      "Requirement already satisfied: regex in e:\\anaconda3\\envs\\ml_env_test\\lib\\site-packages (from keras-hub==0.19.1->keras-nlp) (2024.11.6)\n",
      "Requirement already satisfied: rich in e:\\anaconda3\\envs\\ml_env_test\\lib\\site-packages (from keras-hub==0.19.1->keras-nlp) (13.9.4)\n",
      "Requirement already satisfied: kagglehub in e:\\anaconda3\\envs\\ml_env_test\\lib\\site-packages (from keras-hub==0.19.1->keras-nlp) (0.3.6)\n",
      "Requirement already satisfied: namex in e:\\anaconda3\\envs\\ml_env_test\\lib\\site-packages (from keras>=3.5->keras-hub==0.19.1->keras-nlp) (0.0.8)\n",
      "Requirement already satisfied: h5py in e:\\anaconda3\\envs\\ml_env_test\\lib\\site-packages (from keras>=3.5->keras-hub==0.19.1->keras-nlp) (3.13.0)\n",
      "Requirement already satisfied: optree in e:\\anaconda3\\envs\\ml_env_test\\lib\\site-packages (from keras>=3.5->keras-hub==0.19.1->keras-nlp) (0.14.0)\n",
      "Requirement already satisfied: ml-dtypes in e:\\anaconda3\\envs\\ml_env_test\\lib\\site-packages (from keras>=3.5->keras-hub==0.19.1->keras-nlp) (0.4.1)\n",
      "Requirement already satisfied: requests in e:\\anaconda3\\envs\\ml_env_test\\lib\\site-packages (from kagglehub->keras-hub==0.19.1->keras-nlp) (2.32.3)\n",
      "Requirement already satisfied: tqdm in e:\\anaconda3\\envs\\ml_env_test\\lib\\site-packages (from kagglehub->keras-hub==0.19.1->keras-nlp) (4.66.5)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in e:\\anaconda3\\envs\\ml_env_test\\lib\\site-packages (from rich->keras-hub==0.19.1->keras-nlp) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\user\\appdata\\roaming\\python\\python312\\site-packages (from rich->keras-hub==0.19.1->keras-nlp) (2.18.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in e:\\anaconda3\\envs\\ml_env_test\\lib\\site-packages (from markdown-it-py>=2.2.0->rich->keras-hub==0.19.1->keras-nlp) (0.1.2)\n",
      "Requirement already satisfied: typing-extensions>=4.5.0 in e:\\anaconda3\\envs\\ml_env_test\\lib\\site-packages (from optree->keras>=3.5->keras-hub==0.19.1->keras-nlp) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in e:\\anaconda3\\envs\\ml_env_test\\lib\\site-packages (from requests->kagglehub->keras-hub==0.19.1->keras-nlp) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in e:\\anaconda3\\envs\\ml_env_test\\lib\\site-packages (from requests->kagglehub->keras-hub==0.19.1->keras-nlp) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in e:\\anaconda3\\envs\\ml_env_test\\lib\\site-packages (from requests->kagglehub->keras-hub==0.19.1->keras-nlp) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in e:\\anaconda3\\envs\\ml_env_test\\lib\\site-packages (from requests->kagglehub->keras-hub==0.19.1->keras-nlp) (2024.12.14)\n",
      "Requirement already satisfied: colorama in c:\\users\\user\\appdata\\roaming\\python\\python312\\site-packages (from tqdm->kagglehub->keras-hub==0.19.1->keras-nlp) (0.4.6)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution ~cipy (e:\\anaconda3\\envs\\ml_env_test\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~cipy (e:\\anaconda3\\envs\\ml_env_test\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~cipy (e:\\anaconda3\\envs\\ml_env_test\\Lib\\site-packages)\n"
     ]
    }
   ],
   "source": [
    "%pip install keras-nlp --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tqdm import tqdm\n",
    "import keras_tuner as kt\n",
    "from tensorflow.keras.models import load_model\n",
    "import keras_nlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split, cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier, plot_tree\n",
    "from sklearn.linear_model import Perceptron, LogisticRegression\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import GaussianNB, MultinomialNB\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
    "from sklearn.preprocessing import MaxAbsScaler, MinMaxScaler\n",
    "import xgboost as xgb\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import hmmlearn.hmm\n",
    "from hmmlearn.hmm import GaussianHMM\n",
    "from sklearn_crfsuite import CRF\n",
    "from sklearn.metrics import log_loss, hinge_loss, precision_score, recall_score, f1_score, roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pgmpy.models import BayesianNetwork\n",
    "from pgmpy.estimators import MaximumLikelihoodEstimator\n",
    "from pgmpy.inference import VariableElimination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Options\n",
    "pd.set_option(\"max_colwidth\", None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "e:\\2_LEARNING_BKU\\2_File_2\\K22_HK242\\CO3117_Machine_Learning\\Main\n"
     ]
    }
   ],
   "source": [
    "# Get the absolute path to the 'src' directory\n",
    "project_root = os.path.abspath(os.path.join(os.getcwd(), \"..\", \"..\"))\n",
    "sys.path.append(project_root)\n",
    "print(project_root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.features.build_features_utils import *  # Assuming build_features_utils is inside build_features.py\n",
    "from src.models.models_utils import *  # Assuming utils.py exists inside src/models/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionary for models\n",
    "MODEL_DICT = {\n",
    "    \"decision_tree\": DecisionTreeClassifier,\n",
    "    \"perceptron\": Perceptron,\n",
    "    \"mlp\": MLPClassifier,\n",
    "    \"bayesian\": GaussianNB,\n",
    "    \"random_forest\": RandomForestClassifier,\n",
    "    \"xgboost\": xgb.XGBClassifier,\n",
    "    \"logistic_regression\": LogisticRegression,\n",
    "    \n",
    "    \"svm\": SVC\n",
    "} \n",
    "\n",
    "# Dictionary for model parameters\n",
    "MODEL_PARAMS = {\n",
    "    # \"decision_tree\": {\n",
    "    #     \"criterion\": [\"gini\", \"entropy\"],\n",
    "    #     \"max_depth\": [10, 20],\n",
    "    #     \"min_samples_split\": [2, 5],\n",
    "    #     \"min_samples_leaf\": [1, 2],\n",
    "    #     \"max_features\": [\"sqrt\", \"log2\"]\n",
    "    # },\n",
    "    \n",
    "    \"decision_tree\": {\n",
    "        \"criterion\": [\"gini\", \"entropy\"],\n",
    "        \"max_depth\": [10, 20, 30, 40],\n",
    "        \"min_samples_split\": [2, 5, 10],\n",
    "        \"min_samples_leaf\": [1, 2, 4],\n",
    "        \"max_features\": [\"sqrt\", \"log2\"]\n",
    "    },\n",
    "    \n",
    "    # \"perceptron\": {\n",
    "    #     \"max_iter\": [1000, 2000],\n",
    "    #     \"tol\": [1e-3],\n",
    "    #     \"eta0\": [0.001],\n",
    "    #     \"penalty\": [\"l2\"],\n",
    "    #     \"alpha\": [0.0001, 0.001]\n",
    "    # },\n",
    "    \n",
    "    \"perceptron\": {\n",
    "        \"max_iter\": [1000, 2000],\n",
    "        \"tol\": [1e-3, 1e-4],\n",
    "        \"eta0\": [0.001, 0.01, 0.1],\n",
    "        \"penalty\": [None, \"l2\", \"l1\"],\n",
    "        \"alpha\": [0.0001, 0.001, 0.01]\n",
    "    },\n",
    "    \n",
    "    \"mlp\": {\n",
    "        \"hidden_layer_sizes\": [(100,)],\n",
    "        \"activation\": [\"tanh\", \"logistic\"],\n",
    "        \"solver\": [\"sgd\"],\n",
    "        \"alpha\": [0.01],\n",
    "        \"batch_size\": [32],\n",
    "        \"max_iter\": [2000],\n",
    "    },\n",
    "    \n",
    "    # \"mlp\": {\n",
    "    #     \"hidden_layer_sizes\": [(50,), (100,), (50, 50), (100, 100)],\n",
    "    #     \"activation\": [\"relu\", \"tanh\", \"logistic\"],\n",
    "    #     \"solver\": [\"adam\", \"sgd\"],\n",
    "    #     \"alpha\": [0.0001, 0.001, 0.01],\n",
    "    #     \"batch_size\": [32, 64, 128],\n",
    "    #     \"max_iter\": [500, 1000],\n",
    "    #     \"learning_rate\": [\"constant\", \"invscaling\", \"adaptive\"]\n",
    "    # },\n",
    "    \n",
    "    \"bayesian\": {\n",
    "        \"priors\": [None, [0.5, 0.5], [0.4, 0.6], [0.3, 0.7], [0.2, 0.8], [0.1, 0.9], [0.05, 0.95]],\n",
    "        \"var_smoothing\": [1e-9, 1e-8, 1e-7]\n",
    "    },\n",
    "    \n",
    "    \"random_forest\": {\n",
    "        \"n_estimators\": [100, 200],\n",
    "        \"max_depth\": [10],\n",
    "        \"min_samples_split\": [2, 5],\n",
    "        \"min_samples_leaf\": [1, 2],\n",
    "        \"max_features\": [\"sqrt\", \"log2\"],\n",
    "        \"bootstrap\": [True, False]\n",
    "    },\n",
    "    \n",
    "    # \"random_forest\": {\n",
    "    #     \"n_estimators\": [50, 100, 200],\n",
    "    #     \"max_depth\": [None, 10, 20, 30],\n",
    "    #     \"min_samples_split\": [2, 5, 10],\n",
    "    #     \"min_samples_leaf\": [1, 2, 4],\n",
    "    #     \"max_features\": [\"auto\", \"sqrt\", \"log2\"],\n",
    "    #     \"bootstrap\": [True, False]\n",
    "    # },\n",
    "    \n",
    "    \"xgboost\": {\n",
    "        \"n_estimators\": [100],\n",
    "        \"learning_rate\": [0.01, 0.1],\n",
    "        \"max_depth\": [6, 10]\n",
    "    },\n",
    "    \n",
    "    # \"xgboost\": {\n",
    "    #     \"n_estimators\": [100, 200, 300],\n",
    "    #     \"learning_rate\": [0.01, 0.1, 0.2],\n",
    "    #     \"max_depth\": [3, 6, 10],\n",
    "    #     \"subsample\": [0.8, 1.0],\n",
    "    #     \"colsample_bytree\": [0.8, 1.0],\n",
    "    #     \"gamma\": [0, 0.1, 0.2]\n",
    "    # },\n",
    "    \n",
    "    \"svm\": {\n",
    "        \"kernel\": [\"linear\"],\n",
    "        \"C\": [0.001, 0.01, 0.1, 1],\n",
    "        \"gamma\": [0.1, 0.01, \"scale\", \"auto\"]\n",
    "    },\n",
    "    \n",
    "    # \"svm\": {\n",
    "    #     \"kernel\": [\"linear\", \"rbf\", \"poly\"],\n",
    "    #     \"C\": [0.1, 1, 10, 100],\n",
    "    #     \"gamma\": [0.1, 0.01, \"scale\", \"auto\"],\n",
    "    #     \"degree\": [2, 3, 4]\n",
    "    # },\n",
    "    \n",
    "    # \"logistic_regression\": {\n",
    "    #     \"penalty\": [\"l2\"],\n",
    "    #     \"C\": [0.1, 1.0],\n",
    "    #     \"max_iter\": [1000, 2000]\n",
    "    # },\n",
    "    \n",
    "    \"logistic_regression\": {\n",
    "        \"penalty\": [\"l1\", \"l2\", \"elasticnet\", None],\n",
    "        \"C\": [0.1, 1.0, 10.0],\n",
    "        \"max_iter\": [1000, 2000]\n",
    "    },\n",
    "    \n",
    "    \n",
    "    # \"hmm\": {\n",
    "    #     \"n_components\": [2],  # Keep it small\n",
    "    #     \"covariance_type\": [\"diag\"],  # Simpler covariance type\n",
    "    #     \"n_iter\": [500],  # Reduce iterations\n",
    "    #     \"init_params\": [\"stmc\"],  # Initialize start probabilities, transition matrix, and means/covariance\n",
    "    #     \"params\": [\"stmc\"]\n",
    "    # },\n",
    "    \n",
    "    \"hmm\": {\n",
    "        \"n_components\": [2, 3, 4],\n",
    "        \"covariance_type\": [\"diag\", \"full\", \"tied\"],\n",
    "        \"n_iter\": [100, 200],\n",
    "        \"init_params\": [\"c\", \"s\", \"cs\"],\n",
    "        \"params\": [\"c\", \"t\", \"ct\"]\n",
    "    },\n",
    "    \n",
    "    \"bayes_network\": {\n",
    "        \"structure\": [None],\n",
    "        \"n_bins\": [2],\n",
    "        \"strategy\": [\"kmeans\"],\n",
    "        \"min_unique_values\": [2],\n",
    "        \"max_features\": [10]\n",
    "    },\n",
    "    \n",
    "    # \"crf\": {\n",
    "    #     \"c1\": [0.1, 0.01],  # L1 Regularization\n",
    "    #     \"c2\": [0.1, 0.01],  # L2 Regularization\n",
    "    #     \"max_iterations\": [50, 100]  # Limit iterations\n",
    "    # }\n",
    "}\n",
    "\n",
    "BEST_MODEL_PARAMS = {\n",
    "    \"decision_tree\": {\n",
    "        \"criterion\": [\"gini\"],\n",
    "        \"max_depth\": [40],\n",
    "        \"min_samples_split\": [10],\n",
    "        \"min_samples_leaf\": [4],\n",
    "        \"max_features\": [\"sqrt\"]\n",
    "    },\n",
    "    \n",
    "    \"perceptron\": {\n",
    "        \"max_iter\": [1000],\n",
    "        \"tol\": [1e-3],\n",
    "        \"eta0\": [0.001],\n",
    "        \"penalty\": [\"l2\"],\n",
    "        \"alpha\": [0.0001]\n",
    "    },\n",
    "    \n",
    "    \"mlp\": {\n",
    "        \"hidden_layer_sizes\": [(100,)],\n",
    "        \"activation\": [\"tanh\", \"logistic\"],\n",
    "        \"solver\": [\"sgd\"],\n",
    "        \"alpha\": [0.01],\n",
    "        \"batch_size\": [32],\n",
    "        \"max_iter\": [2000],\n",
    "    },\n",
    "    \n",
    "    \"bayesian\": {\n",
    "        \"priors\": [[0.3, 0.7]],\n",
    "        \"var_smoothing\": [1e-9]\n",
    "    },\n",
    "    \n",
    "    \"random_forest\": {\n",
    "        \"n_estimators\": [100],\n",
    "        \"max_depth\": [10],\n",
    "        \"min_samples_split\": [5],\n",
    "        \"min_samples_leaf\": [1],\n",
    "        \"max_features\": [\"sqrt\"]\n",
    "    },\n",
    "    \n",
    "    \"xgboost\": {\n",
    "        \"n_estimators\": [150],\n",
    "        \"learning_rate\": [0.1],\n",
    "        \"max_depth\": [15]\n",
    "    },\n",
    "    \n",
    "    \"svm\": {\n",
    "        \"kernel\": [\"linear\"],\n",
    "        \"C\": [0.001, 0.01, 0.1, 1],\n",
    "        \"gamma\": [0.1, 0.01, \"scale\", \"auto\"]\n",
    "    },\n",
    "    \n",
    "    \"logistic_regression\": {\n",
    "        \"penalty\": [\"l2\"],\n",
    "        \"C\": [0.1],\n",
    "        \"max_iter\": [1000]\n",
    "    },\n",
    "    \n",
    "    \"hmm\": {\n",
    "        \"n_components\": [2, 3, 4],\n",
    "        \"covariance_type\": [\"diag\", \"full\", \"tied\"],\n",
    "        \"n_iter\": [100, 200],\n",
    "        \"init_params\": [\"c\", \"s\", \"cs\"],\n",
    "        \"params\": [\"c\", \"t\", \"ct\"]\n",
    "    },\n",
    "    \n",
    "    \"bayes_network\": {\n",
    "        \"structure\": [None],\n",
    "        \"n_bins\": [2],\n",
    "        \"strategy\": [\"kmeans\"],\n",
    "        \"min_unique_values\": [2],\n",
    "        \"max_features\": [10]\n",
    "    },\n",
    "    \n",
    "    # \"crf\": {\n",
    "    #     \"c1\": [0.1, 0.01],  # L1 Regularization\n",
    "    #     \"c2\": [0.1, 0.01],  # L2 Regularization\n",
    "    #     \"max_iterations\": [50, 100]  # Limit iterations\n",
    "    # }\n",
    "}\n",
    "\n",
    "# Dictionary for dimensionality reduction methods\n",
    "DIMENSIONALITY_REDUCTION_DICT = {\n",
    "    \"pca\": PCA,\n",
    "    \"lda\": LDA,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "dataset_path = os.path.join(project_root, \"data\", \"final\", \"final_clean_no_neutral_no_duplicates_v1.csv\")\n",
    "df = pd.read_csv(dataset_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "target",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "text",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "text_clean",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "text_length",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "text_clean_length",
         "rawType": "int64",
         "type": "integer"
        }
       ],
       "conversionMethod": "pd.DataFrame",
       "ref": "2390872c-87ac-43ac-947e-90855beb97c5",
       "rows": [
        [
         "0",
         "0.0",
         "@switchfoot http://twitpic.com/2y1zl - Awww, that's a bummer.  You shoulda got David Carr of Third Day to do it. ;D",
         "switchfoot awww thats bummer shoulda got david carr third day",
         "19",
         "10"
        ],
        [
         "1",
         "0.0",
         "is upset that he can't update his Facebook by texting it... and might cry as a result  School today also. Blah!",
         "upset cant update facebook texting might cry result school today also blah",
         "21",
         "12"
        ],
        [
         "2",
         "0.0",
         "@Kenichan I dived many times for the ball. Managed to save 50%  The rest go out of bounds",
         "kenichan dived many times ball managed save  rest go bounds",
         "18",
         "10"
        ],
        [
         "3",
         "0.0",
         "my whole body feels itchy and like its on fire ",
         "whole body feels itchy like fire",
         "10",
         "6"
        ],
        [
         "4",
         "0.0",
         "@nationwideclass no, it's not behaving at all. i'm mad. why am i here? because I can't see you all over there. ",
         "nationwideclass behaving im mad cant see",
         "21",
         "6"
        ]
       ],
       "shape": {
        "columns": 5,
        "rows": 5
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "      <th>text</th>\n",
       "      <th>text_clean</th>\n",
       "      <th>text_length</th>\n",
       "      <th>text_clean_length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>@switchfoot http://twitpic.com/2y1zl - Awww, that's a bummer.  You shoulda got David Carr of Third Day to do it. ;D</td>\n",
       "      <td>switchfoot awww thats bummer shoulda got david carr third day</td>\n",
       "      <td>19</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>is upset that he can't update his Facebook by texting it... and might cry as a result  School today also. Blah!</td>\n",
       "      <td>upset cant update facebook texting might cry result school today also blah</td>\n",
       "      <td>21</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>@Kenichan I dived many times for the ball. Managed to save 50%  The rest go out of bounds</td>\n",
       "      <td>kenichan dived many times ball managed save  rest go bounds</td>\n",
       "      <td>18</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>my whole body feels itchy and like its on fire</td>\n",
       "      <td>whole body feels itchy like fire</td>\n",
       "      <td>10</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>@nationwideclass no, it's not behaving at all. i'm mad. why am i here? because I can't see you all over there.</td>\n",
       "      <td>nationwideclass behaving im mad cant see</td>\n",
       "      <td>21</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   target  \\\n",
       "0     0.0   \n",
       "1     0.0   \n",
       "2     0.0   \n",
       "3     0.0   \n",
       "4     0.0   \n",
       "\n",
       "                                                                                                                  text  \\\n",
       "0  @switchfoot http://twitpic.com/2y1zl - Awww, that's a bummer.  You shoulda got David Carr of Third Day to do it. ;D   \n",
       "1      is upset that he can't update his Facebook by texting it... and might cry as a result  School today also. Blah!   \n",
       "2                            @Kenichan I dived many times for the ball. Managed to save 50%  The rest go out of bounds   \n",
       "3                                                                      my whole body feels itchy and like its on fire    \n",
       "4      @nationwideclass no, it's not behaving at all. i'm mad. why am i here? because I can't see you all over there.    \n",
       "\n",
       "                                                                   text_clean  \\\n",
       "0               switchfoot awww thats bummer shoulda got david carr third day   \n",
       "1  upset cant update facebook texting might cry result school today also blah   \n",
       "2                 kenichan dived many times ball managed save  rest go bounds   \n",
       "3                                            whole body feels itchy like fire   \n",
       "4                                    nationwideclass behaving im mad cant see   \n",
       "\n",
       "   text_length  text_clean_length  \n",
       "0           19                 10  \n",
       "1           21                 12  \n",
       "2           18                 10  \n",
       "3           10                  6  \n",
       "4           21                  6  "
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace target 4 with 1\n",
    "df[\"target\"] = df[\"target\"].replace(4, 1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_methods = [\"tfidf\", \"count\", \"word2vec\", \"glove\"]\n",
    "df_sampled = df.sample(n=1000, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_lst = df_sampled[\"text_clean\"].tolist()\n",
    "label_lst = df_sampled[\"target\"].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔎 Running feature extraction...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Feature Extraction Progress:   0%|          | 0/4 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔍 Processing feature extraction using: tfidf...\n",
      "✅ tfidf - Train shape: (800, 2000), Test shape: (200, 2000)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Feature Extraction Progress:  25%|██▌       | 1/4 [00:00<00:00,  5.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔍 Processing feature extraction using: count...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Feature Extraction Progress:  50%|█████     | 2/4 [00:00<00:00,  3.72it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ count - Train shape: (800, 2000), Test shape: (200, 2000)\n",
      "\n",
      "🔍 Processing feature extraction using: word2vec...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:gensim.models.keyedvectors:loading projection weights from C:\\Users\\User/gensim-data\\word2vec-google-news-300\\word2vec-google-news-300.gz\n",
      "INFO:gensim.utils:KeyedVectors lifecycle event {'msg': 'loaded (3000000, 300) matrix of type float32 from C:\\\\Users\\\\User/gensim-data\\\\word2vec-google-news-300\\\\word2vec-google-news-300.gz', 'binary': True, 'encoding': 'utf8', 'datetime': '2025-03-01T11:11:37.585444', 'gensim': '4.3.3', 'python': '3.12.8 | packaged by Anaconda, Inc. | (main, Dec 11 2024, 16:48:34) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'load_word2vec_format'}\n",
      "Processing Word2Vec: 100%|██████████| 800/800 [00:00<00:00, 1833.10document/s]\n",
      "Processing Word2Vec: 100%|██████████| 200/200 [00:00<00:00, 2985.21document/s]\n",
      "Feature Extraction Progress:  75%|███████▌  | 3/4 [02:19<01:03, 63.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ word2vec - Train shape: (800, 300), Test shape: (200, 300)\n",
      "\n",
      "🔍 Processing feature extraction using: glove...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:gensim.models.keyedvectors:loading projection weights from C:\\Users\\User/gensim-data\\glove-wiki-gigaword-100\\glove-wiki-gigaword-100.gz\n",
      "INFO:gensim.utils:KeyedVectors lifecycle event {'msg': 'loaded (400000, 100) matrix of type float32 from C:\\\\Users\\\\User/gensim-data\\\\glove-wiki-gigaword-100\\\\glove-wiki-gigaword-100.gz', 'binary': False, 'encoding': 'utf8', 'datetime': '2025-03-01T11:14:18.746341', 'gensim': '4.3.3', 'python': '3.12.8 | packaged by Anaconda, Inc. | (main, Dec 11 2024, 16:48:34) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'load_word2vec_format'}\n",
      "Processing GloVe: 100%|██████████| 800/800 [00:00<00:00, 4122.43document/s]\n",
      "Processing GloVe: 100%|██████████| 200/200 [00:00<00:00, 3571.41document/s]\n",
      "Feature Extraction Progress: 100%|██████████| 4/4 [05:00<00:00, 75.14s/it] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ glove - Train shape: (800, 100), Test shape: (200, 100)\n"
     ]
    }
   ],
   "source": [
    "X_train_features_dict, X_test_features_dict, y_train, y_test = build_vector_for_text(df_sampled, feature_methods, project_root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📊 Dataset Shapes:\n",
      "🔹 X_train (tfidf): (800, 2000)\n",
      "🔹 X_train (count): (800, 2000)\n",
      "🔹 X_train (word2vec): (800, 300)\n",
      "🔹 X_train (glove): (800, 100)\n",
      "🔹 X_test (tfidf): (200, 2000)\n",
      "🔹 X_test (count): (200, 2000)\n",
      "🔹 X_test (word2vec): (200, 300)\n",
      "🔹 X_test (glove): (200, 100)\n",
      "\n",
      "🎯 y_train shape: (800,)\n",
      "🎯 y_test shape: (200,)\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n📊 Dataset Shapes:\")\n",
    "\n",
    "# Print the shape of feature matrices for each feature method\n",
    "for feature_method, X_train in X_train_features_dict.items():\n",
    "    print(f\"🔹 X_train ({feature_method}): {X_train.shape}\")\n",
    "    \n",
    "for feature_method, X_test in X_test_features_dict.items():\n",
    "    print(f\"🔹 X_test ({feature_method}): {X_test.shape}\")\n",
    "\n",
    "# Print y_train and y_test shapes\n",
    "print(f\"\\n🎯 y_train shape: {y_train.shape}\")\n",
    "print(f\"🎯 y_test shape: {y_test.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part nay dang test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔎 Running feature extraction...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Feature Extraction Progress:   0%|          | 0/4 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔍 Processing feature extraction using: tfidf...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Feature Extraction Progress:  25%|██▌       | 1/4 [00:03<00:10,  3.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ tfidf - Train shape: (800, 100), Test shape: (200, 100)\n",
      "\n",
      "🔍 Processing feature extraction using: count...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Feature Extraction Progress:  50%|█████     | 2/4 [00:09<00:09,  4.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ count - Train shape: (800, 100), Test shape: (200, 100)\n",
      "\n",
      "🔍 Processing feature extraction using: word2vec...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:gensim.models.keyedvectors:loading projection weights from C:\\Users\\User/gensim-data\\word2vec-google-news-300\\word2vec-google-news-300.gz\n",
      "INFO:gensim.utils:KeyedVectors lifecycle event {'msg': 'loaded (3000000, 300) matrix of type float32 from C:\\\\Users\\\\User/gensim-data\\\\word2vec-google-news-300\\\\word2vec-google-news-300.gz', 'binary': True, 'encoding': 'utf8', 'datetime': '2025-03-01T11:41:06.453684', 'gensim': '4.3.3', 'python': '3.12.8 | packaged by Anaconda, Inc. | (main, Dec 11 2024, 16:48:34) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'load_word2vec_format'}\n",
      "Processing Word2Vec: 100%|██████████| 800/800 [00:00<00:00, 9999.92document/s]\n",
      "Processing Word2Vec: 100%|██████████| 200/200 [00:00<00:00, 8695.47document/s]\n",
      "Feature Extraction Progress:  75%|███████▌  | 3/4 [02:14<00:59, 59.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ word2vec - Train shape: (800, 300), Test shape: (200, 300)\n",
      "\n",
      "🔍 Processing feature extraction using: glove...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:gensim.models.keyedvectors:loading projection weights from C:\\Users\\User/gensim-data\\glove-wiki-gigaword-100\\glove-wiki-gigaword-100.gz\n",
      "INFO:gensim.utils:KeyedVectors lifecycle event {'msg': 'loaded (400000, 100) matrix of type float32 from C:\\\\Users\\\\User/gensim-data\\\\glove-wiki-gigaword-100\\\\glove-wiki-gigaword-100.gz', 'binary': False, 'encoding': 'utf8', 'datetime': '2025-03-01T11:42:21.253975', 'gensim': '4.3.3', 'python': '3.12.8 | packaged by Anaconda, Inc. | (main, Dec 11 2024, 16:48:34) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'load_word2vec_format'}\n",
      "Processing GloVe: 100%|██████████| 800/800 [00:00<00:00, 6153.92document/s]\n",
      "Processing GloVe: 100%|██████████| 200/200 [00:00<00:00, 6666.67document/s]\n",
      "Feature Extraction Progress: 100%|██████████| 4/4 [03:29<00:00, 52.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ glove - Train shape: (800, 100), Test shape: (200, 100)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "X_train_pca_dict, X_test_pca_dict, y_train, y_test = build_vector_for_text(df_sampled, feature_methods, project_root, \"pca\", 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📊 Dataset Shapes:\n",
      "🔹 X_train (tfidf): (800, 100)\n",
      "🔹 X_train (count): (800, 100)\n",
      "🔹 X_train (word2vec): (800, 300)\n",
      "🔹 X_train (glove): (800, 100)\n",
      "🔹 X_test (tfidf): (200, 100)\n",
      "🔹 X_test (count): (200, 100)\n",
      "🔹 X_test (word2vec): (200, 300)\n",
      "🔹 X_test (glove): (200, 100)\n",
      "\n",
      "🎯 y_train shape: (800,)\n",
      "🎯 y_test shape: (200,)\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n📊 Dataset Shapes:\")\n",
    "\n",
    "# Print the shape of feature matrices for each feature method\n",
    "for feature_method, X_train in X_train_pca_dict.items():\n",
    "    print(f\"🔹 X_train ({feature_method}): {X_train.shape}\")\n",
    "    \n",
    "for feature_method, X_test in X_test_pca_dict.items():\n",
    "    print(f\"🔹 X_test ({feature_method}): {X_test.shape}\")\n",
    "\n",
    "# Print y_train and y_test shapes\n",
    "print(f\"\\n🎯 y_train shape: {y_train.shape}\")\n",
    "print(f\"🎯 y_test shape: {y_test.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train_lda_dict, X_test_lda_dict, y_train, y_test = build_vector_for_text(df_sampled, feature_methods, project_root, \"lda\", 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"\\n📊 Dataset Shapes:\")\n",
    "\n",
    "# # Print the shape of feature matrices for each feature method\n",
    "# for feature_method, X_train in X_train_lda_dict.items():\n",
    "#     print(f\"🔹 X_train ({feature_method}): {X_train.shape}\")\n",
    "    \n",
    "# for feature_method, X_test in X_test_lda_dict.items():\n",
    "#     print(f\"🔹 X_test ({feature_method}): {X_test.shape}\")\n",
    "\n",
    "# # Print y_train and y_test shapes\n",
    "# print(f\"\\n🎯 y_train shape: {y_train.shape}\")\n",
    "# print(f\"🎯 y_test shape: {y_test.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# New API Call"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name_lst = [\n",
    "    \"decision_tree\",\n",
    "    \"random_forest\", \n",
    "    \"xgboost\", \n",
    "    \"perceptron\", \n",
    "    \"mlp\", \n",
    "    \"lstm\",\n",
    "    \"bayesian\",\n",
    "    \"GA\",\n",
    "    \"hmm\",\n",
    "    \"bayesnet\",\n",
    "    \"logistic_regression\",\n",
    "    \"svm\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_model = os.path.join(project_root, \"src\", \"models\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip uninstall tf-nightly\n",
    "# %pip install tensorflow\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip show keras-nlp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_general_model(df_sampled, doc_lst, label_lst, model_name_lst, feature_methods, MODEL_DICT, MODEL_PARAMS, X_train_features_dict, X_test_features_dict, y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_general_model(model_name_lst, feature_methods, X_test_features_dict, y_test, trained_model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_env_test",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
