{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install tensorflow keras\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install keras-tuner\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install hmmlearn\n",
    "%pip install pgmpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install keras-nlp --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tqdm import tqdm\n",
    "import keras_tuner as kt\n",
    "from tensorflow.keras.models import load_model\n",
    "import keras_nlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split, cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier, plot_tree\n",
    "from sklearn.linear_model import Perceptron, LogisticRegression\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import GaussianNB, MultinomialNB\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
    "from sklearn.preprocessing import MaxAbsScaler, MinMaxScaler\n",
    "import xgboost as xgb\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import hmmlearn.hmm\n",
    "from hmmlearn.hmm import GaussianHMM\n",
    "from sklearn_crfsuite import CRF\n",
    "from sklearn.metrics import log_loss, hinge_loss, precision_score, recall_score, f1_score, roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pgmpy.models import BayesianNetwork\n",
    "from pgmpy.estimators import MaximumLikelihoodEstimator\n",
    "from pgmpy.inference import VariableElimination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Options\n",
    "pd.set_option(\"max_colwidth\", None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the absolute path to the 'src' directory\n",
    "project_root = os.path.abspath(os.path.join(os.getcwd(), \"..\", \"..\"))\n",
    "sys.path.append(project_root)\n",
    "print(project_root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.features.build_features_utils import *  # Assuming build_features_utils is inside build_features.py\n",
    "from src.models.models_utils import *  # Assuming utils.py exists inside src/models/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionary for models\n",
    "MODEL_DICT = {\n",
    "    \"decision_tree\": DecisionTreeClassifier,\n",
    "    \"perceptron\": Perceptron,\n",
    "    \"mlp\": MLPClassifier,\n",
    "    \"bayesian\": GaussianNB,\n",
    "    \"random_forest\": RandomForestClassifier,\n",
    "    \"xgboost\": xgb.XGBClassifier,\n",
    "    \"svm\": SVC,\n",
    "    \"logistic_regression\": LogisticRegression,\n",
    "    \"hmm\": lambda: hmmlearn.hmm.GaussianHMM(n_components=3),\n",
    "    \"crf\": lambda: CRF(\n",
    "        algorithm=\"lbfgs\",  \n",
    "        max_iterations=100,  \n",
    "        all_possible_transitions=True # not tested yet\n",
    "    ),\n",
    "    \"bayes_network\": BayesianNetworkClassifier, # Loc defined\n",
    "} \n",
    "\n",
    "# Dictionary for model parameters\n",
    "MODEL_PARAMS = {\n",
    "    # \"decision_tree\": {\n",
    "    #     \"criterion\": [\"gini\", \"entropy\"],\n",
    "    #     \"max_depth\": [10, 20],\n",
    "    #     \"min_samples_split\": [2, 5],\n",
    "    #     \"min_samples_leaf\": [1, 2],\n",
    "    #     \"max_features\": [\"sqrt\", \"log2\"]\n",
    "    # },\n",
    "    \n",
    "    \"decision_tree\": {\n",
    "        \"criterion\": [\"gini\", \"entropy\"],\n",
    "        \"max_depth\": [10, 20, 30, 40],\n",
    "        \"min_samples_split\": [2, 5, 10],\n",
    "        \"min_samples_leaf\": [1, 2, 4],\n",
    "        \"max_features\": [\"sqrt\", \"log2\"]\n",
    "    },\n",
    "    \n",
    "    # \"perceptron\": {\n",
    "    #     \"max_iter\": [1000, 2000],\n",
    "    #     \"tol\": [1e-3],\n",
    "    #     \"eta0\": [0.001],\n",
    "    #     \"penalty\": [\"l2\"],\n",
    "    #     \"alpha\": [0.0001, 0.001]\n",
    "    # },\n",
    "    \n",
    "    \"perceptron\": {\n",
    "        \"max_iter\": [1000, 2000],\n",
    "        \"tol\": [1e-3, 1e-4],\n",
    "        \"eta0\": [0.001, 0.01, 0.1],\n",
    "        \"penalty\": [None, \"l2\", \"l1\"],\n",
    "        \"alpha\": [0.0001, 0.001, 0.01]\n",
    "    },\n",
    "    \n",
    "    \"mlp\": {\n",
    "        \"hidden_layer_sizes\": [(100,)],\n",
    "        \"activation\": [\"tanh\", \"logistic\"],\n",
    "        \"solver\": [\"sgd\"],\n",
    "        \"alpha\": [0.01],\n",
    "        \"batch_size\": [32],\n",
    "        \"max_iter\": [2000],\n",
    "    },\n",
    "    \n",
    "    # \"mlp\": {\n",
    "    #     \"hidden_layer_sizes\": [(50,), (100,), (50, 50), (100, 100)],\n",
    "    #     \"activation\": [\"relu\", \"tanh\", \"logistic\"],\n",
    "    #     \"solver\": [\"adam\", \"sgd\"],\n",
    "    #     \"alpha\": [0.0001, 0.001, 0.01],\n",
    "    #     \"batch_size\": [32, 64, 128],\n",
    "    #     \"max_iter\": [500, 1000],\n",
    "    #     \"learning_rate\": [\"constant\", \"invscaling\", \"adaptive\"]\n",
    "    # },\n",
    "    \n",
    "    \"bayesian\": {\n",
    "        \"priors\": [None, [0.5, 0.5], [0.4, 0.6], [0.3, 0.7], [0.2, 0.8], [0.1, 0.9], [0.05, 0.95]],\n",
    "        \"var_smoothing\": [1e-9, 1e-8, 1e-7]\n",
    "    },\n",
    "    \n",
    "    \"random_forest\": {\n",
    "        \"n_estimators\": [100, 200],\n",
    "        \"max_depth\": [10],\n",
    "        \"min_samples_split\": [2, 5],\n",
    "        \"min_samples_leaf\": [1, 2],\n",
    "        \"max_features\": [\"sqrt\", \"log2\"],\n",
    "        \"bootstrap\": [True, False]\n",
    "    },\n",
    "    \n",
    "    # \"random_forest\": {\n",
    "    #     \"n_estimators\": [50, 100, 200],\n",
    "    #     \"max_depth\": [None, 10, 20, 30],\n",
    "    #     \"min_samples_split\": [2, 5, 10],\n",
    "    #     \"min_samples_leaf\": [1, 2, 4],\n",
    "    #     \"max_features\": [\"auto\", \"sqrt\", \"log2\"],\n",
    "    #     \"bootstrap\": [True, False]\n",
    "    # },\n",
    "    \n",
    "    \"xgboost\": {\n",
    "        \"n_estimators\": [100],\n",
    "        \"learning_rate\": [0.01, 0.1],\n",
    "        \"max_depth\": [6, 10]\n",
    "    },\n",
    "    \n",
    "    # \"xgboost\": {\n",
    "    #     \"n_estimators\": [100, 200, 300],\n",
    "    #     \"learning_rate\": [0.01, 0.1, 0.2],\n",
    "    #     \"max_depth\": [3, 6, 10],\n",
    "    #     \"subsample\": [0.8, 1.0],\n",
    "    #     \"colsample_bytree\": [0.8, 1.0],\n",
    "    #     \"gamma\": [0, 0.1, 0.2]\n",
    "    # },\n",
    "    \n",
    "    \"svm\": {\n",
    "        \"kernel\": [\"linear\"],\n",
    "        \"C\": [0.001, 0.01, 0.1, 1],\n",
    "        \"gamma\": [0.1, 0.01, \"scale\", \"auto\"]\n",
    "    },\n",
    "    \n",
    "    # \"svm\": {\n",
    "    #     \"kernel\": [\"linear\", \"rbf\", \"poly\"],\n",
    "    #     \"C\": [0.1, 1, 10, 100],\n",
    "    #     \"gamma\": [0.1, 0.01, \"scale\", \"auto\"],\n",
    "    #     \"degree\": [2, 3, 4]\n",
    "    # },\n",
    "    \n",
    "    # \"logistic_regression\": {\n",
    "    #     \"penalty\": [\"l2\"],\n",
    "    #     \"C\": [0.1, 1.0],\n",
    "    #     \"max_iter\": [1000, 2000]\n",
    "    # },\n",
    "    \n",
    "    \"logistic_regression\": {\n",
    "        \"penalty\": [\"l1\", \"l2\", \"elasticnet\", None],\n",
    "        \"C\": [0.1, 1.0, 10.0],\n",
    "        \"max_iter\": [1000, 2000]\n",
    "    },\n",
    "    \n",
    "    \n",
    "    # \"hmm\": {\n",
    "    #     \"n_components\": [2],  # Keep it small\n",
    "    #     \"covariance_type\": [\"diag\"],  # Simpler covariance type\n",
    "    #     \"n_iter\": [500],  # Reduce iterations\n",
    "    #     \"init_params\": [\"stmc\"],  # Initialize start probabilities, transition matrix, and means/covariance\n",
    "    #     \"params\": [\"stmc\"]\n",
    "    # },\n",
    "    \n",
    "    \"hmm\": {\n",
    "        \"n_components\": [2, 3, 4],\n",
    "        \"covariance_type\": [\"diag\", \"full\", \"tied\"],\n",
    "        \"n_iter\": [100, 200],\n",
    "        \"init_params\": [\"c\", \"s\", \"cs\"],\n",
    "        \"params\": [\"c\", \"t\", \"ct\"]\n",
    "    },\n",
    "    \n",
    "    \"bayes_network\": {\n",
    "        \"structure\": [None],\n",
    "        \"n_bins\": [2],\n",
    "        \"strategy\": [\"kmeans\"],\n",
    "        \"min_unique_values\": [2],\n",
    "        \"max_features\": [10]\n",
    "    },\n",
    "    \n",
    "    # \"crf\": {\n",
    "    #     \"c1\": [0.1, 0.01],  # L1 Regularization\n",
    "    #     \"c2\": [0.1, 0.01],  # L2 Regularization\n",
    "    #     \"max_iterations\": [50, 100]  # Limit iterations\n",
    "    # }\n",
    "}\n",
    "\n",
    "# Dictionary for dimensionality reduction methods\n",
    "DIMENSIONALITY_REDUCTION_DICT = {\n",
    "    \"pca\": PCA,\n",
    "    \"lda\": LDA,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "dataset_path = os.path.join(project_root, \"data\", \"final\", \"final_clean_no_neutral_no_duplicates_v1.csv\")\n",
    "df = pd.read_csv(dataset_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace target 4 with 1\n",
    "df[\"target\"] = df[\"target\"].replace(4, 1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# New API Call"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_methods = [\"tfidf\", \"count\", \"word2vec\", \"glove\"]\n",
    "df_sampled = df.sample(n=1000, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_lst = df_sampled[\"text_clean\"].tolist()\n",
    "label_lst = df_sampled[\"target\"].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_features_dict, X_test_features_dict, y_train, y_test = build_vector_for_text(df_sampled, feature_methods, project_root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name_lst = [\n",
    "    # \"decision_tree\", # ok\n",
    "    # \"random_forest\", # ok\n",
    "    # \"xgboost\", \n",
    "    # \"perceptron\", # ok\n",
    "    # \"mlp\", # lau but ok\n",
    "    # \"lstm\",\n",
    "    \"distilbert\",\n",
    "    # \"bayesian\",\n",
    "    # \"GA\",\n",
    "    # \"hmm\",\n",
    "    # \"bayesnet\",\n",
    "    # \"logistic_regression\",\n",
    "    # \"svm\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_model = os.path.join(project_root, \"src\", \"models\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip uninstall tf-nightly\n",
    "%pip install tensorflow\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip show keras-nlp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_general_model(df_sampled, doc_lst, label_lst, model_name_lst, feature_methods, MODEL_DICT, MODEL_PARAMS, X_train_features_dict, X_test_features_dict, y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_general_model(model_name_lst, feature_methods, X_test_features_dict, y_test, trained_model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_env_test",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
